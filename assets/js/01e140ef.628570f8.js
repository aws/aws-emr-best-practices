"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2793],{6782:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>d,toc:()=>a});var s=r(5893),t=r(1151);const i={sidebar_position:2,sidebar_label:"Price-Performance"},c="Price Performance",d={id:"benchmarks/price_performance",title:"Price Performance",description:'In the context of this guide, "price-performance" refers to the cost (in dollars) of running a workload for a specified level of performance (measured in run time, in seconds). Utilizing price-performance is crucial for benchmarking to grasp the implications of variables that cannot be standardized. These variables may include deployment models, competitor pricing, container scheduling, or engines.',source:"@site/docs/benchmarks/price_performance.md",sourceDirName:"benchmarks",slug:"/benchmarks/price_performance",permalink:"/aws-open-data-analytics/docs/benchmarks/price_performance",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/benchmarks/price_performance.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"Price-Performance"},sidebar:"benchmarks",previous:{title:"Introduction",permalink:"/aws-open-data-analytics/docs/benchmarks/introduction"},next:{title:"Benchmarking Variables",permalink:"/aws-open-data-analytics/docs/benchmarks/benchmarking_variables"}},o={},a=[];function l(e){const n={h1:"h1",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"price-performance",children:"Price Performance"}),"\n",(0,s.jsx)(n.p,{children:'In the context of this guide, "price-performance" refers to the cost (in dollars) of running a workload for a specified level of performance (measured in run time, in seconds). Utilizing price-performance is crucial for benchmarking to grasp the implications of variables that cannot be standardized. These variables may include deployment models, competitor pricing, container scheduling, or engines.'}),"\n",(0,s.jsx)(n.p,{children:"For variables that are within our control, such as infrastructure sizing or application configurations, it's essential to maintain consistency across all benchmarks."}),"\n",(0,s.jsx)(n.p,{children:"Below examples illustrates the importance of price-performance."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example 1:"})," Customer wants to compare OSS Spark vs EMR Spark with different cluster sizes"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"Cluster #1"}),(0,s.jsx)(n.th,{children:"Cluster #2"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Runtime (s)"}),(0,s.jsx)(n.td,{children:"12"}),(0,s.jsx)(n.td,{children:"30"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"# of nodes"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"10"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Engine"}),(0,s.jsx)(n.td,{children:"OSS Spark Runtime"}),(0,s.jsx)(n.td,{children:"EMR Spark Runtime"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cost ($)"}),(0,s.jsx)(n.td,{children:"600"}),(0,s.jsx)(n.td,{children:"300"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"In the above example, Cluster #1 is running OSS spark and completes in 12s with 50 nodes, while EMR Spark completes in 30s with 10 nodes. However, when we look at total cost, cluster #2 total cost is lower than cluster #1 making it a better option. Comparing cost in relation to the work being done considers the difference in # of nodes and engine. Assuming performance is linear, lets look at what happens when we increase the # of nodes in cluster 2."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example 2:"})," Customer wants to compare Open Source Software (OSS) Spark vs EMR Spark  with same cluster sizes"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"Cluster #1"}),(0,s.jsx)(n.th,{children:"Cluster #2"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Runtime (s)"}),(0,s.jsx)(n.td,{children:"12"}),(0,s.jsx)(n.td,{children:"6"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"# of nodes"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"50"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Engine"}),(0,s.jsx)(n.td,{children:"OSS Spark Runtime"}),(0,s.jsx)(n.td,{children:"EMR Spark Runtime"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cost ($)"}),(0,s.jsx)(n.td,{children:"600"}),(0,s.jsx)(n.td,{children:"300"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"After increasing the # of nodes to be the same across both clusters, runtime is reduced to 6seconds on Cluster #2 and cost remains the same at 300$. Our conclusion from the first example remains the same. Cluster #2 is the best option from a price-performance perspective."}),"\n",(0,s.jsx)(n.p,{children:"It\u2019s important to note that price-performance is not always linear. This is often seen when workloads have data skew. In these cases, adding more compute does not reduce runtime proportionally and adds costs."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example 3:"})," Same workload across different # of nodes - data skew"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"Run #1"}),(0,s.jsx)(n.th,{children:"Run #2"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Runtime (s)"}),(0,s.jsx)(n.td,{children:"100"}),(0,s.jsx)(n.td,{children:"75"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"# of nodes"}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"20"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Engine"}),(0,s.jsx)(n.td,{children:"EMR Spark Runtime"}),(0,s.jsx)(n.td,{children:"EMR Spark Runtime"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cost ($)"}),(0,s.jsx)(n.td,{children:"1000"}),(0,s.jsx)(n.td,{children:"1500"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"In the above example, performance is not linear. While runtime reduced to 75s, overall cost increased. In these cases, it\u2019s important ensure the # of nodes are the same for both comparisons."}),"\n",(0,s.jsx)(n.p,{children:"Another scenario where price-performance is useful is when comparing different pricing models or vendors. Take the example below:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example 4:"})," Same workload across different pricing models"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"EMR Spark Runtime"}),(0,s.jsx)(n.th,{children:"Vendor"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Runtime (s)"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"40"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"# of nodes"}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"10"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"$/s"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"1.5"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cost ($)"}),(0,s.jsx)(n.td,{children:"500"}),(0,s.jsx)(n.td,{children:"600"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"In the above , the same workload on vendor runs in 40s, while EMR runs in 50s. While vendor may seem faster, when we factor in price-performance, we see total cost is lower with EMR. If runtime is a key requirement, we can increase the # of nodes in relation to performance as illustrated in example 5."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example 5:"})," Same workload across different pricing models with different # of nodes"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"EMR Spark Runtime"}),(0,s.jsx)(n.th,{children:"EMR Spark Runtime linear performance"}),(0,s.jsx)(n.th,{children:"Vendor"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Runtime (s)"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"25"}),(0,s.jsx)(n.td,{children:"40"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"# of nodes"}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"20"}),(0,s.jsx)(n.td,{children:"10"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"$/s"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"1.5"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cost ($)"}),(0,s.jsx)(n.td,{children:"500"}),(0,s.jsx)(n.td,{children:"500"}),(0,s.jsx)(n.td,{children:"600"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The goal with benchmarking should always be to have like-for-like comparisons. This is especially true for factors such as application configuration settings such as executor sizes, input and output dataset, cluster size and instances. However, factors like vendor/aws pricing model, engine optimizations, and schedulers cannot be made the same. As such, it\u2019s important to use price-performance as a key factor."})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},1151:(e,n,r)=>{r.d(n,{Z:()=>d,a:()=>c});var s=r(7294);const t={},i=s.createContext(t);function c(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);