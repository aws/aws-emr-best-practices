"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9233],{682:(e,r,s)=>{s.r(r),s.d(r,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>d,toc:()=>a});var n=s(4848),t=s(8453);const i={sidebar_position:2,sidebar_label:"Price-Performance"},c="Price Performance",d={id:"benchmarks/price_performance",title:"Price Performance",description:'In the context of this guide, "price-performance" refers to the cost (in dollars) of running a workload for a specified level of performance (measured in run time, in seconds). Utilizing price-performance is crucial for benchmarking to grasp the implications of variables that cannot be standardized. These variables may include deployment models, competitor pricing, container scheduling, or engines.',source:"@site/docs/benchmarks/price_performance.md",sourceDirName:"benchmarks",slug:"/benchmarks/price_performance",permalink:"/aws-emr-best-practices/docs/benchmarks/price_performance",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/benchmarks/price_performance.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"Price-Performance"},sidebar:"benchmarks",previous:{title:"Introduction",permalink:"/aws-emr-best-practices/docs/benchmarks/introduction"},next:{title:"Benchmarking Variables",permalink:"/aws-emr-best-practices/docs/benchmarks/benchmarking_variables"}},o={},a=[];function l(e){const r={h1:"h1",header:"header",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"price-performance",children:"Price Performance"})}),"\n",(0,n.jsx)(r.p,{children:'In the context of this guide, "price-performance" refers to the cost (in dollars) of running a workload for a specified level of performance (measured in run time, in seconds). Utilizing price-performance is crucial for benchmarking to grasp the implications of variables that cannot be standardized. These variables may include deployment models, competitor pricing, container scheduling, or engines.'}),"\n",(0,n.jsx)(r.p,{children:"For variables that are within our control, such as infrastructure sizing or application configurations, it's essential to maintain consistency across all benchmarks."}),"\n",(0,n.jsx)(r.p,{children:"Below examples illustrates the importance of price-performance."}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Example 1:"})," Customer wants to compare OSS Spark vs EMR Spark with different cluster sizes"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{}),(0,n.jsx)(r.th,{children:"Cluster #1"}),(0,n.jsx)(r.th,{children:"Cluster #2"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Runtime (s)"}),(0,n.jsx)(r.td,{children:"12"}),(0,n.jsx)(r.td,{children:"30"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"# of nodes"}),(0,n.jsx)(r.td,{children:"50"}),(0,n.jsx)(r.td,{children:"10"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Engine"}),(0,n.jsx)(r.td,{children:"OSS Spark Runtime"}),(0,n.jsx)(r.td,{children:"EMR Spark Runtime"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Cost ($)"}),(0,n.jsx)(r.td,{children:"600"}),(0,n.jsx)(r.td,{children:"300"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"In the above example, Cluster #1 is running OSS spark and completes in 12s with 50 nodes, while EMR Spark completes in 30s with 10 nodes. However, when we look at total cost, cluster #2 total cost is lower than cluster #1 making it a better option. Comparing cost in relation to the work being done considers the difference in # of nodes and engine. Assuming performance is linear, lets look at what happens when we increase the # of nodes in cluster 2."}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Example 2:"})," Customer wants to compare Open Source Software (OSS) Spark vs EMR Spark  with same cluster sizes"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{}),(0,n.jsx)(r.th,{children:"Cluster #1"}),(0,n.jsx)(r.th,{children:"Cluster #2"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Runtime (s)"}),(0,n.jsx)(r.td,{children:"12"}),(0,n.jsx)(r.td,{children:"6"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"# of nodes"}),(0,n.jsx)(r.td,{children:"50"}),(0,n.jsx)(r.td,{children:"50"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Engine"}),(0,n.jsx)(r.td,{children:"OSS Spark Runtime"}),(0,n.jsx)(r.td,{children:"EMR Spark Runtime"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Cost ($)"}),(0,n.jsx)(r.td,{children:"600"}),(0,n.jsx)(r.td,{children:"300"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"After increasing the # of nodes to be the same across both clusters, runtime is reduced to 6seconds on Cluster #2 and cost remains the same at 300$. Our conclusion from the first example remains the same. Cluster #2 is the best option from a price-performance perspective."}),"\n",(0,n.jsx)(r.p,{children:"It\u2019s important to note that price-performance is not always linear. This is often seen when workloads have data skew. In these cases, adding more compute does not reduce runtime proportionally and adds costs."}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Example 3:"})," Same workload across different # of nodes - data skew"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{}),(0,n.jsx)(r.th,{children:"Run #1"}),(0,n.jsx)(r.th,{children:"Run #2"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Runtime (s)"}),(0,n.jsx)(r.td,{children:"100"}),(0,n.jsx)(r.td,{children:"75"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"# of nodes"}),(0,n.jsx)(r.td,{children:"10"}),(0,n.jsx)(r.td,{children:"20"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Engine"}),(0,n.jsx)(r.td,{children:"EMR Spark Runtime"}),(0,n.jsx)(r.td,{children:"EMR Spark Runtime"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Cost ($)"}),(0,n.jsx)(r.td,{children:"1000"}),(0,n.jsx)(r.td,{children:"1500"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"In the above example, performance is not linear. While runtime reduced to 75s, overall cost increased. In these cases, it\u2019s important ensure the # of nodes are the same for both comparisons."}),"\n",(0,n.jsx)(r.p,{children:"Another scenario where price-performance is useful is when comparing different pricing models or vendors. Take the example below:"}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Example 4:"})," Same workload across different pricing models"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{}),(0,n.jsx)(r.th,{children:"EMR Spark Runtime"}),(0,n.jsx)(r.th,{children:"Vendor"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Runtime (s)"}),(0,n.jsx)(r.td,{children:"50"}),(0,n.jsx)(r.td,{children:"40"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"# of nodes"}),(0,n.jsx)(r.td,{children:"10"}),(0,n.jsx)(r.td,{children:"10"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"$/s"}),(0,n.jsx)(r.td,{children:"1"}),(0,n.jsx)(r.td,{children:"1.5"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Cost ($)"}),(0,n.jsx)(r.td,{children:"500"}),(0,n.jsx)(r.td,{children:"600"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"In the above , the same workload on vendor runs in 40s, while EMR runs in 50s. While vendor may seem faster, when we factor in price-performance, we see total cost is lower with EMR. If runtime is a key requirement, we can increase the # of nodes in relation to performance as illustrated in example 5."}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Example 5:"})," Same workload across different pricing models with different # of nodes"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{}),(0,n.jsx)(r.th,{children:"EMR Spark Runtime"}),(0,n.jsx)(r.th,{children:"EMR Spark Runtime linear performance"}),(0,n.jsx)(r.th,{children:"Vendor"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Runtime (s)"}),(0,n.jsx)(r.td,{children:"50"}),(0,n.jsx)(r.td,{children:"25"}),(0,n.jsx)(r.td,{children:"40"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"# of nodes"}),(0,n.jsx)(r.td,{children:"10"}),(0,n.jsx)(r.td,{children:"20"}),(0,n.jsx)(r.td,{children:"10"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"$/s"}),(0,n.jsx)(r.td,{children:"1"}),(0,n.jsx)(r.td,{children:"1"}),(0,n.jsx)(r.td,{children:"1.5"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Cost ($)"}),(0,n.jsx)(r.td,{children:"500"}),(0,n.jsx)(r.td,{children:"500"}),(0,n.jsx)(r.td,{children:"600"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"The goal with benchmarking should always be to have like-for-like comparisons. This is especially true for factors such as application configuration settings such as executor sizes, input and output dataset, cluster size and instances. However, factors like vendor/aws pricing model, engine optimizations, and schedulers cannot be made the same. As such, it\u2019s important to use price-performance as a key factor."})]})}function h(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},8453:(e,r,s)=>{s.d(r,{R:()=>c,x:()=>d});var n=s(6540);const t={},i=n.createContext(t);function c(e){const r=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function d(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),n.createElement(i.Provider,{value:r},e.children)}}}]);