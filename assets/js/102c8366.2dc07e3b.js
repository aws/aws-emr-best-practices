"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6655],{2034:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var s=n(5893),a=n(1151);const o={},i="Best Practice for HDFS",r={id:"bestpractices/Applications/Hbase/best_practice_hdfs",title:"Best Practice for HDFS",description:"This section highlights some of the features / best practice that you could use to improve the performance in your cluster when using HDFS as storage layer for HBase.",source:"@site/docs/bestpractices/5 - Applications/Hbase/best_practice_hdfs.md",sourceDirName:"bestpractices/5 - Applications/Hbase",slug:"/bestpractices/Applications/Hbase/best_practice_hdfs",permalink:"/aws-open-data-analytics/docs/bestpractices/Applications/Hbase/best_practice_hdfs",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bestpractices/5 - Applications/Hbase/best_practice_hdfs.md",tags:[],version:"current",frontMatter:{},sidebar:"bestpractices",previous:{title:"Best Practices",permalink:"/aws-open-data-analytics/docs/bestpractices/Applications/Hbase/best_practice"},next:{title:"Best Practice for Amazon S3",permalink:"/aws-open-data-analytics/docs/bestpractices/Applications/Hbase/best_practice_s3"}},l={},c=[{value:"HDFS - Name Node memory",id:"hdfs---name-node-memory",level:2},{value:"HDFS - Service Threads",id:"hdfs---service-threads",level:2},{value:"HDFS - Short Circuit Reads",id:"hdfs---short-circuit-reads",level:2},{value:"HDFS - Replication Factor",id:"hdfs---replication-factor",level:2},{value:"HBase - Hedged Reads",id:"hbase---hedged-reads",level:2},{value:"HBase - Tiered Storage",id:"hbase---tiered-storage",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"best-practice-for-hdfs",children:"Best Practice for HDFS"}),"\n",(0,s.jsx)(t.p,{children:"This section highlights some of the features / best practice that you could use to improve the performance in your cluster when using HDFS as storage layer for HBase."}),"\n",(0,s.jsx)(t.h2,{id:"hdfs---name-node-memory",children:"HDFS - Name Node memory"}),"\n",(0,s.jsxs)(t.p,{children:["When handling large cluster deployments, it\u2019s important to properly size the HDFS NameNode (NN) heap memory which Amazon EMR set accordingly to the ",(0,s.jsx)(t.a,{href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-daemons.html",children:"instance used"}),". The NN keeps in memory metadata for each file / block allocated in the HDFS, so it\u2019s important to properly size the memory to prevent failures that might create down-times in our services."]}),"\n",(0,s.jsx)(t.p,{children:"To size the NN memory, we can consider that each HDFS block persisted in memory uses approximately 150 bytes. Using this value as reference, you can do a rough estimate of the memory required to store data in the HDFS, considering that a block is 128MB (please note that a file smaller than the HDFS block size will still count as a individual block in memory). As alternative, you can use a rule of thumb and specify 1GB of memory each 1 million blocks stored in the HDFS."}),"\n",(0,s.jsxs)(t.p,{children:["To change the default NN memory, you can use the following ",(0,s.jsx)(t.a,{href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html",children:"EMR Configuration"}),":"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hadoop-env",\n    "Configurations": [\n      {\n        "Classification": "export",\n        "Properties": {\n          "HADOOP_NAMENODE_HEAPSIZE": "8192"\n        }\n      }\n    ],\n    "Properties": {}\n  }\n]\n'})}),"\n",(0,s.jsx)(t.h2,{id:"hdfs---service-threads",children:"HDFS - Service Threads"}),"\n",(0,s.jsxs)(t.p,{children:["Amazon EMR already configures most of the HDFS parameters that are required to get good HDFS performance for HBase. However, if you\u2019re using a large instance with several vCpu, you might benefit in increasing the number of service threads that are available for the HDFS DataNode service. Please note that if you\u2019re using ",(0,s.jsx)(t.a,{href:"#hdfs-short-circuit-reads",children:"HDFS - Short Circuit Reads"})," you might not get any additional benefits from this parameter tuning, but this might still be handy if your HDFS is used by other applications."]}),"\n",(0,s.jsxs)(t.p,{children:["In this case, setting the ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.datanode.handler.count"})})," to 3 times the number of vCpu available on the node can be a good starting point. In the same way we can also tune the number of ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.namenode.handler.count"})})," for larger cluster installations. For this last parameter, you can use the following formula to determine a good value for your cluster"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"20 * log2(number of CORE nodes)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["Please note that this value might be useful to increase, if you have more than 20 CORE nodes provisioned in the cluster, otherwise you might stick to the default values set by the service. Also for both ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.namenode.handler.count"})})," and ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.datanode.handler.count"})})," you should not set a value higher than 200."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hdfs-site",\n    "Properties": {\n      "dfs.namenode.handler.count": "64",\n      "dfs.datanode.handler.count": "48"\n    }\n  }\n]\n'})}),"\n",(0,s.jsx)(t.h2,{id:"hdfs---short-circuit-reads",children:"HDFS - Short Circuit Reads"}),"\n",(0,s.jsx)(t.p,{children:'In HDFS, reads normally go through the Data Node service. When the client asks the Data Node to read a file, the service reads that file off of the disk and sends the data to the client over a TCP socket. The "short-circuit reads" bypass the Data Node, allowing the client to read the file directly. This is only possible in cases where the client is co-located with the data.'}),"\n",(0,s.jsx)(t.p,{children:"The following configurations allow HBase to directly read store files on the local node bypassing the HDFS service providing better performance while accessing data not cached."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hdfs-site",\n    "Properties": {\n      "dfs.client.read.shortcircuit": "true",\n      "dfs.client.socket-timeout": "60000", \n      "dfs.domain.socket.path": "/var/run/hadoop-hdfs/dn_socket"\n    }\n  }\n]\n'})}),"\n",(0,s.jsxs)(t.p,{children:["For additional details, see ",(0,s.jsx)(t.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html",children:"HDFS Short-Circuit Local Reads"})]}),"\n",(0,s.jsx)(t.h2,{id:"hdfs---replication-factor",children:"HDFS - Replication Factor"}),"\n",(0,s.jsx)(t.p,{children:"As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API."}),"\n",(0,s.jsxs)(t.p,{children:["If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see ",(0,s.jsx)(t.a,{href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html",children:"HDFS configuration"})," in the Amazon EMR documentation."]}),"\n",(0,s.jsx)(t.h2,{id:"hbase---hedged-reads",children:"HBase - Hedged Reads"}),"\n",(0,s.jsx)(t.p,{children:"Hadoop 2.4 introduced a new feature called Hedged Reads. If a read from a block is slow, the HDFS client starts up another parallel read against a different block replica. The result of whichever read returns first is used, and the outstanding read is cancelled. This feature helps in situations where a read occasionally takes a long time rather than when there is a systemic problem. Hedged reads can be enabled for HBase when the HFiles are stored in HDFS. This feature is disabled by default."}),"\n",(0,s.jsxs)(t.p,{children:["To enable hedged reads, set ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.client.hedged.read.threadpool.size"})})," to the number of threads to dedicate to running hedged threads, and ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.code,{children:"dfs.client.hedged.read.threshold.millis"})})," to the number of milliseconds to wait before starting another read against a different block replica."]}),"\n",(0,s.jsx)(t.p,{children:"The following is an example configuration to enable hedged reads using EMR Configurations:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hdfs-site",\n    "Properties": {\n      "dfs.client.hedged.read.threadpool.size": "20",\n      "dfs.client.hedged.read.threshold.millis": "100"\n    }\n  }\n]\n'})}),"\n",(0,s.jsx)(t.h2,{id:"hbase---tiered-storage",children:"HBase - Tiered Storage"}),"\n",(0,s.jsxs)(t.p,{children:["HBase can take advantage of the ",(0,s.jsx)(t.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html",children:"Heterogeneous Storage and Archival Storage"})," feature available in the HDFS to store more efficiently data in different type of storage and provide better performance."]}),"\n",(0,s.jsx)(t.p,{children:"One of the use case where this setup might be useful, is for write intensive clusters that have a high ingestion rate and trigger a lot of internal compaction operations. In this case we can define a policy to store HBase WALs on SSD disks present in our nodes (NVMe instance store volumes), while storing HFiles on additional EBS volumes attached to our instances. Please note that this is an advanced configuration that requires additional steps to be enabled on an EMR cluster and might not be beneficial for small clusters with simple ingestion patterns."}),"\n",(0,s.jsx)(t.p,{children:"Amazon EMR automatically configures both instances volumes stores and EBS disks that are defined while launching the cluster. However, we need to label the volumes attached to our node to specify the corresponding Storage Type for the  corresponding volume."}),"\n",(0,s.jsxs)(t.p,{children:["The first step is to attach a ",(0,s.jsx)(t.a,{href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html",children:"Bootstrap Action"})," while launching the cluster to label NVMe disks. You can use the following script to label as ",(0,s.jsx)(t.strong,{children:"SSD"})," the NVMe disks attached to the cluster's nodes."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'#!/bin/bash\n#===============================================================================\n#!# script: emr-ba-disk_labels.sh\n#!# version: v0.1\n#!#\n#!# This Bootstrap Action can be attached to an EMR Cluster to automatically\n#!# tag NVMe Disks using the HDFS Storage Type SSD.\n#!#\n#===============================================================================\n#?#\n#?# usage: ./emr-ba-disk_labels.sh\n#?#\n#===============================================================================\n\n# Force the script to run as root\nif [ $(id -u) != "0" ]\nthen\n    sudo "$0" "$@"\n    exit $?\nfi\n\n## Install nvme-cli\nyum install -y nvme-cli\ncd /tmp && wget -O epel.rpm \u2013nv https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nyum install -y ./epel.rpm && yum -y install xmlstarlet\n\n## List NVMe disks\nnvme_disks=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | awk -F\'[[:space:]][[:space:]]+\' \'{print $1}\'))\n\n## If there\'s no nvme exit\n[[ ${#nvme_disks[@]} -eq 0 ]] && echo "No EC2 NVMe Instance Storage found. End script..." && exit 0\n\nSCRIPT_NAME="/tmp/disk_labels.sh"\ncat << \'EOF\' > $SCRIPT_NAME\n#!/bin/bash\n\n# retrieve dfs.data.dir value\nHDFS_CORE_SITE="/etc/hadoop/conf/hdfs-site.xml"\n\nnvme_disks=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | awk -F\'[[:space:]][[:space:]]+\' \'{print $1}\'))\n\nfor disk in "${nvme_disks[@]}"; do\n  # Find corresponding mounted partition\n  mount_path=$(mount | grep "$disk" | awk -F\'[[:space:]]\' \'{print $3}\')\n\n  echo "Apply Hadoop Storaget Type Label [SSD] to $disk ($mount_path)"\n  curr_value=$(xmlstarlet sel -t -v \'//configuration/property[name = "dfs.data.dir"]/value\' $HDFS_CORE_SITE)\n  echo "current: $curr_value"\n  new_value=$(echo $curr_value | sed "s|$mount_path|[SSD]$mount_path|g")\n  echo "new: $new_value"\n  xmlstarlet ed -L -u "/configuration/property[name=\'dfs.data.dir\']/value" -v "$new_value" $HDFS_CORE_SITE\ndone\n\nsystemctl restart hadoop-hdfs-datanode.service\nexit 0\nEOF\nchmod +x $SCRIPT_NAME\n\nsed -i "s|null &|null \\&\\& bash $SCRIPT_NAME >> \\$STDOUT_LOG 2>> \\$STDERR_LOG 0</dev/null \\&|" /usr/share/aws/emr/node-provisioner/bin/provision-node\n\nexit 0\n'})}),"\n",(0,s.jsxs)(t.p,{children:["Once done, we can specify the following HBase configuration in the ",(0,s.jsx)(t.em,{children:"hbase-site"})," in order to store our WALs files on SSD disks only."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hbase-site",\n    "Properties": {\n      "hbase.wal.storage.policy": "ALL_SSD"\n    }\n  }\n]\n'})}),"\n",(0,s.jsx)(t.p,{children:"By doing this, WALs will be allocated and persisted on the HDFS using disks that have been labeled as SSD. To verify the setup, you can run the following command from the EMR master node that will display the corresponding allocations on the blocks on the HDFS for WALs."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"# Describe block allocation for hbase root dir\nhdfs fsck /user/hbase/WALs -files -blocks -locations\n\n# Sample output\n/user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746301597 135252162 bytes, replicated: replication=1, 1 block(s):  OK\n0. BP-581531277-172.31.3.43-1674746228762:blk_1073741836_1012 len=135252162 Live_repl=1  [DatanodeInfoWithStorage[172.31.3.138:9866,DS-5ef6e227-738d-4cb5-9fc9-4d636744674d,SSD]]\n\n/user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746426864 135213883 bytes, replicated: replication=1, 1 block(s):  OK\n0. BP-581531277-172.31.3.43-1674746228762:blk_1073742073_1255 len=135213883 Live_repl=1  [DatanodeInfoWithStorage[172.31.3.138:9866,DS-bf9acb8e-ad9f-4757-a8cb-59b9d1d0e659,SSD]]\n"})}),"\n",(0,s.jsx)(t.p,{children:"Based on this example, you can create more complex scenarios depending on the volumes attached to the nodes."}),"\n",(0,s.jsxs)(t.p,{children:["HBase also provides another useful feature called ",(0,s.jsx)(t.a,{href:"https://issues.apache.org/jira/browse/HBASE-24289",children:"Heterogeneous Storage for Date Tiered Compaction"})," to better handle cold and hot data separation. However, this feature has been introduced in the newer HBase 3.x versions only."]}),"\n",(0,s.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(t.p,{children:"The following summarize a minimal set of configurations you can tune to improve the performance on an HDFS cluster."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "Classification": "hbase-site",\n    "Properties": {\n      "hbase.regionserver.handler.count": "120"\n    }\n  },\n  {\n    "Classification": "hdfs-site",\n    "Properties": {\n      "dfs.namenode.handler.count": "64",\n      "dfs.datanode.handler.count": "48",\n      "dfs.client.hedged.read.threadpool.size": "20",\n      "dfs.client.hedged.read.threshold.millis": "100",\n      "dfs.client.read.shortcircuit": "true",\n      "dfs.client.socket-timeout": "60000", \n      "dfs.domain.socket.path": "/var/run/hadoop-hdfs/dn_socket"\n    }\n  }\n]\n'})})]})}function h(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>i});var s=n(7294);const a={},o=s.createContext(a);function i(e){const t=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:t},e.children)}}}]);