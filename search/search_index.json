{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to the EMR Best Practices Guides. The goal of this project is to offer a set of best practices, templates and guides for operating Amazon EMR. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations Reliability Security Features Managed Scaling Spot Usage Applications Spark Hive Architecture Batch Ad Hoc Notebooks Datalake Storage Amazon EMR utilities github here Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"#introduction","text":"Welcome to the EMR Best Practices Guides. The goal of this project is to offer a set of best practices, templates and guides for operating Amazon EMR. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations Reliability Security Features Managed Scaling Spot Usage Applications Spark Hive Architecture Batch Ad Hoc Notebooks Datalake Storage Amazon EMR utilities github here","title":"Introduction"},{"location":"#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"applications/hive/best_practices/","text":"5.2 - Hive \u00b6 BP 5.2.1 - Upgrading Hive Metastore \u00b6 When upgrading from EMR 5.x (hive 2.x) to EMR 6.x (hive 3.x), hive metastore requires a schema upgrade to support changes and new features in Hive 3.x The following steps assumes hive metastore is running on Amazon RDS. Upgrading hive metastore is backwards compatible. Once hive metastore is upgraded, both hive 2x and hive 3.x clients/clusters can use the same hive metastore. 1. Take a snapshot of current Hive Metastore on Amazon RDS 2. Provision a new Amazon RDS with the snapshot that was created in step 1 3. Provision target EMR 6.x version without configuring an external hive metastore 4. SSH into EMR 6 cluster and update the below in hive-site.xml to point to new RDS from the previous step \"javax.jdo.option.ConnectionURL\": \"jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true\", \"javax.jdo.option.ConnectionDriverName\": \"org.mariadb.jdbc.Driver\", \"javax.jdo.option.ConnectionUserName\": \"username\", \"javax.jdo.option.ConnectionPassword\": \"password\" 5. Run the following to check current hms schema version: hive --service schemaTool -dbType mysql -info You should see the below. Make note of the current metastore schema version (2.3.0 in this case) Hive distribution version: 3.1.0 Metastore schema version: 2.3.0 org.apache.hadoop.hive.metastore.HiveMetaException: Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0 Use --verbose for detailed stacktrace. *** schemaTool failed \\*** 6. Change directory to the hive metastore upgrade scripts location cd /usr/lib/hive/scripts/metastore/upgrade/mysql Doing these steps on EMR 6.x is required because you need need the target hive distribution version (hive 3.1) and the upgrade scripts inorder to ugprade the schema. 7. Connect to mysql using below command and upgrade the schema as per the hive version. For example, if you are upgrading from 2.3.0 to 3.1.0, you would need to source the 2 scripts. Scripts can also be found in this location: https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql mysql - u < HIVEUSER > - h < ENDPOINT - RDS > - p 'PASSWORD' mysql > use hive ; mysql > source upgrade - 2.3 . 0 - to - 3.0 . 0 . mysql . sql ; mysql > source upgrade - 3.0 . 0 - to - 3.1 . 0 . mysql . sql ; 8. Verify the upgrade was succesful / usr / lib / hive / bin / schematool - dbType mysql - info Metastore connection URL : jdbc : mysql : // hostname : 3306 / hive ? createDatabaseIfNotExist = true Metastore Connection Driver : org . mariadb . jdbc . Driver Metastore connection User : admin Hive distribution version : 3.1 . 0 Metastore schema version : 3.1 . 0 schemaTool completed 9. After all commands are run, terminate the cluster. 10. Further validation: Provision new 5.x and 6.x cluster with updated hive-site.xml that points to new RDS. In both version, you can run hive --service schemaTool -dbType mysql -info In 5.x you'll see Hive distribution version: 2.3.0 Metastore schema version: 3.1.0 and in 6.x, you'll see Hive distribution version: 3.1.0 Metastore schema version: 3.1.0","title":"Best Practices"},{"location":"applications/hive/best_practices/#52-hive","text":"","title":"5.2 - Hive"},{"location":"applications/hive/best_practices/#bp-521-upgrading-hive-metastore","text":"When upgrading from EMR 5.x (hive 2.x) to EMR 6.x (hive 3.x), hive metastore requires a schema upgrade to support changes and new features in Hive 3.x The following steps assumes hive metastore is running on Amazon RDS. Upgrading hive metastore is backwards compatible. Once hive metastore is upgraded, both hive 2x and hive 3.x clients/clusters can use the same hive metastore. 1. Take a snapshot of current Hive Metastore on Amazon RDS 2. Provision a new Amazon RDS with the snapshot that was created in step 1 3. Provision target EMR 6.x version without configuring an external hive metastore 4. SSH into EMR 6 cluster and update the below in hive-site.xml to point to new RDS from the previous step \"javax.jdo.option.ConnectionURL\": \"jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true\", \"javax.jdo.option.ConnectionDriverName\": \"org.mariadb.jdbc.Driver\", \"javax.jdo.option.ConnectionUserName\": \"username\", \"javax.jdo.option.ConnectionPassword\": \"password\" 5. Run the following to check current hms schema version: hive --service schemaTool -dbType mysql -info You should see the below. Make note of the current metastore schema version (2.3.0 in this case) Hive distribution version: 3.1.0 Metastore schema version: 2.3.0 org.apache.hadoop.hive.metastore.HiveMetaException: Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0 Use --verbose for detailed stacktrace. *** schemaTool failed \\*** 6. Change directory to the hive metastore upgrade scripts location cd /usr/lib/hive/scripts/metastore/upgrade/mysql Doing these steps on EMR 6.x is required because you need need the target hive distribution version (hive 3.1) and the upgrade scripts inorder to ugprade the schema. 7. Connect to mysql using below command and upgrade the schema as per the hive version. For example, if you are upgrading from 2.3.0 to 3.1.0, you would need to source the 2 scripts. Scripts can also be found in this location: https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql mysql - u < HIVEUSER > - h < ENDPOINT - RDS > - p 'PASSWORD' mysql > use hive ; mysql > source upgrade - 2.3 . 0 - to - 3.0 . 0 . mysql . sql ; mysql > source upgrade - 3.0 . 0 - to - 3.1 . 0 . mysql . sql ; 8. Verify the upgrade was succesful / usr / lib / hive / bin / schematool - dbType mysql - info Metastore connection URL : jdbc : mysql : // hostname : 3306 / hive ? createDatabaseIfNotExist = true Metastore Connection Driver : org . mariadb . jdbc . Driver Metastore connection User : admin Hive distribution version : 3.1 . 0 Metastore schema version : 3.1 . 0 schemaTool completed 9. After all commands are run, terminate the cluster. 10. Further validation: Provision new 5.x and 6.x cluster with updated hive-site.xml that points to new RDS. In both version, you can run hive --service schemaTool -dbType mysql -info In 5.x you'll see Hive distribution version: 2.3.0 Metastore schema version: 3.1.0 and in 6.x, you'll see Hive distribution version: 3.1.0 Metastore schema version: 3.1.0","title":"BP 5.2.1  -  Upgrading Hive Metastore"},{"location":"applications/hive/introduction/","text":"Introduction \u00b6 This section offers best practices and tuning guidance for running Apache Hive workloads on Amazon EMR.","title":"Introduction"},{"location":"applications/hive/introduction/#introduction","text":"This section offers best practices and tuning guidance for running Apache Hive workloads on Amazon EMR.","title":"Introduction"},{"location":"applications/spark/best_practices/","text":"5.1 - Spark \u00b6 BP 5.1.1 - Use the most recent version of EMR \u00b6 Amazon EMR provides several Spark optimizations out of the box with EMR Spark runtime which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to improve the performance of this Spark runtime engine for new releases. Several optimizations such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference). As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits. Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of EMR Serverless (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code. aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- name spark - 6.5.0 - demo - application Detailed documentation for running Spark jobs using EMR Serverless can be found here . Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available. BP 5.1.2 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements. Memory-optimized \u00b6 Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized \u00b6 CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose \u00b6 General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU:memory ratio based on your workload needs. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute c5.18xlarge $3.06 $0.27 72 144 2 Memory r5.12xlarge $3.02 $0.27 48 384 8 General m5.16xlarge $3.07 $0.27 64 256 4 Storage-optimized \u00b6 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors. GPU instances \u00b6 GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Graviton instances \u00b6 Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance. BP 5.1.3 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application's Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications. * You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application. * You want to write output to the console i.e., send the results back to the client program where you submitted your application. * Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes. * There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors. * You are saving results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property spark.yarn.maxAppAttempts . You can increase this value further if needed. * You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property spark.yarn.submit.waitAppCompletion is set to \"false\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.1.4 - Use right file formats and compression type \u00b6 Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support. When writing Parquet files to S3, EMR Spark will use EMRFSOutputCommitter which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc. Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults. You can also apply columnar encryption on Parquet files using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.1.5 - Partitioning \u00b6 Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans. You can choose one or more partition fields from your dataset or table columns based on :- Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field. Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions). Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields. File sizes per partition. It is recommended that your individual file sizes within each partition are >=128 MB. The number of shuffle partitions will determine the number of output files per table partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property spark.sql.shuffle.partitions . You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property spark.sql.files.maxPartitionBytes . Partitioning ensures that dynamic partition pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark. For example, following query will push partition filters for better performance. l_shipdate and l_shipmode are partition fields of the table \"testdb.lineitem_shipmodesuppkey_part\". val df = spark.sql(\"select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\") df.queryExecution.toString Printing the query execution plan where we can see pushed filters for the two partition fields in where clause: == Physical Plan == AdaptiveSparkPlan isFinalPlan = true +- == Final Plan == * ( 2 ) HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- ShuffleQueryStage 0 +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 198 ] +- * ( 1 ) HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- * ( 1 ) Project +- * ( 1 ) ColumnarToRow +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> +- == Initial Plan == HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 179 ] +- HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- Project +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> BP 5.1.6 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning. For a starting point, generally, it is advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory , you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory ). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb ). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + ( spark.executor.memory * spark.yarn.executor.memoryOverheadFactor ) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor =0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to \"true\" will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property spark.yarn.heterogeneousExecutors.enabled and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties spark.executor.maxMemory and spark.executor.maxCores . Minimum resources are calculated with spark.executor.cores and spark.executor.memory . For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting spark.yarn.heterogeneousExecutors.enabled to \"false\" and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn't matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions: Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune spark.driver.maxResultSize . You can use smaller driver memory (or use the default spark.driver.memory ) if you are running multiple jobs in parallel. Now, coming to spark.sql.shuffle.partitions for Dataframes and Datasets and spark.default.parallelism for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce spark.sql.shuffle.partitions to increase partition size during the exchange. Apart from this, if you want to use tools to receive tuning suggestions, consider using Sparklens and Dr. Elephant with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application. BP 5.1.7 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below: val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also optionally fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet). spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m. spark.kryoserializer.buffer - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.1.8 - Tune Garbage Collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can monitor GC performance using Spark UI. The GC time should be ideally <= 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance. BP 5.1.9 - Use optimal APIs wherever possible \u00b6 When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes. BP 5.1.10 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads. If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks. BP 5.1.11 - For workloads with predictable pattern, consider disabling dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature for following cases: Workloads processing variable amount of data When your cluster uses autoscaling Dynamic processing requirements or unpredictable workload patterns Streaming and ad-hoc workloads When your cluster runs multiple concurrent applications Your cluster is long-running The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where: Workloads have a very predicatable pattern Amount of data processed is predictable and consistent throughout the application Cluster runs Spark application in batch mode Clusters are transient and are of fixed size (no autoscaling) Application processing is relatively uniform. Workload is not spikey in nature. For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs. For such use cases, you can consider disabling dynamic allocation along with setting the precise number and size of executors and cores like below. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"8G\", \"spark.executor.cores\": \"4\" }, \"configurations\": [] }] Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well. However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources. BP 5.1.12 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes. For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors. Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations. Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput. BP 5.1.13 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, spark.speculation is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp Using HDFS as storage (not recommended) Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data. You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation =\"true\"). [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.speculation\": \"true\" }, \"configurations\": [] }] Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter. BP 5.1.14 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running. In order to check data integrity, consider using Deequ for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on \"Status\" and check for error message of failed tasks. If you are seeing too many random unexpected errors such as \"ArrayIndexOutOfBounds\" or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job. BP 5.1.15 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes tries to skip. Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort. BP 5.1.16 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level. Salting \u00b6 Salting is one of the most common skew mitigation techniques where you add a \"salt\" to the skewed column say \"col1\". You can split it into multiple columns like \"col1_0\",\"col1_1\",\"col1_2\" and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved. Original data Salted 4 times Salted 8 times A typical Salting workflow looks like below: For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms). n = 100 salted_df = df.withColumn(\"salt\", (rand * n).cast(IntegerType)) Now, aggregation is performed on this salt column and the results are reduced by keys unsalted_df = salted_df . groupBy ( \"salt\" , groupByFields ). agg ( aggregateFields ). groupBy ( groupByFields ). agg ( aggregateFields ) Similar logic can be applied for windowing functions as well. A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance. Isolated Salting \u00b6 In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results. Isolated Salting workflow looks like below: Example code looks like below: val count = 4 val salted = df . withColumn ( \" salt \" , when ( ' col === \"A\", rand(1) * count cast IntegerType) otherwise 0) val replicaDF = skewDF . withColumn ( \" replica \" , when ( ' col === \"A\", (0 until count) toArray) otherwise Array(0)) . withColumn ( \" salt \" , explode ( ' replica ' )) . drop ( ' replica ' ) val merged = salted . join ( replicaDF , joinColumns : + \" salt \" ) Isolated broadcast join \u00b6 In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the \u201dnormal\u201d keys and perform map-side join on isolated \u201dskewed\u201d keys. Finally, we can merge the results of skewed and normal joins Isolated map-side join workflow looks like below: Example code looks like below: val count = 8 val salted = skewDF.withColumn(\"salt\", when('col === \"A\", rand(1) * count cast IntegerType) otherwise 0).repartition('col', 'salt') // Re-partition to remove skew val broadcastDF = salted.join(broadcast(sourceDF), \"symbol\") Hashing for SparkSQL queries \u00b6 While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes. Following could be an example query working on top of a skewed dataset. select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes. select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 ) limit 10; If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written. BP 5.1.17 - Choose the right type of join \u00b6 There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join \"hints\" as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively. SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; Broadcast Join \u00b6 Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (=) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is <=10 MB, this join strategy is chosen. This is based on the parameter spark.sql.autoBroadcastJoinThreshold which is defaulted to 10 MB. If one of your join tables are larger than 10 MB, you can either modify spark.sql.autoBroadcastJoinThreshold or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI. Please note that you should not use this join if your \"small\" table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is <1 GB. Sort Merge Join \u00b6 This is the most common join used by Spark. If you are joining two large tables (>10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI. Spark configuration spark.sql.join.preferSortMergeJoin is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets. Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is < 1 GB. All join types are supported. Shuffle Hash Join \u00b6 Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless spark.sql.join.preferSortMergeJoin is set to \"false\" or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join. Broadcast Nested Loop Join \u00b6 Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions (<,>,<=,>=,like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice. Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable. For example, below code leads to Broadcast Nested Loop Join. val df1 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\" ) val df2 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-04' and l_shipmode='SHIP'\" ) val nestedLoopDF = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" ) || df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) Instead, you can change the code like below: val result1 = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" )) val result2 = df1 . join ( df2 , df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) val resultDF = result1 . union ( result2 ) The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met spark.sql.autoBroadcastJoinThreshold . Cartesian Join \u00b6 Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running \"inner like\" queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks. for l_key in lhs_table : for r_key in rhs_table : # Execute join condition If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance. Following code will lead to a Cartesian product provided the tables do not meet spark.sql.autoBroadcastJoinThreshold . val crossJoinDF = df1.join(df2, df1(\"l_partkey\") >= df2(\"l_partkey\")) Now, passing a broadcast hint which leads to Broadcast Nested Loop Join val crossJoinDF = df1.join(broadcast(df2), df1(\"l_partkey\") >= df2(\"l_partkey\")) BP 5.1.18 - Consider Spark Blacklisting for large clusters \u00b6 Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14. This blacklisting is enabled by default in Amazon EMR with the spark.blacklist.decommissioning.enabled property set to true. You can control the time for which the node is blacklisted using spark.blacklist.decommissioning.timeout property , which is set to 1 hour by default, equal to the default value for yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs . It is recommended to set spark.blacklist.decommissioning.timeout to a value equal to or greater than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs to make sure that Amazon EMR blacklists the node for the entire decommissioning period. Following are some experimental blacklisting properties. spark.blacklist.task.maxTaskAttemptsPerExecutor determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2. spark.blacklist.task.maxTaskAttemptsPerNode determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2. spark.blacklist.stage.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire stage. spark.blacklist.stage.maxFailedExecutorsPerNode determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2. spark.blacklist.application.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire application. spark.blacklist.application.maxFailedExecutorsPerNode is same as spark.blacklist.stage.maxFailedExecutorsPerNode but the worker node is blacklisted for the entire application. spark.blacklist.killBlacklistedExecutors when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring. spark.blacklist.application.fetchFailure.enabled when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring. The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.blacklist.killBlacklistedExecutors\": \"true\", \"spark.blacklist.application.fetchFailure.enabled\": \"true\" }, \"configurations\": [] }] You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs. When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by spark.stage.maxConsecutiveAttempts . This is determined by the setting spark.stage.attempt.ignoreOnDecommissionFetchFailure being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions. BP 5.1.19 - Debugging and monitoring Spark applications \u00b6 EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR. Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates. Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated. EMR Persistent UI - Amazon EMR provides Persistent User Interface for Spark . This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active. Spark UI options are also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on \"stderr\" or \"stdout\" from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs. Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step's stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live. If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property yarn.log-aggregation.retain-seconds . BP 5.1.20 - Spark Observability Platforms \u00b6 Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed here . You can choose an observability platform based on your requirements. Following are some example native options. Amazon Managed Services for Prometheus and Grafana \u00b6 AWS offers Amazon Managed Prometheus (AMP) which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. Amazon Managed Grafana (AMG) is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the deployment instructions available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using PrometheusServlet and prometheus/jmx_exporter . However, some bootstrapping is necessary for this integration. Amazon Opensearch \u00b6 Amazon Opensearch is a community-driven open source fork of Elasticsearch and Kibana . It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs. Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic. BP 5.1.21 - Potential resolutions for not-so-common errors \u00b6 Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions. Potential strategies to mitigate S3 throttling errors \u00b6 For mitigating S3 throttling errors (503: Slow Down), consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg format ObjectStoreLocationProvider to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job's processing requirements increase and thus help mitigate the S3 throttling errors. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProvider here . If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ /. Precautions to take while running too many executors \u00b6 If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs. ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970 For this issue, you can increase spark.scheduler.listenerbus.eventqueue.size from default of 10000 to 2x or more until you do not see dropped events anymore. Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing spark.executor.heartbeatInterval from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources. If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing spark.yarn.containerLauncherMaxThreads which is defaulted to 25. You may also want to increase spark.yarn.am.memory (default: 512 MB) and spark.yarn.am.cores (default: 1). Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows \u00b6 You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster. In hadoop-env.sh, you can see heap sizes for HDFS daemons. export HADOOP_OPTS = \"$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError\" export HADOOP_NAMENODE_HEAPSIZE = 25190 export HADOOP_DATANODE_HEAPSIZE = 4096 In yarn-env.sh, you can see heap sizes for YARN daemons. export YARN_NODEMANAGER_HEAPSIZE = 2048 export YARN_RESOURCEMANAGER_HEAPSIZE = 7086 Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like \"MissingBlocksException\" in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below: OpenJDK 64-Bit Server VM warning : INFO : os :: commit_memory ( 0x00007f0beb662000 , 12288 , 0 ) failed ; error = 'Cannot allocate memory' ( errno = 12 ) # # There is insufficient memory for the Java Runtime Environment to continue . # Native memory allocation ( mmap ) failed to map 12288 bytes for committing reserved memory . # An error report file with more information is saved as : # / tmp / hs_err_pid14730 . log In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well: [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_OPTS\" : \"-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS\" \u201c HADOOP_DATANODE_HEAPSIZE \u201d : \"4096\" }, \"Configurations\" : [] } ] } ] Additionally, you can also consider reducing yarn.nodemanager.resource.memory-mb by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from yarn.nodemanager.resource.memory-mb for your instance types. Precautions to take for highly concurrent workloads \u00b6 When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like \u201cCaused by: java.util.zip.ZipException: error in opening zip file\u201d or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files. 22 / 02 / 25 21 : 39 : 45 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 39 : 45 WARN Client : Neither spark . yarn . jars nor spark . yarn . archive is set , falling back to uploading libraries under SPARK_HOME . 22 / 02 / 25 21 : 39 : 48 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_libs__3016570917637060246 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_libs__3016570917637060246 . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / hive - site . xml 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / pyspark . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 39 : 50 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_conf__7549408525505552236 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_conf__ . zip To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set spark.yarn.archive to that location. Below is an example: zip -r spark-dependencies.zip /mnt/jars/ hdfs dfs -mkdir /user/hadoop/deps/ hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/ /mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults. spark.yarn.archive hdfs:///user/hadoop/deps/spark-dependencies.zip You can see that this file size is large. hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip -rw-r--r-- 1 hadoop hdfsadmingroup 287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node's CPU to ensure that the utilization has been brought down. 22 / 02 / 25 21 : 56 : 08 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 56 : 08 INFO Client : Source and destination file systems are the same . Not copying hdfs : / user / hadoop / deps / spark - dependencies . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / hive - site . xml 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / pyspark . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / mnt / tmp / spark - 0 fbfb5a9 - 7 c0c - 4 f9f - befd - 3 c8f56bc4688 / __spark_conf__5472705335503774914 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / __spark_conf__ . zip If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop\u2019s ShutdownHook . If this happens, increase hadoop.service.shutdown.timeout property from 30s to to a larger value. Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR.","title":"Best Practices"},{"location":"applications/spark/best_practices/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices/#bp-511-use-the-most-recent-version-of-emr","text":"Amazon EMR provides several Spark optimizations out of the box with EMR Spark runtime which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to improve the performance of this Spark runtime engine for new releases. Several optimizations such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference). As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits. Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of EMR Serverless (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code. aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- name spark - 6.5.0 - demo - application Detailed documentation for running Spark jobs using EMR Serverless can be found here . Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available.","title":"BP 5.1.1  -  Use the most recent version of EMR"},{"location":"applications/spark/best_practices/#bp-512-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements.","title":"BP 5.1.2  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices/#memory-optimized","text":"Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively.","title":"Memory-optimized"},{"location":"applications/spark/best_practices/#cpu-optimized","text":"CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia.","title":"CPU-optimized"},{"location":"applications/spark/best_practices/#general-purpose","text":"General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU:memory ratio based on your workload needs. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute c5.18xlarge $3.06 $0.27 72 144 2 Memory r5.12xlarge $3.02 $0.27 48 384 8 General m5.16xlarge $3.07 $0.27 64 256 4","title":"General purpose"},{"location":"applications/spark/best_practices/#storage-optimized","text":"Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors.","title":"Storage-optimized"},{"location":"applications/spark/best_practices/#gpu-instances","text":"GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines.","title":"GPU instances"},{"location":"applications/spark/best_practices/#graviton-instances","text":"Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance.","title":"Graviton instances"},{"location":"applications/spark/best_practices/#bp-513-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application's Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.1.3  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications. * You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application. * You want to write output to the console i.e., send the results back to the client program where you submitted your application. * Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode.","title":"Client deploy mode"},{"location":"applications/spark/best_practices/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes. * There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors. * You are saving results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property spark.yarn.maxAppAttempts . You can increase this value further if needed. * You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property spark.yarn.submit.waitAppCompletion is set to \"false\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices/#bp-514-use-right-file-formats-and-compression-type","text":"Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support. When writing Parquet files to S3, EMR Spark will use EMRFSOutputCommitter which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc. Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults. You can also apply columnar encryption on Parquet files using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.1.4  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices/#bp-515-partitioning","text":"Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans. You can choose one or more partition fields from your dataset or table columns based on :- Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field. Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions). Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields. File sizes per partition. It is recommended that your individual file sizes within each partition are >=128 MB. The number of shuffle partitions will determine the number of output files per table partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property spark.sql.shuffle.partitions . You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property spark.sql.files.maxPartitionBytes . Partitioning ensures that dynamic partition pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark. For example, following query will push partition filters for better performance. l_shipdate and l_shipmode are partition fields of the table \"testdb.lineitem_shipmodesuppkey_part\". val df = spark.sql(\"select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\") df.queryExecution.toString Printing the query execution plan where we can see pushed filters for the two partition fields in where clause: == Physical Plan == AdaptiveSparkPlan isFinalPlan = true +- == Final Plan == * ( 2 ) HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- ShuffleQueryStage 0 +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 198 ] +- * ( 1 ) HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- * ( 1 ) Project +- * ( 1 ) ColumnarToRow +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> +- == Initial Plan == HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 179 ] +- HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- Project +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<>","title":"BP 5.1.5  -  Partitioning"},{"location":"applications/spark/best_practices/#bp-516-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning. For a starting point, generally, it is advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory , you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory ). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb ). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + ( spark.executor.memory * spark.yarn.executor.memoryOverheadFactor ) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor =0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to \"true\" will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property spark.yarn.heterogeneousExecutors.enabled and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties spark.executor.maxMemory and spark.executor.maxCores . Minimum resources are calculated with spark.executor.cores and spark.executor.memory . For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting spark.yarn.heterogeneousExecutors.enabled to \"false\" and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn't matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions: Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune spark.driver.maxResultSize . You can use smaller driver memory (or use the default spark.driver.memory ) if you are running multiple jobs in parallel. Now, coming to spark.sql.shuffle.partitions for Dataframes and Datasets and spark.default.parallelism for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce spark.sql.shuffle.partitions to increase partition size during the exchange. Apart from this, if you want to use tools to receive tuning suggestions, consider using Sparklens and Dr. Elephant with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application.","title":"BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices/#bp-517-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below: val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also optionally fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet). spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m. spark.kryoserializer.buffer - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices/#bp-518-tune-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can monitor GC performance using Spark UI. The GC time should be ideally <= 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance.","title":"BP 5.1.8  -   Tune Garbage Collector"},{"location":"applications/spark/best_practices/#bp-519-use-optimal-apis-wherever-possible","text":"When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples.","title":"BP 5.1.9  -   Use optimal APIs wherever possible"},{"location":"applications/spark/best_practices/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices/#bp-5110-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads. If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks.","title":"BP 5.1.10 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices/#bp-5111-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature for following cases: Workloads processing variable amount of data When your cluster uses autoscaling Dynamic processing requirements or unpredictable workload patterns Streaming and ad-hoc workloads When your cluster runs multiple concurrent applications Your cluster is long-running The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where: Workloads have a very predicatable pattern Amount of data processed is predictable and consistent throughout the application Cluster runs Spark application in batch mode Clusters are transient and are of fixed size (no autoscaling) Application processing is relatively uniform. Workload is not spikey in nature. For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs. For such use cases, you can consider disabling dynamic allocation along with setting the precise number and size of executors and cores like below. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"8G\", \"spark.executor.cores\": \"4\" }, \"configurations\": [] }] Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well. However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.","title":"BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation"},{"location":"applications/spark/best_practices/#bp-5112-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes. For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors. Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations. Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.","title":"BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices/#bp-5113-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, spark.speculation is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp Using HDFS as storage (not recommended) Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data. You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation =\"true\"). [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.speculation\": \"true\" }, \"configurations\": [] }] Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.","title":"BP 5.1.13  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices/#bp-5114-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running. In order to check data integrity, consider using Deequ for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on \"Status\" and check for error message of failed tasks. If you are seeing too many random unexpected errors such as \"ArrayIndexOutOfBounds\" or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job.","title":"BP 5.1.14 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices/#bp-5115-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes tries to skip. Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort.","title":"BP 5.1.15 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices/#bp-5116-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level.","title":"BP 5.1.16  -   Data Skew"},{"location":"applications/spark/best_practices/#salting","text":"Salting is one of the most common skew mitigation techniques where you add a \"salt\" to the skewed column say \"col1\". You can split it into multiple columns like \"col1_0\",\"col1_1\",\"col1_2\" and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved. Original data Salted 4 times Salted 8 times A typical Salting workflow looks like below: For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms). n = 100 salted_df = df.withColumn(\"salt\", (rand * n).cast(IntegerType)) Now, aggregation is performed on this salt column and the results are reduced by keys unsalted_df = salted_df . groupBy ( \"salt\" , groupByFields ). agg ( aggregateFields ). groupBy ( groupByFields ). agg ( aggregateFields ) Similar logic can be applied for windowing functions as well. A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance.","title":"Salting"},{"location":"applications/spark/best_practices/#isolated-salting","text":"In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results. Isolated Salting workflow looks like below: Example code looks like below: val count = 4 val salted = df . withColumn ( \" salt \" , when ( ' col === \"A\", rand(1) * count cast IntegerType) otherwise 0) val replicaDF = skewDF . withColumn ( \" replica \" , when ( ' col === \"A\", (0 until count) toArray) otherwise Array(0)) . withColumn ( \" salt \" , explode ( ' replica ' )) . drop ( ' replica ' ) val merged = salted . join ( replicaDF , joinColumns : + \" salt \" )","title":"Isolated Salting"},{"location":"applications/spark/best_practices/#isolated-broadcast-join","text":"In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the \u201dnormal\u201d keys and perform map-side join on isolated \u201dskewed\u201d keys. Finally, we can merge the results of skewed and normal joins Isolated map-side join workflow looks like below: Example code looks like below: val count = 8 val salted = skewDF.withColumn(\"salt\", when('col === \"A\", rand(1) * count cast IntegerType) otherwise 0).repartition('col', 'salt') // Re-partition to remove skew val broadcastDF = salted.join(broadcast(sourceDF), \"symbol\")","title":"Isolated broadcast join"},{"location":"applications/spark/best_practices/#hashing-for-sparksql-queries","text":"While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes. Following could be an example query working on top of a skewed dataset. select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes. select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 ) limit 10; If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written.","title":"Hashing for SparkSQL queries"},{"location":"applications/spark/best_practices/#bp-5117-choose-the-right-type-of-join","text":"There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join \"hints\" as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively. SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;","title":"BP 5.1.17  -  Choose the right type of join"},{"location":"applications/spark/best_practices/#broadcast-join","text":"Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (=) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is <=10 MB, this join strategy is chosen. This is based on the parameter spark.sql.autoBroadcastJoinThreshold which is defaulted to 10 MB. If one of your join tables are larger than 10 MB, you can either modify spark.sql.autoBroadcastJoinThreshold or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI. Please note that you should not use this join if your \"small\" table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is <1 GB.","title":"Broadcast Join"},{"location":"applications/spark/best_practices/#sort-merge-join","text":"This is the most common join used by Spark. If you are joining two large tables (>10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI. Spark configuration spark.sql.join.preferSortMergeJoin is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets. Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is < 1 GB. All join types are supported.","title":"Sort Merge Join"},{"location":"applications/spark/best_practices/#shuffle-hash-join","text":"Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless spark.sql.join.preferSortMergeJoin is set to \"false\" or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join.","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices/#broadcast-nested-loop-join","text":"Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions (<,>,<=,>=,like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice. Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable. For example, below code leads to Broadcast Nested Loop Join. val df1 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\" ) val df2 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-04' and l_shipmode='SHIP'\" ) val nestedLoopDF = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" ) || df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) Instead, you can change the code like below: val result1 = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" )) val result2 = df1 . join ( df2 , df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) val resultDF = result1 . union ( result2 ) The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met spark.sql.autoBroadcastJoinThreshold .","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices/#cartesian-join","text":"Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running \"inner like\" queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks. for l_key in lhs_table : for r_key in rhs_table : # Execute join condition If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance. Following code will lead to a Cartesian product provided the tables do not meet spark.sql.autoBroadcastJoinThreshold . val crossJoinDF = df1.join(df2, df1(\"l_partkey\") >= df2(\"l_partkey\")) Now, passing a broadcast hint which leads to Broadcast Nested Loop Join val crossJoinDF = df1.join(broadcast(df2), df1(\"l_partkey\") >= df2(\"l_partkey\"))","title":"Cartesian Join"},{"location":"applications/spark/best_practices/#bp-5118-consider-spark-blacklisting-for-large-clusters","text":"Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14. This blacklisting is enabled by default in Amazon EMR with the spark.blacklist.decommissioning.enabled property set to true. You can control the time for which the node is blacklisted using spark.blacklist.decommissioning.timeout property , which is set to 1 hour by default, equal to the default value for yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs . It is recommended to set spark.blacklist.decommissioning.timeout to a value equal to or greater than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs to make sure that Amazon EMR blacklists the node for the entire decommissioning period. Following are some experimental blacklisting properties. spark.blacklist.task.maxTaskAttemptsPerExecutor determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2. spark.blacklist.task.maxTaskAttemptsPerNode determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2. spark.blacklist.stage.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire stage. spark.blacklist.stage.maxFailedExecutorsPerNode determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2. spark.blacklist.application.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire application. spark.blacklist.application.maxFailedExecutorsPerNode is same as spark.blacklist.stage.maxFailedExecutorsPerNode but the worker node is blacklisted for the entire application. spark.blacklist.killBlacklistedExecutors when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring. spark.blacklist.application.fetchFailure.enabled when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring. The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.blacklist.killBlacklistedExecutors\": \"true\", \"spark.blacklist.application.fetchFailure.enabled\": \"true\" }, \"configurations\": [] }] You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs. When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by spark.stage.maxConsecutiveAttempts . This is determined by the setting spark.stage.attempt.ignoreOnDecommissionFetchFailure being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions.","title":"BP 5.1.18  - Consider Spark Blacklisting for large clusters"},{"location":"applications/spark/best_practices/#bp-5119-debugging-and-monitoring-spark-applications","text":"EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR. Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates. Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated. EMR Persistent UI - Amazon EMR provides Persistent User Interface for Spark . This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active. Spark UI options are also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on \"stderr\" or \"stdout\" from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs. Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step's stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live. If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property yarn.log-aggregation.retain-seconds .","title":"BP 5.1.19  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices/#bp-5120-spark-observability-platforms","text":"Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed here . You can choose an observability platform based on your requirements. Following are some example native options.","title":"BP 5.1.20  -  Spark Observability Platforms"},{"location":"applications/spark/best_practices/#amazon-managed-services-for-prometheus-and-grafana","text":"AWS offers Amazon Managed Prometheus (AMP) which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. Amazon Managed Grafana (AMG) is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the deployment instructions available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using PrometheusServlet and prometheus/jmx_exporter . However, some bootstrapping is necessary for this integration.","title":"Amazon Managed Services for Prometheus and Grafana"},{"location":"applications/spark/best_practices/#amazon-opensearch","text":"Amazon Opensearch is a community-driven open source fork of Elasticsearch and Kibana . It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs. Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic.","title":"Amazon Opensearch"},{"location":"applications/spark/best_practices/#bp-5121-potential-resolutions-for-not-so-common-errors","text":"Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions.","title":"BP 5.1.21  -  Potential resolutions for not-so-common errors"},{"location":"applications/spark/best_practices/#potential-strategies-to-mitigate-s3-throttling-errors","text":"For mitigating S3 throttling errors (503: Slow Down), consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg format ObjectStoreLocationProvider to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job's processing requirements increase and thus help mitigate the S3 throttling errors. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProvider here . If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ /.","title":"Potential strategies to mitigate S3 throttling errors"},{"location":"applications/spark/best_practices/#precautions-to-take-while-running-too-many-executors","text":"If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs. ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970 For this issue, you can increase spark.scheduler.listenerbus.eventqueue.size from default of 10000 to 2x or more until you do not see dropped events anymore. Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing spark.executor.heartbeatInterval from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources. If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing spark.yarn.containerLauncherMaxThreads which is defaulted to 25. You may also want to increase spark.yarn.am.memory (default: 512 MB) and spark.yarn.am.cores (default: 1).","title":"Precautions to take while running too many executors"},{"location":"applications/spark/best_practices/#adjust-hadoop-yarn-and-hdfs-heap-sizes-for-intensive-workflows","text":"You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster. In hadoop-env.sh, you can see heap sizes for HDFS daemons. export HADOOP_OPTS = \"$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError\" export HADOOP_NAMENODE_HEAPSIZE = 25190 export HADOOP_DATANODE_HEAPSIZE = 4096 In yarn-env.sh, you can see heap sizes for YARN daemons. export YARN_NODEMANAGER_HEAPSIZE = 2048 export YARN_RESOURCEMANAGER_HEAPSIZE = 7086 Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like \"MissingBlocksException\" in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below: OpenJDK 64-Bit Server VM warning : INFO : os :: commit_memory ( 0x00007f0beb662000 , 12288 , 0 ) failed ; error = 'Cannot allocate memory' ( errno = 12 ) # # There is insufficient memory for the Java Runtime Environment to continue . # Native memory allocation ( mmap ) failed to map 12288 bytes for committing reserved memory . # An error report file with more information is saved as : # / tmp / hs_err_pid14730 . log In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well: [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_OPTS\" : \"-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS\" \u201c HADOOP_DATANODE_HEAPSIZE \u201d : \"4096\" }, \"Configurations\" : [] } ] } ] Additionally, you can also consider reducing yarn.nodemanager.resource.memory-mb by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from yarn.nodemanager.resource.memory-mb for your instance types.","title":"Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows"},{"location":"applications/spark/best_practices/#precautions-to-take-for-highly-concurrent-workloads","text":"When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like \u201cCaused by: java.util.zip.ZipException: error in opening zip file\u201d or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files. 22 / 02 / 25 21 : 39 : 45 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 39 : 45 WARN Client : Neither spark . yarn . jars nor spark . yarn . archive is set , falling back to uploading libraries under SPARK_HOME . 22 / 02 / 25 21 : 39 : 48 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_libs__3016570917637060246 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_libs__3016570917637060246 . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / hive - site . xml 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / pyspark . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 39 : 50 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_conf__7549408525505552236 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_conf__ . zip To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set spark.yarn.archive to that location. Below is an example: zip -r spark-dependencies.zip /mnt/jars/ hdfs dfs -mkdir /user/hadoop/deps/ hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/ /mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults. spark.yarn.archive hdfs:///user/hadoop/deps/spark-dependencies.zip You can see that this file size is large. hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip -rw-r--r-- 1 hadoop hdfsadmingroup 287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node's CPU to ensure that the utilization has been brought down. 22 / 02 / 25 21 : 56 : 08 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 56 : 08 INFO Client : Source and destination file systems are the same . Not copying hdfs : / user / hadoop / deps / spark - dependencies . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / hive - site . xml 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / pyspark . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / mnt / tmp / spark - 0 fbfb5a9 - 7 c0c - 4 f9f - befd - 3 c8f56bc4688 / __spark_conf__5472705335503774914 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / __spark_conf__ . zip If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop\u2019s ShutdownHook . If this happens, increase hadoop.service.shutdown.timeout property from 30s to to a larger value. Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR.","title":"Precautions to take for highly concurrent workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/","text":"<<<<<<< HEAD 5.1 - Spark \u00b6 BP 5.1.1 - Determine right infrastructure for your Spark workloads \u00b6 ======= 5 - Spark \u00b6 BP 5.1 - Determine right infrastructure for your Spark workloads \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. <<<<<<< HEAD BP 5.1.2 - Choose the right deploy mode \u00b6 ======= BP 5.2 - Choose the right deploy mode \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. <<<<<<< HEAD BP 5.1.3 - Use right file formats and compression type \u00b6 ======= BP 5.3 - Use right file formats and compression type \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) <<<<<<< HEAD BP 5.1.4 - Partitioning \u00b6 ======= BP 5.4 - Partitioning \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. <<<<<<< HEAD BP 5.1.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 ======= BP 5.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. <<<<<<< HEAD For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\". BP 5.1.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 ======= BP 5.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. <<<<<<< HEAD BP 5.1.7 - Use appropriate garbage collector \u00b6 ======= BP 5.7 - Use appropriate garbage collector \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. <<<<<<< HEAD BP 5.1.8 - Use appropriate APIs wherever possible \u00b6 ======= BP 5.8 - Use appropriate APIs wherever possible \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. <<<<<<< HEAD BP 5.1.9 - Leverage spot nodes with managed autoscaling \u00b6 ======= BP 5.9 - Leverage spot nodes with managed autoscaling \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. <<<<<<< HEAD BP 5.1.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 ======= BP 5.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] <<<<<<< HEAD BP 5.1.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 ======= BP 5.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. <<<<<<< HEAD BP 5.1.12 - Spark speculation with EMRFS \u00b6 ======= BP 5.12 - Spark speculation with EMRFS \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. <<<<<<< HEAD BP 5.1.13 - Data quality and integrity checks with deequ \u00b6 ======= BP 5.13 - Data quality and integrity checks with deequ \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog <<<<<<< HEAD BP 5.1.14 - Use DataFrames wherever possible \u00b6 ======= BP 5.14 - Use DataFrames wherever possible \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. <<<<<<< HEAD BP 5.1.15 - Data Skew \u00b6 ======= BP 5.15 - Data Skew \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, <<<<<<< HEAD BP 5.1.16 - Use right type of join \u00b6 ======= BP 5.16 - Use right type of join \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 <<<<<<< HEAD BP 5.1.17 - Configure observability \u00b6 ======= BP 5.17 - Configure observability \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Choose an observability platform based on your requirements. <<<<<<< HEAD BP 5.1.18 - Debugging and monitoring Spark applications \u00b6 BP 5.1.19 - Common Errors \u00b6 ======= BP 5.18 - Debugging and monitoring Spark applications \u00b6 BP 5.19 - Common Errors \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"best practices BACKUP 3845"},{"location":"applications/spark/best_practices_BACKUP_3845/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-511-determine-right-infrastructure-for-your-spark-workloads","text":"=======","title":"BP 5.1.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#5-spark","text":"","title":"5 - Spark"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-51-determine-right-infrastructure-for-your-spark-workloads","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. <<<<<<< HEAD","title":"BP 5.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-512-choose-the-right-deploy-mode","text":"=======","title":"BP 5.1.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-52-choose-the-right-deploy-mode","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. <<<<<<< HEAD","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-513-use-right-file-formats-and-compression-type","text":"=======","title":"BP 5.1.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-53-use-right-file-formats-and-compression-type","text":"be7df0756ee7b84580040efd25305e72c02f8054 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) <<<<<<< HEAD","title":"BP 5.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-514-partitioning","text":"=======","title":"BP 5.1.4  -  Partitioning"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-54-partitioning","text":"be7df0756ee7b84580040efd25305e72c02f8054 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. <<<<<<< HEAD","title":"BP 5.4  -  Partitioning"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"=======","title":"BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-55-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"be7df0756ee7b84580040efd25305e72c02f8054 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. <<<<<<< HEAD For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\".","title":"BP 5.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"=======","title":"BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-56-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. <<<<<<< HEAD","title":"BP 5.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-517-use-appropriate-garbage-collector","text":"=======","title":"BP 5.1.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-57-use-appropriate-garbage-collector","text":"be7df0756ee7b84580040efd25305e72c02f8054 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. <<<<<<< HEAD","title":"BP 5.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-518-use-appropriate-apis-wherever-possible","text":"=======","title":"BP 5.1.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-58-use-appropriate-apis-wherever-possible","text":"be7df0756ee7b84580040efd25305e72c02f8054 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_BACKUP_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_BACKUP_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. <<<<<<< HEAD","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-519-leverage-spot-nodes-with-managed-autoscaling","text":"=======","title":"BP 5.1.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-59-leverage-spot-nodes-with-managed-autoscaling","text":"be7df0756ee7b84580040efd25305e72c02f8054 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. <<<<<<< HEAD","title":"BP 5.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5110-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"=======","title":"BP 5.1.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-510-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"be7df0756ee7b84580040efd25305e72c02f8054 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] <<<<<<< HEAD","title":"BP 5.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"=======","title":"BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-511-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"be7df0756ee7b84580040efd25305e72c02f8054 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. <<<<<<< HEAD","title":"BP 5.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5112-spark-speculation-with-emrfs","text":"=======","title":"BP 5.1.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-512-spark-speculation-with-emrfs","text":"be7df0756ee7b84580040efd25305e72c02f8054 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. <<<<<<< HEAD","title":"BP 5.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5113-data-quality-and-integrity-checks-with-deequ","text":"=======","title":"BP 5.1.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-513-data-quality-and-integrity-checks-with-deequ","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog <<<<<<< HEAD","title":"BP 5.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5114-use-dataframes-wherever-possible","text":"=======","title":"BP 5.1.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-514-use-dataframes-wherever-possible","text":"be7df0756ee7b84580040efd25305e72c02f8054 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. <<<<<<< HEAD","title":"BP 5.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5115-data-skew","text":"=======","title":"BP 5.1.15  -   Data Skew"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-515-data-skew","text":"be7df0756ee7b84580040efd25305e72c02f8054 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, <<<<<<< HEAD","title":"BP 5.15  -   Data Skew"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5116-use-right-type-of-join","text":"=======","title":"BP 5.1.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-516-use-right-type-of-join","text":"be7df0756ee7b84580040efd25305e72c02f8054 There are several types of joins in Spark","title":"BP 5.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BACKUP_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#broadcast-nested-loop-join","text":"<<<<<<< HEAD","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5117-configure-observability","text":"=======","title":"BP 5.1.17 -   Configure observability"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-517-configure-observability","text":"be7df0756ee7b84580040efd25305e72c02f8054 Choose an observability platform based on your requirements. <<<<<<< HEAD","title":"BP 5.17 -   Configure observability"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5118-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.1.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5119-common-errors","text":"=======","title":"BP 5.1.19  -   Common Errors"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-518-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-519-common-errors","text":"be7df0756ee7b84580040efd25305e72c02f8054 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.19  -   Common Errors"},{"location":"applications/spark/best_practices_BASE_3845/","text":"6 - Spark \u00b6 BP 6.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 6.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 6.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 6.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 6.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for BP 6.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 6.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 6.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 6.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 6.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 6.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 6.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 6.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 6.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 6.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 6.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 6.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 6.18 - Debugging and monitoring Spark applications \u00b6 BP 6.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 6 - Spark **"},{"location":"applications/spark/best_practices_BASE_3845/#6-spark","text":"","title":"6 - Spark"},{"location":"applications/spark/best_practices_BASE_3845/#bp-61-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 6.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BASE_3845/#bp-62-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 6.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#bp-63-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 6.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BASE_3845/#bp-64-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 6.4  -  Partitioning"},{"location":"applications/spark/best_practices_BASE_3845/#bp-65-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for","title":"BP 6.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BASE_3845/#bp-66-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 6.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BASE_3845/#bp-67-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 6.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BASE_3845/#bp-68-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 6.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BASE_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_BASE_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_BASE_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_BASE_3845/#bp-69-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 6.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BASE_3845/#bp-610-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 6.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BASE_3845/#bp-611-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 6.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BASE_3845/#bp-612-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 6.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BASE_3845/#bp-613-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 6.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BASE_3845/#bp-614-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 6.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BASE_3845/#bp-615-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 6.15  -   Data Skew"},{"location":"applications/spark/best_practices_BASE_3845/#bp-616-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 6.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BASE_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_BASE_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_BASE_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_BASE_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_BASE_3845/#bp-617-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 6.17 -   Configure observability"},{"location":"applications/spark/best_practices_BASE_3845/#bp-618-debugging-and-monitoring-spark-applications","text":"","title":"BP 6.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BASE_3845/#bp-619-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 6.19  -   Common Errors"},{"location":"applications/spark/best_practices_LOCAL_3845/","text":"5.1 - Spark \u00b6 BP 5.1.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 5.1.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.1.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.1.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 5.1.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\". BP 5.1.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.1.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 5.1.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 5.1.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 5.1.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 5.1.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 5.1.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 5.1.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 5.1.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 5.1.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 5.1.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 5.1.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 5.1.18 - Debugging and monitoring Spark applications \u00b6 BP 5.1.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 5.1 - Spark **"},{"location":"applications/spark/best_practices_LOCAL_3845/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-511-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 5.1.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-512-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.1.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-513-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.1.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-514-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 5.1.4  -  Partitioning"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\".","title":"BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-517-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 5.1.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-518-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.1.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_LOCAL_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_LOCAL_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_LOCAL_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-519-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 5.1.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5110-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 5.1.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5112-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 5.1.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5113-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 5.1.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5114-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 5.1.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5115-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 5.1.15  -   Data Skew"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5116-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 5.1.16  -   Use right type of join"},{"location":"applications/spark/best_practices_LOCAL_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5117-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 5.1.17 -   Configure observability"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5118-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.1.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5119-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.1.19  -   Common Errors"},{"location":"applications/spark/best_practices_REMOTE_3845/","text":"5 - Spark \u00b6 BP 5.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 5.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 5.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for BP 5.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 5.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 5.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 5.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 5.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 5.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 5.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 5.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 5.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 5.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 5.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 5.18 - Debugging and monitoring Spark applications \u00b6 BP 5.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 5 - Spark **"},{"location":"applications/spark/best_practices_REMOTE_3845/#5-spark","text":"","title":"5 - Spark"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-51-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 5.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-52-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-53-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-54-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 5.4  -  Partitioning"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-55-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for","title":"BP 5.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-56-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-57-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 5.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-58-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_REMOTE_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_REMOTE_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_REMOTE_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-59-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 5.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-510-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 5.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-511-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 5.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-512-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 5.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-513-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 5.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-514-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 5.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-515-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 5.15  -   Data Skew"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-516-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 5.16  -   Use right type of join"},{"location":"applications/spark/best_practices_REMOTE_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-517-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 5.17 -   Configure observability"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-518-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-519-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.19  -   Common Errors"},{"location":"applications/spark/introduction/","text":"Introduction \u00b6 This section offers best practices and tuning guidance for running Apache Spark workloads on Amazon EMR. The guidances cover the following main themes :- Cost optimization Performance optimization Error mitigation","title":"Introduction"},{"location":"applications/spark/introduction/#introduction","text":"This section offers best practices and tuning guidance for running Apache Spark workloads on Amazon EMR. The guidances cover the following main themes :- Cost optimization Performance optimization Error mitigation","title":"Introduction"},{"location":"architecture/adhoc/architecture/","text":"Ad Hoc - Architecture \u00b6 The following diagram illustrates a common architecture to use PrestoSQL/Trino on Amazon EMR with the Glue Data Catalog as a big data query engine to query data in Amazon S3 using standard SQL.","title":"Architecture"},{"location":"architecture/adhoc/architecture/#ad-hoc-architecture","text":"The following diagram illustrates a common architecture to use PrestoSQL/Trino on Amazon EMR with the Glue Data Catalog as a big data query engine to query data in Amazon S3 using standard SQL.","title":"Ad Hoc - Architecture"},{"location":"architecture/adhoc/introduction/","text":"Ad Hoc - Introduction \u00b6 Amazon EMR on EC2 provides a number of engines to support your ad hoc query use cases. Trino, (formely PrestoSQL) has become a popular choice for its low latency, ANSI SQL standard, ease of use and integrations with Amazon EMR feature set. Choosing between Amazon Athena and Trino on Amazon EMR \u00b6 Amazon Athena is a serverless interactive query engine that executes SQL queries on data that rests in Amazon S3. Many customers use Athena for a wide variety of use cases, including interactive querying of data to exploring data, to powering dashboards on top of operational metrics saved on S3, to powering visualization tools, such as Amazon QuickSight or Tableau. We recommend you consider Amazon Athena for these types of workloads. Athena is easy to integrate with, has several features, such as cost management and security controls, and requires little capacity planning. All of these characteristics lead to lower operational burden and costs. However, there are some use cases where Trino on Amazon EMR may be better suited than Amazon Athena. For example, consider the following priorities: Cost reduction: If cost reduction is your primary goal, we recommend that you estimate cost based on both approaches. You may find that the load and query patterns are lower in cost with Trino on Amazon EMR. Keep in mind that there is an operational cost associated with managing a Trino EMR environment. You\u2019ll need to weight the cost benefits of Trino on EMR vs its operational overhead. Performance or Specific Tuning requirements: If your use case includes a high sensitivity to performance or you want the ability to fine-tune a Presto cluster to meet the performance requirements then Trino on EMR may be a better fit. Critical features: If there are features that Amazon Athena does not currently provide, such as the use of custom serializer/deserializers for custom data types, or connectors to data stores other than those currently supported, then Trino on EMR may be a better fit. The rest of the section will focus on Trino on Amazon EMR. For more details on Amazon Athena, see here: https://aws.amazon.com/athena/","title":"Introduction"},{"location":"architecture/adhoc/introduction/#ad-hoc-introduction","text":"Amazon EMR on EC2 provides a number of engines to support your ad hoc query use cases. Trino, (formely PrestoSQL) has become a popular choice for its low latency, ANSI SQL standard, ease of use and integrations with Amazon EMR feature set.","title":"Ad Hoc - Introduction"},{"location":"architecture/adhoc/introduction/#choosing-between-amazon-athena-and-trino-on-amazon-emr","text":"Amazon Athena is a serverless interactive query engine that executes SQL queries on data that rests in Amazon S3. Many customers use Athena for a wide variety of use cases, including interactive querying of data to exploring data, to powering dashboards on top of operational metrics saved on S3, to powering visualization tools, such as Amazon QuickSight or Tableau. We recommend you consider Amazon Athena for these types of workloads. Athena is easy to integrate with, has several features, such as cost management and security controls, and requires little capacity planning. All of these characteristics lead to lower operational burden and costs. However, there are some use cases where Trino on Amazon EMR may be better suited than Amazon Athena. For example, consider the following priorities: Cost reduction: If cost reduction is your primary goal, we recommend that you estimate cost based on both approaches. You may find that the load and query patterns are lower in cost with Trino on Amazon EMR. Keep in mind that there is an operational cost associated with managing a Trino EMR environment. You\u2019ll need to weight the cost benefits of Trino on EMR vs its operational overhead. Performance or Specific Tuning requirements: If your use case includes a high sensitivity to performance or you want the ability to fine-tune a Presto cluster to meet the performance requirements then Trino on EMR may be a better fit. Critical features: If there are features that Amazon Athena does not currently provide, such as the use of custom serializer/deserializers for custom data types, or connectors to data stores other than those currently supported, then Trino on EMR may be a better fit. The rest of the section will focus on Trino on Amazon EMR. For more details on Amazon Athena, see here: https://aws.amazon.com/athena/","title":"Choosing between Amazon Athena and Trino on Amazon EMR"},{"location":"architecture/batch/introduction/","text":"coming soon...","title":"Introduction"},{"location":"architecture/datalake_storage/introduction/","text":"coming soon...","title":"Introduction"},{"location":"architecture/notebooks/introduction/","text":"coming soon...","title":"Introduction"},{"location":"cost_optimization/best_practices/","text":"1 - Cost Optimizations \u00b6 Best Practices (BP) for running cost optimized workloads on EMR. BP 1.1 Use Amazon S3 as your persistent data store \u00b6 As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost. BP 1.2 Compress, compact and convert your Amazon S3 Objects \u00b6 Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size. BP 1.3 Partition and Bucket your data in Amazon S3 \u00b6 Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones. BP 1.4 Use the right hardware family for the job type \u00b6 Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure. BP 1.5 Use instances with instance store for jobs that require high disk IOPS \u00b6 Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost. BP 1.6 Use Graviton2 instances \u00b6 Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances BP 1.7 Select the appropriate pricing model for your use case and node type \u00b6 The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html BP 1.8 Use spot instances \u00b6 Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. For more information, see the spot usage best practices section Additionally, AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances BP 1.9 Mix on-Demand and spot instances \u00b6 Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand. BP 1.10 Use EMR managed scaling \u00b6 With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost. BP 1.11 Right size application containers \u00b6 By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html BP 1.12 Monitor cluster utilization \u00b6 Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ BP 1.13 Monitor and decommission idle EMR cluster \u00b6 Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/ BP 1.14 Use the latest Amazon EMR version \u00b6 Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html BP 1.15 Using transient and long-running clusters \u00b6 Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"Best Practices"},{"location":"cost_optimization/best_practices/#1-cost-optimizations","text":"Best Practices (BP) for running cost optimized workloads on EMR.","title":"1 - Cost Optimizations"},{"location":"cost_optimization/best_practices/#bp-11-use-amazon-s3-as-your-persistent-data-store","text":"As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost.","title":"BP 1.1 Use Amazon S3 as your persistent data store"},{"location":"cost_optimization/best_practices/#bp-12-compress-compact-and-convert-your-amazon-s3-objects","text":"Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size.","title":"BP 1.2 Compress, compact and convert your Amazon S3 Objects"},{"location":"cost_optimization/best_practices/#bp-13-partition-and-bucket-your-data-in-amazon-s3","text":"Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones.","title":"BP 1.3 Partition and Bucket your data in Amazon S3"},{"location":"cost_optimization/best_practices/#bp-14-use-the-right-hardware-family-for-the-job-type","text":"Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure.","title":"BP 1.4 Use the right hardware family for the job type"},{"location":"cost_optimization/best_practices/#bp-15-use-instances-with-instance-store-for-jobs-that-require-high-disk-iops","text":"Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost.","title":"BP 1.5 Use instances with instance store for jobs that require high disk IOPS"},{"location":"cost_optimization/best_practices/#bp-16-use-graviton2-instances","text":"Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances","title":"BP 1.6 Use Graviton2 instances"},{"location":"cost_optimization/best_practices/#bp-17-select-the-appropriate-pricing-model-for-your-use-case-and-node-type","text":"The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html","title":"BP 1.7 Select the appropriate pricing model for your use case and node type"},{"location":"cost_optimization/best_practices/#bp-18-use-spot-instances","text":"Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. For more information, see the spot usage best practices section Additionally, AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances","title":"BP 1.8 Use spot instances"},{"location":"cost_optimization/best_practices/#bp-19-mix-on-demand-and-spot-instances","text":"Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand.","title":"BP 1.9 Mix on-Demand and spot instances"},{"location":"cost_optimization/best_practices/#bp-110-use-emr-managed-scaling","text":"With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost.","title":"BP 1.10 Use EMR managed scaling"},{"location":"cost_optimization/best_practices/#bp-111-right-size-application-containers","text":"By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html","title":"BP 1.11 Right size application containers"},{"location":"cost_optimization/best_practices/#bp-112-monitor-cluster-utilization","text":"Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/","title":"BP 1.12 Monitor cluster utilization"},{"location":"cost_optimization/best_practices/#bp-113-monitor-and-decommission-idle-emr-cluster","text":"Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/","title":"BP 1.13 Monitor and decommission idle EMR cluster"},{"location":"cost_optimization/best_practices/#bp-114-use-the-latest-amazon-emr-version","text":"Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html","title":"BP 1.14 Use the latest Amazon EMR version"},{"location":"cost_optimization/best_practices/#bp-115-using-transient-and-long-running-clusters","text":"Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"BP 1.15 Using transient and long-running clusters"},{"location":"cost_optimization/introduction/","text":"EMR Cost Optimization best practices focus on the continual process of refinement and improvement of a system over its entire lifecycle. From the initial design of your very first proof of concept to the ongoing operation of production workloads, adopting the practices in this document can enable you to build and operate cost-aware systems that achieve business outcomes and minimize costs, thus allowing your business to maximize its return on investment. A cost-optimized workload is one that Meets functional and non functional requirements Fully utilizes all cluster resources and Achieves an outcome at the lowest possible price point To better understand this, let\u2019s look at an example. Let\u2019s assume we have an ETL job that needs to be completed within 8 hours. In order to meet the requirements of completing the job within 8 hours, a certain amount of compute resources will be required. This is represented in the graph by the \u201cJob Demand\u201d. Sometimes this is static, where the amount of resources needed is consistent throughout the duration of the job. And sometimes it\u2019s more dynamic, where throughout the job, you have various peaks and valleys depending on the number of tasks that are running at each stage. In order for the job to finish within the 8 hours, it needs enough cluster capacity to meet the jobs compute demand - represented by the blue dotted line. If our cluster capacity is below our jobs compute demand Our job will be resource constrained and It\u2019ll cause the job to run longer than our 8 hour sla Now, just being able to meet your SLA is not enough to be cost optimized. This leads us to our 2nd step of a cost optimized workload - Fully utilizing all cluster resources Take these next two graphs as an example, in the first case, we have a cluster that has compute capacity well beyond the jobs needs, represented by space in between the jobs demand and cluster capacity In this 2nd graph, we have a better match between the clusters capacity and jobs compute demands The space in between in between two is unused resources. These are resources that are being charged for but the job does not actually need. Fully utilizing all resources means reducing this space as much as possible. Going back to our job with less predictable workload patterns, a static cluster size may not be the best way to maximize cluster resources, but instead, using something like EMR autoscaling that adjusts cluster capacity based off of your workload demand would be a better fit. In this graph, our cluster scales up and down depending on demand. Our cluster capacity becomes a function of the jobs demand of resources. The last part of being \u201cCost Optimized\u201d is achieving your jobs outcomes at the lowest price point possible. EMR has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your needs. For example, On-Demand, Spot and Commitment discounts - Savings Plans/ Reserved Instances/Capacity All of these pricing options leverage the exact same infrastructure but depending on which option you choose, the cost of your job will vary significantly. The numbers are just examples, with spot you can get up to 90% off on demand prices and with saving plans or RI, up to 72%. In the next sections, we\u2019ll discuss best practices on choosing the right pricing model for your workload. For the purpose of this example, regardless of of which option you choose, the cluster compute capacity stays the same.","title":"Introduction"},{"location":"features/capacity/best_practices/","text":"best_practices.md","title":"Best practices"},{"location":"features/managed_scaling/best_practices/","text":"4.1 - Managed Scaling \u00b6 BP 4.1.1 Keep core nodes constant and scale with only task nodes \u00b6 Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. During scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need re balance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes disk volume during HDFS re balance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the hdfs replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot. BP 4.1.2 Monitor Managed Scaling with Cloudwatch Metrics \u00b6 You can monitor your managed scaling cluster with CloudWatch metrics. This is useful if you want to better understand how your cluster is resizing to the change in job load/usage. Lets looks at an example: At 18:25, \u201cYARNMeoryAvailablePercentage\u201d starts at 100%. This means that no jobs are running. At 18:27 a job starts and we see \u201cYARNMeoryAvailablePercentage\u201d begin to drop, reaching 0% at 18:29. This triggers managed scaling to start a resize request - represented by the increase in the metric \u201cTotalNodesRequested\u201d. After 5-6 mins, at 18:35 the nodes finish provisioning and are considered \u201cRUNNING\u201d. We see an increase in the metric, \u201cTotalNodesRunning\u201d. Around the same time, we see \u201cYARNMeoryAvailablePercentage\u201d begin increasing back to 100%. For a full list of metrics and description of each, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/managed-scaling-metrics.html BP 4.1.3 Consider adjusting YARN decommissioning timeouts depending on your workload \u00b6 There are two decommissioning timeouts that are important in managed scaling: yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED spark.blacklist.decommissioning.timeout: This is the maximal time that spark does not schedule new tasks on executors running on that node. Tasks already running are allowed to complete. When managed scaling triggers a scale down, YARN will put nodes it wants to decomission in a \u201cDECOMMISSIONING\u201d state. Spark will detect this and add these nodes to a \u201cblack list\u201d. In this state, Spark will not assign any new tasks to the node and once all tasks are completed, YARN will finish decommissioning the node. If the task runs longer than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs , the node is force terminated and task will be re-assigned to another node. In certain scale down scenarios where you have long running tasks, many nodes can end up in this state where they are \u201cDECOMMISSIONING\u201d and \u201cblacklisted\u201d because of spark.blacklist.decommissioning.timeout. You may observe that new jobs run slower because it cannot assign tasks to all nodes in the cluster. To mitigate this, you can lower spark.blacklist.decommissioning.timeout to make the node available for other pending containers to continue task processing. This can improve job run times. However, please take the below into consideration: If a task is assigned to this node, and YARN transitions from DECOMMISSIONING into DECOMMISSIONED, the task will fail and will need to be reassigned to another node Spark blacklist also protects from bad nodes in the cluster -e.g faulty hardware leading to high task failure rate. Lowering the blacklist timeout can increase task failure rate since tasks will continue to assigned to these nodes. Nodes can be transitioned from DECOMMISSIONING to RUNNING due to a scale up request. In this scenario, tasks will not fail and with a lower blacklist timeout and pending tasks can continuosuly be assigned to the node With yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, consider increasing this from the default of 1hr to the length of your longest running task. This is to ensure that YARN does not force terminate the node while the task is running and having it to re-run on another node. The cost associated with re-running the long running task is generally higher than keeping the node running to ensure its completed. For more information, see: https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-3.html https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#spark-decommissioning BP 4.1.5 EMR Managed Scaling compared to Custom Automatic Scaling \u00b6 The following link highlights the key differences between EMR Managed Scaling vs. Custom Automatic Scaling. https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-scale-on-demand.html In general, we recommend using EMR managed Scaling since the metric evaluation is every 5-10 seconds. This means your EMR cluster will adjust quicker to the change in the required cluster resources. In addition, EMR managed Scaling also supports Instance Fleets and the the scaling policy is simpler to configure because EMR managed scaling only requires min and max amounts for purchasing options (On demand/Spot) and Node Type (Core/Task). Custom Automatic Scaling should be considered if you want autoscaling outside of YARN applications or if you want full control over your scaling policies (e.g evaluation period, cool down, number of nodes) BP 4.1.6 Configure Spark History Server (SHS) custom executor log URL to point to Job History Server (JHS) Directly \u00b6 When you use SHS to access application container logs, YARN Resource manager relies on the NodeManager that the jobs Application Master (AM) ran on, to redirect to the JHS. The JHS is what hosts the container logs. A jobs executor logs cannot be accessed if the AM ran on a node that\u2019s been decommissioned due to managed scaling or spot A solution to this is pointing SHS to the JHS directly, instead of letting node manager redirect. Spark 3.0 introduced spark.history.custom.executor.log.url, which allows you specify custom spark executor log url. You can configure spark.history.custom.executor.log.url as below to point to JHS directly: {{HTTP_SCHEME}} : /jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096 Where and with actual value - JHS Host is the master node with EMR","title":"4.1 - Managed Scaling"},{"location":"features/managed_scaling/best_practices/#41-managed-scaling","text":"","title":"4.1 - Managed Scaling"},{"location":"features/managed_scaling/best_practices/#bp-411-keep-core-nodes-constant-and-scale-with-only-task-nodes","text":"Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. During scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need re balance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes disk volume during HDFS re balance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the hdfs replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot.","title":"BP 4.1.1 Keep core nodes constant and scale with only task nodes"},{"location":"features/managed_scaling/best_practices/#bp-412-monitor-managed-scaling-with-cloudwatch-metrics","text":"You can monitor your managed scaling cluster with CloudWatch metrics. This is useful if you want to better understand how your cluster is resizing to the change in job load/usage. Lets looks at an example: At 18:25, \u201cYARNMeoryAvailablePercentage\u201d starts at 100%. This means that no jobs are running. At 18:27 a job starts and we see \u201cYARNMeoryAvailablePercentage\u201d begin to drop, reaching 0% at 18:29. This triggers managed scaling to start a resize request - represented by the increase in the metric \u201cTotalNodesRequested\u201d. After 5-6 mins, at 18:35 the nodes finish provisioning and are considered \u201cRUNNING\u201d. We see an increase in the metric, \u201cTotalNodesRunning\u201d. Around the same time, we see \u201cYARNMeoryAvailablePercentage\u201d begin increasing back to 100%. For a full list of metrics and description of each, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/managed-scaling-metrics.html","title":"BP 4.1.2 Monitor Managed Scaling with Cloudwatch Metrics"},{"location":"features/managed_scaling/best_practices/#bp-413-consider-adjusting-yarn-decommissioning-timeouts-depending-on-your-workload","text":"There are two decommissioning timeouts that are important in managed scaling: yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED spark.blacklist.decommissioning.timeout: This is the maximal time that spark does not schedule new tasks on executors running on that node. Tasks already running are allowed to complete. When managed scaling triggers a scale down, YARN will put nodes it wants to decomission in a \u201cDECOMMISSIONING\u201d state. Spark will detect this and add these nodes to a \u201cblack list\u201d. In this state, Spark will not assign any new tasks to the node and once all tasks are completed, YARN will finish decommissioning the node. If the task runs longer than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs , the node is force terminated and task will be re-assigned to another node. In certain scale down scenarios where you have long running tasks, many nodes can end up in this state where they are \u201cDECOMMISSIONING\u201d and \u201cblacklisted\u201d because of spark.blacklist.decommissioning.timeout. You may observe that new jobs run slower because it cannot assign tasks to all nodes in the cluster. To mitigate this, you can lower spark.blacklist.decommissioning.timeout to make the node available for other pending containers to continue task processing. This can improve job run times. However, please take the below into consideration: If a task is assigned to this node, and YARN transitions from DECOMMISSIONING into DECOMMISSIONED, the task will fail and will need to be reassigned to another node Spark blacklist also protects from bad nodes in the cluster -e.g faulty hardware leading to high task failure rate. Lowering the blacklist timeout can increase task failure rate since tasks will continue to assigned to these nodes. Nodes can be transitioned from DECOMMISSIONING to RUNNING due to a scale up request. In this scenario, tasks will not fail and with a lower blacklist timeout and pending tasks can continuosuly be assigned to the node With yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, consider increasing this from the default of 1hr to the length of your longest running task. This is to ensure that YARN does not force terminate the node while the task is running and having it to re-run on another node. The cost associated with re-running the long running task is generally higher than keeping the node running to ensure its completed. For more information, see: https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-3.html https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#spark-decommissioning","title":"BP 4.1.3 Consider adjusting YARN decommissioning timeouts depending on your workload"},{"location":"features/managed_scaling/best_practices/#bp-415-emr-managed-scaling-compared-to-custom-automatic-scaling","text":"The following link highlights the key differences between EMR Managed Scaling vs. Custom Automatic Scaling. https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-scale-on-demand.html In general, we recommend using EMR managed Scaling since the metric evaluation is every 5-10 seconds. This means your EMR cluster will adjust quicker to the change in the required cluster resources. In addition, EMR managed Scaling also supports Instance Fleets and the the scaling policy is simpler to configure because EMR managed scaling only requires min and max amounts for purchasing options (On demand/Spot) and Node Type (Core/Task). Custom Automatic Scaling should be considered if you want autoscaling outside of YARN applications or if you want full control over your scaling policies (e.g evaluation period, cool down, number of nodes)","title":"BP 4.1.5 EMR Managed Scaling compared to Custom Automatic Scaling"},{"location":"features/managed_scaling/best_practices/#bp-416-configure-spark-history-server-shs-custom-executor-log-url-to-point-to-job-history-server-jhs-directly","text":"When you use SHS to access application container logs, YARN Resource manager relies on the NodeManager that the jobs Application Master (AM) ran on, to redirect to the JHS. The JHS is what hosts the container logs. A jobs executor logs cannot be accessed if the AM ran on a node that\u2019s been decommissioned due to managed scaling or spot A solution to this is pointing SHS to the JHS directly, instead of letting node manager redirect. Spark 3.0 introduced spark.history.custom.executor.log.url, which allows you specify custom spark executor log url. You can configure spark.history.custom.executor.log.url as below to point to JHS directly: {{HTTP_SCHEME}} : /jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096 Where and with actual value - JHS Host is the master node with EMR","title":"BP 4.1.6 Configure Spark History Server (SHS) custom executor log URL to point to Job History Server (JHS) Directly"},{"location":"features/spot_usage/best_practices/","text":"4.2 - Spot Usage \u00b6 BP 4.2.1 When to use spot vs. on demand \u00b6 Spot is a great way to help reduce costs. However, there are certain scenarios where you should consider on demand because there's always a chance that an interruption can happen. The considerations are: Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Avoid spot if your workload has 0 fault tolerance or when recomputing tasks are expensive Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost. BP 4.2.1 Use Instancefleets when using Spot Instances \u00b6 Instancefleets provides clusters with Instance flexibility. Instead of relying on a single instance to reach your target capacity, you can specify up to 30 instances. This is a best practice when using Spot because EMR will automatically provision instances from the most-available Spot capacity pools when allocation strategy is enabled. Because your Spot Instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your Spot Instances are reclaimed. A good rule of thumb is to be flexible across at least 10 instance types for each workload. In addition, make sure that all Availability Zones are configured for use in your VPC and selected for your workload. An EMR cluster will only be provisioned in a single AZ but will look across all for the initial provisioning. BP 4.2.2 Ensure Application Masters only run on an On Demand Node \u00b6 When a job is submitted to EMR, the Application Master (AM) can run on any of the nodes*. The AM is is the main container requesting, launching and monitoring application specific resources. Each job launches a single AM and if the AM is assigned to a spot node, and that spot node is interrupted, your job will fail. Therefore, it's important to ensure the AM is as resilient as possible. Assuming you are running a mixed cluster of On demand and Spot, by placing AM's on On demand nodes, you'll ensure AM's do not fail due to a spot interruption. The following uses \"yarn.nodemanager.node-labels.provider.script.path\" to run a script that sets node label to the market type - On Demand or Spot. yarn-site is also updated so that application masters are only assigned to the \"on_demand\" label. Finally, the cluster is updated to include the new node label. This is a good option when you run a mix of On demand and Spot. You can enable this with the following steps: 1. Save getNodeLabels_bootstrap.sh and getNodeLabels.py in S3 and run getNodeLabels_bootstrap.sh as an EMR bootstrap action getNodeLabels_bootstrap.sh 1 2 3 #!/bin/bash aws s3 cp s3://<bucket>/getNodeLabels.py /home/hadoop chmod +x /home/hadoop/getNodeLabels.py This script will copy getNodeLabels.py onto each node which is used by YARN to set NODE_PARTITION getNodeLabels.py 1 2 3 4 5 6 7 8 #!/usr/bin/python3 import json k = '/mnt/var/lib/info/extraInstanceData.json' with open ( k ) as f : response = json . load ( f ) #print ((response['instanceRole'],response['marketType'])) if ( response [ 'instanceRole' ] in [ 'core' , 'task' ] and response [ 'marketType' ] == 'on_demand' ): print ( f \"NODE_PARTITION: { response [ 'marketType' ] . upper () } \" ) This script is run every time a node is provisioned and sets NODE_PARTITION to on_demand. 2. Set yarn-site classification to schedule AMs on ON_DEMAND nodes. [ { \"classification\":\"yarn-site\", \"Properties\":{ \"yarn.nodemanager.node-labels.provider\":\"script\", \"yarn.nodemanager.node-labels.provider.script.path\":\"/home/hadoop/getNodeLabels.py\", \"yarn.node-labels.enabled\":\"true\", \"yarn.node-labels.am.default-node-label-expression\":\"ON_DEMAND\", \"yarn.nodemanager.node-labels.provider.configured-node-partition\":\"ON_DEMAND,SPOT\" } }, { \"classification\":\"capacity-scheduler\", \"Properties\":{ \"yarn.scheduler.capacity.root.accessible-node-labels.ON_DEMAND.capacity\":\"100\", \"yarn.scheduler.capacity.root.default.accessible-node-labels.ON_DEMAND.capacity\":\"100\" } } ] 3. Add EMR Step 1 2 #!/bin/bash sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"SPOT(exclusive=false),ON_DEMAND(exclusive=false)\" Step should be the first step on the EMR cluster. This step adds the new node labels. Once your cluster is provisioned, AM's will only run on On Demand nodes. Other non AM containers will run on all nodes. * EMR 5.19 and later uses the node label feature to assign AMs on core nodes only. Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The application master processes can run on both core and task nodes by default. BP 4.2.3 Allow application masters (AM) to run on all nodes \u00b6 With EMR 5.x, AM only run on core nodes. Because Spot Instances are often used to run task nodes, it prevents applications from failing in case an AM is assigned to a spot node. As a result of this, in scenarios where applications are occupying the full core node capacity, AM's will be in a PENDING state since they can only run on core nodes. The application will have to wait for capacity to be available on the core nodes even if there's capacity on the task nodes. Allowing AM's to run on all nodes is a good option if you are not using Spot, or run a small number of core nodes and do not want your cluster to be limited by Core capacity. You can disable this behavior with the bootstrap action below: 1 2 3 4 5 #!/bin/bash echo \"backup original init.pp\" sudo cp cp /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp /tmp/ echo \"replacing node label check\" sudo sed -i '/add-to-cluster-node-labels.*/,+5d' /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The AM processes can run on both core and task nodes by default. You can enable the YARN node labels feature by configuring following properties: yarn.node-labels.enabled: true yarn.node-labels.am.default-node-label-expression: 'CORE' When you allow AM's to run on all nodes and are using managed scaling, consider increasing yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs so AM's are not automatically terminated after the 1hr timeout in the event of a scale down. See BP 4.1.3 for more details. BP 4.2.4 Reserve core nodes for only application masters (am) \u00b6 This is not necessarily related to Spot, but An alternative to BP 4.2.2 is to reserve core nodes for only application masters/spark drivers. This means tasks spawned from executors or AMs will only run on the task nodes. The approach keeps the \u201cCORE\u201d label for core nodes and specifies it as exclusive=true. This means that containers will only be allocated to CORE nodes when it matches the node partition during job submission. By default, EMR will set AM=Core and as long as users are not specifying node label = core, all containers will run on task. Add EMR step during EMR provisioning 1 2 3 4 #!/bin/bash #Change core label from exclusive=false to exclusive=true. sudo -u yarn yarn rmadmin -removeFromClusterNodeLabels \"CORE\" sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"CORE(exclusive=true)\" Applications can still be waiting for resources if the # of jobs you\u2019re submitting exceeds the available space on your core nodes. However, this is less likely to occur now that tasks cant be assigned to core. The other option to consider is allowing AM to run on all nodes but OD. I would not recommend having AM run on task. BP 4.2.5 Reduce spot interruptions by setting purchase Option to \"Use on-demand as max price\" \u00b6 By setting the spot purchase option to \"use on-demand as max price\", your spot nodes will only be interrupted when EC2 takes back spot capacity and not because of someone outbidding your spot price. BP 4.2.6 Reduce the impact of spot interruptions \u00b6 There's a few strategies to consider when using that spot will help you take advantage of spot pricing while still getting capacity: Mix on demad nodes Spot Use on demand for core nodes and spot for task Reduce individual task run time - shorter running tasks means the impact of a spot interruption is less because the amount of time to recompute is less. Reduce provisioning timeout and switch to on demand - When using Instancefleets, EMR allows you to set a timeout duration for getting spot capacity. Once the duration is hit, you can choose to terminate the cluster or fall back to on demand. The default value is 60min but consider lowering this quickly fall back to on demand when spot is not available Checkpoint often - This allows you to retry from a certain part of your pipeline if you ever lose too many spot nodes","title":"4.2 - Spot Usage"},{"location":"features/spot_usage/best_practices/#42-spot-usage","text":"","title":"4.2 - Spot Usage"},{"location":"features/spot_usage/best_practices/#bp-421-when-to-use-spot-vs-on-demand","text":"Spot is a great way to help reduce costs. However, there are certain scenarios where you should consider on demand because there's always a chance that an interruption can happen. The considerations are: Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Avoid spot if your workload has 0 fault tolerance or when recomputing tasks are expensive Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost.","title":"BP 4.2.1 When to use spot vs. on demand"},{"location":"features/spot_usage/best_practices/#bp-421-use-instancefleets-when-using-spot-instances","text":"Instancefleets provides clusters with Instance flexibility. Instead of relying on a single instance to reach your target capacity, you can specify up to 30 instances. This is a best practice when using Spot because EMR will automatically provision instances from the most-available Spot capacity pools when allocation strategy is enabled. Because your Spot Instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your Spot Instances are reclaimed. A good rule of thumb is to be flexible across at least 10 instance types for each workload. In addition, make sure that all Availability Zones are configured for use in your VPC and selected for your workload. An EMR cluster will only be provisioned in a single AZ but will look across all for the initial provisioning.","title":"BP 4.2.1 Use Instancefleets when using Spot Instances"},{"location":"features/spot_usage/best_practices/#bp-422-ensure-application-masters-only-run-on-an-on-demand-node","text":"When a job is submitted to EMR, the Application Master (AM) can run on any of the nodes*. The AM is is the main container requesting, launching and monitoring application specific resources. Each job launches a single AM and if the AM is assigned to a spot node, and that spot node is interrupted, your job will fail. Therefore, it's important to ensure the AM is as resilient as possible. Assuming you are running a mixed cluster of On demand and Spot, by placing AM's on On demand nodes, you'll ensure AM's do not fail due to a spot interruption. The following uses \"yarn.nodemanager.node-labels.provider.script.path\" to run a script that sets node label to the market type - On Demand or Spot. yarn-site is also updated so that application masters are only assigned to the \"on_demand\" label. Finally, the cluster is updated to include the new node label. This is a good option when you run a mix of On demand and Spot. You can enable this with the following steps: 1. Save getNodeLabels_bootstrap.sh and getNodeLabels.py in S3 and run getNodeLabels_bootstrap.sh as an EMR bootstrap action getNodeLabels_bootstrap.sh 1 2 3 #!/bin/bash aws s3 cp s3://<bucket>/getNodeLabels.py /home/hadoop chmod +x /home/hadoop/getNodeLabels.py This script will copy getNodeLabels.py onto each node which is used by YARN to set NODE_PARTITION getNodeLabels.py 1 2 3 4 5 6 7 8 #!/usr/bin/python3 import json k = '/mnt/var/lib/info/extraInstanceData.json' with open ( k ) as f : response = json . load ( f ) #print ((response['instanceRole'],response['marketType'])) if ( response [ 'instanceRole' ] in [ 'core' , 'task' ] and response [ 'marketType' ] == 'on_demand' ): print ( f \"NODE_PARTITION: { response [ 'marketType' ] . upper () } \" ) This script is run every time a node is provisioned and sets NODE_PARTITION to on_demand. 2. Set yarn-site classification to schedule AMs on ON_DEMAND nodes. [ { \"classification\":\"yarn-site\", \"Properties\":{ \"yarn.nodemanager.node-labels.provider\":\"script\", \"yarn.nodemanager.node-labels.provider.script.path\":\"/home/hadoop/getNodeLabels.py\", \"yarn.node-labels.enabled\":\"true\", \"yarn.node-labels.am.default-node-label-expression\":\"ON_DEMAND\", \"yarn.nodemanager.node-labels.provider.configured-node-partition\":\"ON_DEMAND,SPOT\" } }, { \"classification\":\"capacity-scheduler\", \"Properties\":{ \"yarn.scheduler.capacity.root.accessible-node-labels.ON_DEMAND.capacity\":\"100\", \"yarn.scheduler.capacity.root.default.accessible-node-labels.ON_DEMAND.capacity\":\"100\" } } ] 3. Add EMR Step 1 2 #!/bin/bash sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"SPOT(exclusive=false),ON_DEMAND(exclusive=false)\" Step should be the first step on the EMR cluster. This step adds the new node labels. Once your cluster is provisioned, AM's will only run on On Demand nodes. Other non AM containers will run on all nodes. * EMR 5.19 and later uses the node label feature to assign AMs on core nodes only. Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The application master processes can run on both core and task nodes by default.","title":"BP 4.2.2 Ensure Application Masters only run on an On Demand Node"},{"location":"features/spot_usage/best_practices/#bp-423-allow-application-masters-am-to-run-on-all-nodes","text":"With EMR 5.x, AM only run on core nodes. Because Spot Instances are often used to run task nodes, it prevents applications from failing in case an AM is assigned to a spot node. As a result of this, in scenarios where applications are occupying the full core node capacity, AM's will be in a PENDING state since they can only run on core nodes. The application will have to wait for capacity to be available on the core nodes even if there's capacity on the task nodes. Allowing AM's to run on all nodes is a good option if you are not using Spot, or run a small number of core nodes and do not want your cluster to be limited by Core capacity. You can disable this behavior with the bootstrap action below: 1 2 3 4 5 #!/bin/bash echo \"backup original init.pp\" sudo cp cp /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp /tmp/ echo \"replacing node label check\" sudo sed -i '/add-to-cluster-node-labels.*/,+5d' /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The AM processes can run on both core and task nodes by default. You can enable the YARN node labels feature by configuring following properties: yarn.node-labels.enabled: true yarn.node-labels.am.default-node-label-expression: 'CORE' When you allow AM's to run on all nodes and are using managed scaling, consider increasing yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs so AM's are not automatically terminated after the 1hr timeout in the event of a scale down. See BP 4.1.3 for more details.","title":"BP 4.2.3 Allow application masters (AM) to run on all nodes"},{"location":"features/spot_usage/best_practices/#bp-424-reserve-core-nodes-for-only-application-masters-am","text":"This is not necessarily related to Spot, but An alternative to BP 4.2.2 is to reserve core nodes for only application masters/spark drivers. This means tasks spawned from executors or AMs will only run on the task nodes. The approach keeps the \u201cCORE\u201d label for core nodes and specifies it as exclusive=true. This means that containers will only be allocated to CORE nodes when it matches the node partition during job submission. By default, EMR will set AM=Core and as long as users are not specifying node label = core, all containers will run on task. Add EMR step during EMR provisioning 1 2 3 4 #!/bin/bash #Change core label from exclusive=false to exclusive=true. sudo -u yarn yarn rmadmin -removeFromClusterNodeLabels \"CORE\" sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"CORE(exclusive=true)\" Applications can still be waiting for resources if the # of jobs you\u2019re submitting exceeds the available space on your core nodes. However, this is less likely to occur now that tasks cant be assigned to core. The other option to consider is allowing AM to run on all nodes but OD. I would not recommend having AM run on task.","title":"BP 4.2.4 Reserve core nodes for only application masters (am)"},{"location":"features/spot_usage/best_practices/#bp-425-reduce-spot-interruptions-by-setting-purchase-option-to-use-on-demand-as-max-price","text":"By setting the spot purchase option to \"use on-demand as max price\", your spot nodes will only be interrupted when EC2 takes back spot capacity and not because of someone outbidding your spot price.","title":"BP 4.2.5 Reduce spot interruptions by setting purchase Option to \"Use on-demand as max price\""},{"location":"features/spot_usage/best_practices/#bp-426-reduce-the-impact-of-spot-interruptions","text":"There's a few strategies to consider when using that spot will help you take advantage of spot pricing while still getting capacity: Mix on demad nodes Spot Use on demand for core nodes and spot for task Reduce individual task run time - shorter running tasks means the impact of a spot interruption is less because the amount of time to recompute is less. Reduce provisioning timeout and switch to on demand - When using Instancefleets, EMR allows you to set a timeout duration for getting spot capacity. Once the duration is hit, you can choose to terminate the cluster or fall back to on demand. The default value is 60min but consider lowering this quickly fall back to on demand when spot is not available Checkpoint often - This allows you to retry from a certain part of your pipeline if you ever lose too many spot nodes","title":"BP 4.2.6 Reduce the impact of spot interruptions"},{"location":"reliability/best_practices/","text":"2 - Reliability \u00b6 Best Practices (BP) for running reliable workloads on EMR. BP 2.1 Treat all clusters as transient resources \u00b6 Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html BP 2.2 Decouple storage and compute \u00b6 Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3. BP 2.3 Use the latest AMI and EMR version available \u00b6 In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html BP 2.4 Spread clusters across availability zones/subnets and time of provisioning \u00b6 Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html BP 2.5 Use on demand for core nodes and spot for task \u00b6 Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable. BP 2.6 Use instance fleet with allocation strategy \u00b6 The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy BP 2.7 With instance fleet, diversify with instances in the same family and across generations first \u00b6 When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes. BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet \u00b6 When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different. BP 2.9 If optimizing for availability, avoid exotic instance types \u00b6 Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability. BP 2.10 Handling S3 503 slow downs \u00b6 When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic BP 2.11 Audit and update EMR and EC2 limits to avoid throttling \u00b6 Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html BP 2.12 Set dfs.replication > 1 if using Spot for core nodes or for long running clusters \u00b6 dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes \u00b6 When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"Best Practices"},{"location":"reliability/best_practices/#2-reliability","text":"Best Practices (BP) for running reliable workloads on EMR.","title":"2 - Reliability"},{"location":"reliability/best_practices/#bp-21-treat-all-clusters-as-transient-resources","text":"Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html","title":"BP 2.1 Treat all clusters as transient resources"},{"location":"reliability/best_practices/#bp-22-decouple-storage-and-compute","text":"Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3.","title":"BP 2.2 Decouple storage and compute"},{"location":"reliability/best_practices/#bp-23-use-the-latest-ami-and-emr-version-available","text":"In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html","title":"BP 2.3 Use the latest AMI and EMR version available"},{"location":"reliability/best_practices/#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning","text":"Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html","title":"BP 2.4 Spread clusters across availability zones/subnets and time of provisioning"},{"location":"reliability/best_practices/#bp-25-use-on-demand-for-core-nodes-and-spot-for-task","text":"Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable.","title":"BP 2.5 Use on demand for core nodes and spot for task"},{"location":"reliability/best_practices/#bp-26-use-instance-fleet-with-allocation-strategy","text":"The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy","title":"BP 2.6 Use instance fleet with allocation strategy"},{"location":"reliability/best_practices/#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first","text":"When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes.","title":"BP 2.7 With instance fleet, diversify with instances in the same family and across generations first"},{"location":"reliability/best_practices/#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet","text":"When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different.","title":"BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet"},{"location":"reliability/best_practices/#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types","text":"Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability.","title":"BP 2.9 If optimizing for availability, avoid exotic instance types"},{"location":"reliability/best_practices/#bp-210-handling-s3-503-slow-downs","text":"When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic","title":"BP 2.10 Handling S3 503 slow downs"},{"location":"reliability/best_practices/#bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling","text":"Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html","title":"BP 2.11 Audit and update  EMR and EC2 limits to avoid throttling"},{"location":"reliability/best_practices/#bp-212-set-dfsreplication-1-if-using-spot-for-core-nodes-or-for-long-running-clusters","text":"dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume","title":"BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters"},{"location":"reliability/best_practices/#bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes","text":"When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes"},{"location":"reliability/introduction/","text":"Introduction \u00b6 EMR Reliability best practices discusses how to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues.","title":"Introduction"},{"location":"reliability/introduction/#introduction","text":"EMR Reliability best practices discusses how to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues.","title":"Introduction"},{"location":"security/best_practices/","text":"3 - Security \u00b6 Best Practices (BP) for running secure workloads on EMR. BP 3.1 Encrypt Data at rest and in transit \u00b6 Properly protecting your data at rest and in transit using encryption is a core component of our well-architected pillar of security. Amazon EMR security configurations make it easy for you to encrypt data both at rest and in transit. A security configuration is like a template for encryption and other security configurations that you can apply to any cluster when you launch it. For data at rest, EMR provides encryption options for reading and writing data in S3 via EMRFS. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) as the Default encryption mode when you enable encryption at rest. Optionally, you can specify different encryption methods for individual buckets using Per bucket encryption overrides. EMR also provides the option to encrypt local disk storage. These are EC2 instance store volumes and the attached Amazon Elastic Block Store (EBS) storage that are provisioned with your cluster. You have the options of using linux Unified Key Setup (LUKS) encryption or using AWS KMS as your key provider. For data in transit, EMR security configurations allows you you to either manually create PEM certificates, zip them in a file, and reference from Amazon S3 or implement a certificate custom provider in Java and specify the S3 path to the JAR. In either case, EMR automatically downloads artifacts to each node in the cluster and later uses them to implement the open-source, in-transit encryption features. For more information on how these certificates are used with different big data technologies, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post Secure Amazon EMR with Encryption, see: https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption BP 3.2 Restrict network access to your EMR cluster and keep EMR block public access feature enabled \u00b6 Inbound and outbound network access to your EMR cluster is controlled by security groups. It is recommended to apply the principle of least privilege to your security groups. This is so that your cluster is locked down to only the applications or individuals who need access from the expected source IPs. It\u2019s also recommended to not allow SSH access to the hadoop user. The hadoop user has elevated sudo access and access to this user is typically not requred. EMR provides a number of ways for users to interact with clusters remotely. For job submission, users can use EMR Steps API or an orchestration service like Managed airflow or AWS Step functions. For Ad hoc or notebook use cases, you can use EMR studio or allow users to connect to the specific application portsports e.g Hiveserver2 JDBC, Livy or Notebook UI\u2019s The block public access feature prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception - port 22 is an exception by default. This feature is enabled by default for each AWS Region in your AWS account and is not recommended to be turned off. Use Persistent Application UI's to remove the need to open firewall to get access to debugging UI BP 3.3 Provision clusters in a private subnet \u00b6 It is recommended to provision your EMR clusters in Private Subnets. Private subnets allow you to limit access to deployed components, and to control security and routing of the system. With a private subnet, you can enable communication with your own network over a VPN tunnel or AWS direct connect. This would allow you to access your EMR clusters from your network, without exposure to the internet. For access to other AWS services from your EMR Cluster e.g S3, VPC endpoints can be used. For more information on configuring EMR clusters in private subnets or VPC endpoints, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html BP 3.4 Configure EC2 instance metadata service (IMDS) v2 \u00b6 In AWS, Instance Metadata Service (IMDS) provides \u201cdata about your instance that you can use to configure or manage the running instance. Every instance has access to its own MDS using any HTTP client request, such as, curl command from the instance to http://169.254.169.254/latest/meta-data. IMDSv1 is fully secure and AWS will continue to support it. But IMDSv2 adds new \u201cbelt and suspenders\u201d protections for four types of vulnerabilities that could be used to try to access the IMDS. For more see: https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/ From EMR 5.32 and 6.2 onward, Amazon EMR components use IMDSv2 for all IMDS calls. For IMDS calls in your application code, you can use both IMDSv1 and IMDSv2. It is recommended to turn off IMDSv1 and only allow IMDSv2 for added security. This can be configured in EMR Security Configurations. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html#emr-security-configuration-imdsv2 BP 3.5 Create a separate IAM role for each cluster or use case \u00b6 EMR uses an IAM service roles to perform actions on your behalf to provision and manage clusters. It is recommended to create a separate IAM role for each use case and workload. This allows you to segregate access control between clusters. If you have multiple clusters, each cluster can only the services and data defined within the IAM policy. BP 3.6 Use scoped down IAM policies for authorization such as AmazonEMRFullAccessPolicy_v2 \u00b6 EMR provides managed IAM policies to grant specific access privileges to users. Managed policies offer the benefit of updating automatically if permission requirements change. If you use inline policies, service changes may occur that cause permission errors to appear. It is recommended to use new managed policies (v2 policies) which have been scoped-down to align with AWS best practices. The v2 managed policies restrict access using tags. They allow only specified Amazon EMR actions and require cluster resources that are tagged with an EMR-specific key. For more details and usage, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-policy-fullaccess-v2.html BP 3.7 Audit user activity with AWS CloudTrail \u00b6 AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service, is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events. The calls captured include calls from the Amazon EMR console and code calls to the Amazon EMR API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR. You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn\u2019t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR. Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements. BP 3.8 Upgrade your EMR Releases frequently or use a Custom AMI to get the latest OS and application software patches \u00b6 Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. If you must use an earlier release version of Amazon EMR for compatibility, we recommend that you use the latest release in a series. For example, if you must use the 5.12 series, use 5.12.2 instead of 5.12.0 or 5.12.1. If a new release becomes available in a series, consider migrating your applications to the new release.","title":"Best Practices"},{"location":"security/best_practices/#3-security","text":"Best Practices (BP) for running secure workloads on EMR.","title":"3 - Security"},{"location":"security/best_practices/#bp-31-encrypt-data-at-rest-and-in-transit","text":"Properly protecting your data at rest and in transit using encryption is a core component of our well-architected pillar of security. Amazon EMR security configurations make it easy for you to encrypt data both at rest and in transit. A security configuration is like a template for encryption and other security configurations that you can apply to any cluster when you launch it. For data at rest, EMR provides encryption options for reading and writing data in S3 via EMRFS. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) as the Default encryption mode when you enable encryption at rest. Optionally, you can specify different encryption methods for individual buckets using Per bucket encryption overrides. EMR also provides the option to encrypt local disk storage. These are EC2 instance store volumes and the attached Amazon Elastic Block Store (EBS) storage that are provisioned with your cluster. You have the options of using linux Unified Key Setup (LUKS) encryption or using AWS KMS as your key provider. For data in transit, EMR security configurations allows you you to either manually create PEM certificates, zip them in a file, and reference from Amazon S3 or implement a certificate custom provider in Java and specify the S3 path to the JAR. In either case, EMR automatically downloads artifacts to each node in the cluster and later uses them to implement the open-source, in-transit encryption features. For more information on how these certificates are used with different big data technologies, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post Secure Amazon EMR with Encryption, see: https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption","title":"BP 3.1 Encrypt Data at rest and in transit"},{"location":"security/best_practices/#bp-32-restrict-network-access-to-your-emr-cluster-and-keep-emr-block-public-access-feature-enabled","text":"Inbound and outbound network access to your EMR cluster is controlled by security groups. It is recommended to apply the principle of least privilege to your security groups. This is so that your cluster is locked down to only the applications or individuals who need access from the expected source IPs. It\u2019s also recommended to not allow SSH access to the hadoop user. The hadoop user has elevated sudo access and access to this user is typically not requred. EMR provides a number of ways for users to interact with clusters remotely. For job submission, users can use EMR Steps API or an orchestration service like Managed airflow or AWS Step functions. For Ad hoc or notebook use cases, you can use EMR studio or allow users to connect to the specific application portsports e.g Hiveserver2 JDBC, Livy or Notebook UI\u2019s The block public access feature prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception - port 22 is an exception by default. This feature is enabled by default for each AWS Region in your AWS account and is not recommended to be turned off. Use Persistent Application UI's to remove the need to open firewall to get access to debugging UI","title":"BP 3.2 Restrict network access to your EMR cluster and keep EMR block public access feature enabled"},{"location":"security/best_practices/#bp-33-provision-clusters-in-a-private-subnet","text":"It is recommended to provision your EMR clusters in Private Subnets. Private subnets allow you to limit access to deployed components, and to control security and routing of the system. With a private subnet, you can enable communication with your own network over a VPN tunnel or AWS direct connect. This would allow you to access your EMR clusters from your network, without exposure to the internet. For access to other AWS services from your EMR Cluster e.g S3, VPC endpoints can be used. For more information on configuring EMR clusters in private subnets or VPC endpoints, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html","title":"BP 3.3 Provision clusters in a private subnet"},{"location":"security/best_practices/#bp-34-configure-ec2-instance-metadata-service-imds-v2","text":"In AWS, Instance Metadata Service (IMDS) provides \u201cdata about your instance that you can use to configure or manage the running instance. Every instance has access to its own MDS using any HTTP client request, such as, curl command from the instance to http://169.254.169.254/latest/meta-data. IMDSv1 is fully secure and AWS will continue to support it. But IMDSv2 adds new \u201cbelt and suspenders\u201d protections for four types of vulnerabilities that could be used to try to access the IMDS. For more see: https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/ From EMR 5.32 and 6.2 onward, Amazon EMR components use IMDSv2 for all IMDS calls. For IMDS calls in your application code, you can use both IMDSv1 and IMDSv2. It is recommended to turn off IMDSv1 and only allow IMDSv2 for added security. This can be configured in EMR Security Configurations. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html#emr-security-configuration-imdsv2","title":"BP 3.4 Configure EC2 instance metadata service (IMDS) v2"},{"location":"security/best_practices/#bp-35-create-a-separate-iam-role-for-each-cluster-or-use-case","text":"EMR uses an IAM service roles to perform actions on your behalf to provision and manage clusters. It is recommended to create a separate IAM role for each use case and workload. This allows you to segregate access control between clusters. If you have multiple clusters, each cluster can only the services and data defined within the IAM policy.","title":"BP 3.5 Create a separate IAM role for each cluster or use case"},{"location":"security/best_practices/#bp-36-use-scoped-down-iam-policies-for-authorization-such-as-amazonemrfullaccesspolicy_v2","text":"EMR provides managed IAM policies to grant specific access privileges to users. Managed policies offer the benefit of updating automatically if permission requirements change. If you use inline policies, service changes may occur that cause permission errors to appear. It is recommended to use new managed policies (v2 policies) which have been scoped-down to align with AWS best practices. The v2 managed policies restrict access using tags. They allow only specified Amazon EMR actions and require cluster resources that are tagged with an EMR-specific key. For more details and usage, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-policy-fullaccess-v2.html","title":"BP 3.6 Use scoped down IAM policies for authorization such as AmazonEMRFullAccessPolicy_v2"},{"location":"security/best_practices/#bp-37-audit-user-activity-with-aws-cloudtrail","text":"AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service, is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events. The calls captured include calls from the Amazon EMR console and code calls to the Amazon EMR API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR. You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn\u2019t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR. Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements.","title":"BP 3.7 Audit user activity with AWS CloudTrail"},{"location":"security/best_practices/#bp-38-upgrade-your-emr-releases-frequently-or-use-a-custom-ami-to-get-the-latest-os-and-application-software-patches","text":"Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. If you must use an earlier release version of Amazon EMR for compatibility, we recommend that you use the latest release in a series. For example, if you must use the 5.12 series, use 5.12.2 instead of 5.12.0 or 5.12.1. If a new release becomes available in a series, consider migrating your applications to the new release.","title":"BP 3.8 Upgrade your EMR Releases frequently or use a Custom AMI to get the latest OS and application software patches"},{"location":"security/introduction/","text":"Introduction \u00b6 EMR Security best practices discusses how to take advantage of AWS and EMR features to protect data, systems, and assets in a way that can improve your security posture. It's recommended to first read our Well Architected paper on security to understand the risks we mitigate and how we think about security. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html","title":"Introduction"},{"location":"security/introduction/#introduction","text":"EMR Security best practices discusses how to take advantage of AWS and EMR features to protect data, systems, and assets in a way that can improve your security posture. It's recommended to first read our Well Architected paper on security to understand the risks we mitigate and how we think about security. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html","title":"Introduction"}]}