{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to the EMR Best Practices Guides. The goal of this project is to offer a set of best practices, templates and guides for operating Amazon EMR. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations Reliability Security Features Managed Scaling Spot Usage Applications Spark Hive HBase Architecture Batch Ad Hoc Notebooks Datalake Storage Amazon EMR utilities github here Contributing \u00b6 We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Introduction"},{"location":"#introduction","text":"Welcome to the EMR Best Practices Guides. The goal of this project is to offer a set of best practices, templates and guides for operating Amazon EMR. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. We currently have published guides for the following topics: Cost Optimizations Reliability Security Features Managed Scaling Spot Usage Applications Spark Hive HBase Architecture Batch Ad Hoc Notebooks Datalake Storage Amazon EMR utilities github here","title":"Introduction"},{"location":"#contributing","text":"We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it.","title":"Contributing"},{"location":"applications/hbase/best_practice/","text":"Best Practice \u00b6 The following section describes some general HBase tuning and best practice that can be applied both when using HDFS or Amazon S3 as storage layer for HBase. EMR Multi Master \u00b6 When working with HBase on Amazon EMR, it is good practice to enable the EMR Multi Master feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down. Nevertheless, this functionality is highly recommended both when using HDFS or S3 as storage layer for your HBase cluster. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason (e.g. human error, hardware impairment, etc.), it will not be possible to recover any data from the HDFS storage on the cluster as the HDFS metadata will be lost after the termination of the EMR master. EMR Termination Protection \u00b6 Using termination protection in Amazon EMR is highly recommended both when using HDFS or Amazon S3 for your HBase cluster. Amazon EMR periodically checks the Apache Hadoop YARN status of nodes running on CORE and TASK nodes in a cluster. The health status is reported by the YARN NodeManager health checker service . If a node reports an UNHEALTHY status, it will not be possible to allocate YARN containers to it until it becomes healthy again. A common reason for unhealthy nodes is that disk utilization goes above 90%. If the node stays in this state for more than 45 minutes and Termination Protection is disabled, the EMR service terminates the node and launch a fresh new one as replacement. When a node is in an UNHEALTHY state, with the termination protection enabled the nodes will not be terminated and replaced by the EMR service. This prevents to lose HDFS data blocks in case the utilization of the disks of a CORE node goes above 90%, so preventing data integrity issues in HBase tables. HBase RPC Listeners \u00b6 One of the most important parameters to configure in your HBase cluster is the number of active RPC listeners defined per Region Server. Tuning the parameter hbase.regionserver.handler.count (default: 30) can increase the number of requests that you can concurrently serve in each region server and so the overall throughput of your cluster. To modify the default number of RPC listeners you can use the following EMR configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.handler.count\" : \"120\" } } ] However, please be mindful that this parameter should be tuned accordingly to the average size of data stored or retrieved from your tables. As rule of thumb, you should increase this number when the payload of your data is lower than 100KB, while you should stick to the default, or decrease it when the payload size is >= 1MB. For small payloads (<= 1KB), you can push this value up to 4 times the number of vCpu available in your Region Servers. To determine the average payload of data stored in your tables, see Determine average row size . HBase Heap Memory \u00b6 On Amazon EMR, when you install HBase, the memory will be evenly re-partitioned between Hadoop YARN and HBase services. For a list of the default memory settings used per instance type see Task configuration in the EMR documentation. However, when working with HBase it might be convenient to override the default parameters and increase the available memory for our HBase services. This might be required if we want to host a higher number of Regions per Region Server. To modify the default memory, you should modify the HBase environmental variables defined in the hbase-env which defines the default heap memory available for each HBase service. The following list highlight the variables that should be modified by service: HBASE_MASTER_OPTS JVM options for the HBase master HBASE_REGIONSERVER_OPTS JVM options for the HBase Region Servers HBASE_THRIFT_OPTS JVM options for the HBase Thrift service HBASE_REST_OPTS JVM options for the HBase REST service It\u2019s best practice to modify the memory of each component using its own dedicated variable, rather than using the more general HBASE_OPTS , which is used to apply common JVM options across all HBase services. To override the default memory we should specify the following java parameter in our environmental variable: -Xmx<size>[g|G|m|M|k|K] . Please also make sure to add a self reference in the environmental variable to avoid loosing other parameters that are set in the script. Besides, if we modify the default HBase memory, we should also lower accordingly the memory specified for the YARN Node Manager service to avoid incurring in Out Of Memory errors. Please note that either if you\u2019re just installing HBase, it might still be convenient to keep some memory reserved for YARN. This can be useful as some HBase utility runs on YARN (e.g. HBase export utility). The example below highlights the configurations that should be modified in an EMR cluster while tuning the HBase heap memory. Please make sure that the sum of the YARN and HBase memory is not greater than the memory available on the node. Also make sure to keep at least 2GB of available memory for the Operating System and other internal components running on the node. [ { \"Classification\" : \"yarn-site\" , \"Properties\" : { \"yarn.scheduler.maximum-allocation-mb\" : \"MAX_MEMORY_BYTES\" , \"yarn.nodemanager.resource.memory-mb\" : \"MAX_MEMORY_BYTES\" } }, { \"Classification\" : \"hbase-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_MASTER_OPTS\" : \"\\\"$HBASE_MASTER_OPTS -Xmx30g\\\"\" , \"HBASE_REGIONSERVER_OPTS\" : \"\\\"$HBASE_REGIONSERVER_OPTS -Xmx30g\\\"\" } } ], \"Properties\" : {} } ] HBase MultiWal Provider \u00b6 By default, HBase uses a single Write Ahead Log file (WAL) per Region Server to persist mutate operations that are performed against Regions hosted on the node. This implementation can be a bottleneck as WALs are stored on the HDFS and each operation is performed sequentially against the same file. In write intensive clusters, you might increase the HBase throughput by adopting a multiwal strategy. In this scenario is recommended to have multiple disks attached to the node to get the most out of this feature. This configuration can be enabled specifying the following properties while launching an EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.wal.provider\" : \"multiwal\" , \"hbase.wal.regiongrouping.numgroups\" : \"2\" } } ] The parameter hbase.wal.regiongrouping.numgroups determines the number of WALs that will be created per Region Server. By default, this parameter is set to two, but you can tune this parameter accordingly to the number of disks attached to the node for better performance. HBase OffHeap Caching \u00b6 The following example, shows how to enable OffHeap memory caching on HBase. This configuration, can be used both when using Amazon S3 or HDFS as storage layer. The example below sets an offheap memory of 5GB while the bucket cache allocated for this memory will be 4GB. [ { \"Classification\" : \"hbase-env\" , \"Properties\" : {}, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_OFFHEAPSIZE\" : \"5G\" }, \"Configurations\" : [] } ] }, { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.bucketcache.size\" : \"4096\" , \"hbase.bucketcache.ioengine\" : \"offheap\" } } ] In order to use the configured cache, make sure to enable the following configurations in the tables you want to cache. For example, from the HBase shell: # creating new table t with column family info0 hbase> create 't' , { NAME = > 'info0' , CONFIGURATION = > { CACHE_DATA_IN_L1 = > 'true' }} # modify existing table t with column family info0 hbase> alter 't' , { NAME = > 'info0' , CONFIGURATION = > { CACHE_DATA_IN_L1 = > 'true' }}","title":"Best Practice"},{"location":"applications/hbase/best_practice/#best-practice","text":"The following section describes some general HBase tuning and best practice that can be applied both when using HDFS or Amazon S3 as storage layer for HBase.","title":"Best Practice"},{"location":"applications/hbase/best_practice/#emr-multi-master","text":"When working with HBase on Amazon EMR, it is good practice to enable the EMR Multi Master feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down. Nevertheless, this functionality is highly recommended both when using HDFS or S3 as storage layer for your HBase cluster. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason (e.g. human error, hardware impairment, etc.), it will not be possible to recover any data from the HDFS storage on the cluster as the HDFS metadata will be lost after the termination of the EMR master.","title":"EMR Multi Master"},{"location":"applications/hbase/best_practice/#emr-termination-protection","text":"Using termination protection in Amazon EMR is highly recommended both when using HDFS or Amazon S3 for your HBase cluster. Amazon EMR periodically checks the Apache Hadoop YARN status of nodes running on CORE and TASK nodes in a cluster. The health status is reported by the YARN NodeManager health checker service . If a node reports an UNHEALTHY status, it will not be possible to allocate YARN containers to it until it becomes healthy again. A common reason for unhealthy nodes is that disk utilization goes above 90%. If the node stays in this state for more than 45 minutes and Termination Protection is disabled, the EMR service terminates the node and launch a fresh new one as replacement. When a node is in an UNHEALTHY state, with the termination protection enabled the nodes will not be terminated and replaced by the EMR service. This prevents to lose HDFS data blocks in case the utilization of the disks of a CORE node goes above 90%, so preventing data integrity issues in HBase tables.","title":"EMR Termination Protection"},{"location":"applications/hbase/best_practice/#hbase-rpc-listeners","text":"One of the most important parameters to configure in your HBase cluster is the number of active RPC listeners defined per Region Server. Tuning the parameter hbase.regionserver.handler.count (default: 30) can increase the number of requests that you can concurrently serve in each region server and so the overall throughput of your cluster. To modify the default number of RPC listeners you can use the following EMR configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.handler.count\" : \"120\" } } ] However, please be mindful that this parameter should be tuned accordingly to the average size of data stored or retrieved from your tables. As rule of thumb, you should increase this number when the payload of your data is lower than 100KB, while you should stick to the default, or decrease it when the payload size is >= 1MB. For small payloads (<= 1KB), you can push this value up to 4 times the number of vCpu available in your Region Servers. To determine the average payload of data stored in your tables, see Determine average row size .","title":"HBase RPC Listeners"},{"location":"applications/hbase/best_practice/#hbase-heap-memory","text":"On Amazon EMR, when you install HBase, the memory will be evenly re-partitioned between Hadoop YARN and HBase services. For a list of the default memory settings used per instance type see Task configuration in the EMR documentation. However, when working with HBase it might be convenient to override the default parameters and increase the available memory for our HBase services. This might be required if we want to host a higher number of Regions per Region Server. To modify the default memory, you should modify the HBase environmental variables defined in the hbase-env which defines the default heap memory available for each HBase service. The following list highlight the variables that should be modified by service: HBASE_MASTER_OPTS JVM options for the HBase master HBASE_REGIONSERVER_OPTS JVM options for the HBase Region Servers HBASE_THRIFT_OPTS JVM options for the HBase Thrift service HBASE_REST_OPTS JVM options for the HBase REST service It\u2019s best practice to modify the memory of each component using its own dedicated variable, rather than using the more general HBASE_OPTS , which is used to apply common JVM options across all HBase services. To override the default memory we should specify the following java parameter in our environmental variable: -Xmx<size>[g|G|m|M|k|K] . Please also make sure to add a self reference in the environmental variable to avoid loosing other parameters that are set in the script. Besides, if we modify the default HBase memory, we should also lower accordingly the memory specified for the YARN Node Manager service to avoid incurring in Out Of Memory errors. Please note that either if you\u2019re just installing HBase, it might still be convenient to keep some memory reserved for YARN. This can be useful as some HBase utility runs on YARN (e.g. HBase export utility). The example below highlights the configurations that should be modified in an EMR cluster while tuning the HBase heap memory. Please make sure that the sum of the YARN and HBase memory is not greater than the memory available on the node. Also make sure to keep at least 2GB of available memory for the Operating System and other internal components running on the node. [ { \"Classification\" : \"yarn-site\" , \"Properties\" : { \"yarn.scheduler.maximum-allocation-mb\" : \"MAX_MEMORY_BYTES\" , \"yarn.nodemanager.resource.memory-mb\" : \"MAX_MEMORY_BYTES\" } }, { \"Classification\" : \"hbase-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_MASTER_OPTS\" : \"\\\"$HBASE_MASTER_OPTS -Xmx30g\\\"\" , \"HBASE_REGIONSERVER_OPTS\" : \"\\\"$HBASE_REGIONSERVER_OPTS -Xmx30g\\\"\" } } ], \"Properties\" : {} } ]","title":"HBase Heap Memory"},{"location":"applications/hbase/best_practice/#hbase-multiwal-provider","text":"By default, HBase uses a single Write Ahead Log file (WAL) per Region Server to persist mutate operations that are performed against Regions hosted on the node. This implementation can be a bottleneck as WALs are stored on the HDFS and each operation is performed sequentially against the same file. In write intensive clusters, you might increase the HBase throughput by adopting a multiwal strategy. In this scenario is recommended to have multiple disks attached to the node to get the most out of this feature. This configuration can be enabled specifying the following properties while launching an EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.wal.provider\" : \"multiwal\" , \"hbase.wal.regiongrouping.numgroups\" : \"2\" } } ] The parameter hbase.wal.regiongrouping.numgroups determines the number of WALs that will be created per Region Server. By default, this parameter is set to two, but you can tune this parameter accordingly to the number of disks attached to the node for better performance.","title":"HBase MultiWal Provider"},{"location":"applications/hbase/best_practice/#hbase-offheap-caching","text":"The following example, shows how to enable OffHeap memory caching on HBase. This configuration, can be used both when using Amazon S3 or HDFS as storage layer. The example below sets an offheap memory of 5GB while the bucket cache allocated for this memory will be 4GB. [ { \"Classification\" : \"hbase-env\" , \"Properties\" : {}, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_OFFHEAPSIZE\" : \"5G\" }, \"Configurations\" : [] } ] }, { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.bucketcache.size\" : \"4096\" , \"hbase.bucketcache.ioengine\" : \"offheap\" } } ] In order to use the configured cache, make sure to enable the following configurations in the tables you want to cache. For example, from the HBase shell: # creating new table t with column family info0 hbase> create 't' , { NAME = > 'info0' , CONFIGURATION = > { CACHE_DATA_IN_L1 = > 'true' }} # modify existing table t with column family info0 hbase> alter 't' , { NAME = > 'info0' , CONFIGURATION = > { CACHE_DATA_IN_L1 = > 'true' }}","title":"HBase OffHeap Caching"},{"location":"applications/hbase/best_practice_hdfs/","text":"Best Practice for HDFS \u00b6 This section highlights some of the features / best practice that you could use to improve the performance in your cluster when using HDFS as storage layer for HBase. HDFS - Name Node memory \u00b6 When handling large cluster deployments, it\u2019s important to properly size the HDFS NameNode (NN) heap memory which Amazon EMR set accordingly to the instance used . The NN keeps in memory metadata for each file / block allocated in the HDFS, so it\u2019s important to properly size the memory to prevent failures that might create down-times in our services. To size the NN memory, we can consider that each HDFS block persisted in memory uses approximately 150 bytes. Using this value as reference, you can do a rough estimate of the memory required to store data in the HDFS, considering that a block is 128MB (please note that a file smaller than the HDFS block size will still count as a individual block in memory). As alternative, you can use a rule of thumb and specify 1GB of memory each 1 million blocks stored in the HDFS. To change the default NN memory, you can use the following EMR Configuration : [ { \"Classification\" : \"hadoop-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_NAMENODE_HEAPSIZE\" : \"8192\" } } ], \"Properties\" : {} } ] HDFS - Service Threads \u00b6 Amazon EMR already configures most of the HDFS parameters that are required to get good HDFS performance for HBase. However, if you\u2019re using a large instance with several vCpu, you might benefit in increasing the number of service threads that are available for the HDFS DataNode service. Please note that if you\u2019re using HDFS - Short Circuit Reads you might not get any additional benefits from this parameter tuning, but this might still be handy if your HDFS is used by other applications. In this case, setting the dfs.datanode.handler.count to 3 times the number of vCpu available on the node can be a good starting point. In the same way we can also tune the number of dfs.namenode.handler.count for larger cluster installations. For this last parameter, you can use the following formula to determine a good value for your cluster 20 * log 2 ( number of CORE nodes ) Please note that this value might be useful to increase, if you have more than 20 CORE nodes provisioned in the cluster, otherwise you might stick to the default values set by the service. Also for both dfs.namenode.handler.count and dfs.datanode.handler.count you should not set a value higher than 200. [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.namenode.handler.count\" : \"64\" , \"dfs.datanode.handler.count\" : \"48\" } } ] HDFS - Short Circuit Reads \u00b6 In HDFS, reads normally go through the Data Node service. When the client asks the Data Node to read a file, the service reads that file off of the disk and sends the data to the client over a TCP socket. The \"short-circuit reads\" bypass the Data Node, allowing the client to read the file directly. This is only possible in cases where the client is co-located with the data. The following configurations allow HBase to directly read store files on the local node bypassing the HDFS service providing better performance while accessing data not cached. [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.client.read.shortcircuit\" : \"true\" , \"dfs.client.socket-timeout\" : \"60000\" , \"dfs.domain.socket.path\" : \"/var/run/hadoop-hdfs/dn_socket\" } } ] For additional details, see HDFS Short-Circuit Local Reads HDFS - Replication Factor \u00b6 As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API. If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation. HBase - Hedged Reads \u00b6 Hadoop 2.4 introduced a new feature called Hedged Reads. If a read from a block is slow, the HDFS client starts up another parallel read against a different block replica. The result of whichever read returns first is used, and the outstanding read is cancelled. This feature helps in situations where a read occasionally takes a long time rather than when there is a systemic problem. Hedged reads can be enabled for HBase when the HFiles are stored in HDFS. This feature is disabled by default. To enable hedged reads, set dfs.client.hedged.read.threadpool.size to the number of threads to dedicate to running hedged threads, and dfs.client.hedged.read.threshold.millis to the number of milliseconds to wait before starting another read against a different block replica. The following is an example configuration to enable hedged reads using EMR Configurations: [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.client.hedged.read.threadpool.size\" : \"20\" , \"dfs.client.hedged.read.threshold.millis\" : \"100\" } } ] HBase - Tiered Storage \u00b6 HBase can take advantage of the Heterogeneous Storage and Archival Storage feature available in the HDFS to store more efficiently data in different type of storage and provide better performance. One of the use case where this setup might be useful, is for write intensive clusters that have a high ingestion rate and trigger a lot of internal compaction operations. In this case we can define a policy to store HBase WALs on SSD disks present in our nodes (NVMe instance store volumes), while storing HFiles on additional EBS volumes attached to our instances. Please note that this is an advanced configuration that requires additional steps to be enabled on an EMR cluster and might not be beneficial for small clusters with simple ingestion patterns. Amazon EMR automatically configures both instances volumes stores and EBS disks that are defined while launching the cluster. However, we need to label the volumes attached to our node to specify the corresponding Storage Type for the corresponding volume. The first step is to attach a Bootstrap Action while launching the cluster to label NVMe disks. You can use the following script to label as SSD the NVMe disks attached to the cluster's nodes. #!/bin/bash #=============================================================================== #!# script: emr-ba-disk_labels.sh #!# version: v0.1 #!# #!# This Bootstrap Action can be attached to an EMR Cluster to automatically #!# tag NVMe Disks using the HDFS Storage Type SSD. #!# #=============================================================================== #?# #?# usage: ./emr-ba-disk_labels.sh #?# #=============================================================================== # Force the script to run as root if [ $( id -u ) ! = \"0\" ] then sudo \" $0 \" \" $@ \" exit $? fi ## Install nvme-cli yum install -y nvme-cli cd /tmp && wget -O epel.rpm \u2013nv https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y ./epel.rpm && yum -y install xmlstarlet ## List NVMe disks nvme_disks =( $( nvme list | grep \"Amazon EC2 NVMe Instance Storage\" | awk -F '[[:space:]][[:space:]]+' '{print $1}' ) ) ## If there's no nvme exit [[ ${# nvme_disks [@] } -eq 0 ]] && echo \"No EC2 NVMe Instance Storage found. End script...\" && exit 0 SCRIPT_NAME = \"/tmp/disk_labels.sh\" cat << 'EOF' > $SCRIPT_NAME #!/bin/bash # retrieve dfs.data.dir value HDFS_CORE_SITE=\"/etc/hadoop/conf/hdfs-site.xml\" nvme_disks=($(nvme list | grep \"Amazon EC2 NVMe Instance Storage\" | awk -F'[[:space:]][[:space:]]+' '{print $1}')) for disk in \"${nvme_disks[@]}\"; do # Find corresponding mounted partition mount_path=$(mount | grep \"$disk\" | awk -F'[[:space:]]' '{print $3}') echo \"Apply Hadoop Storaget Type Label [SSD] to $disk ($mount_path)\" curr_value=$(xmlstarlet sel -t -v '//configuration/property[name = \"dfs.data.dir\"]/value' $HDFS_CORE_SITE) echo \"current: $curr_value\" new_value=$(echo $curr_value | sed \"s|$mount_path|[SSD]$mount_path|g\") echo \"new: $new_value\" xmlstarlet ed -L -u \"/configuration/property[name='dfs.data.dir']/value\" -v \"$new_value\" $HDFS_CORE_SITE done systemctl restart hadoop-hdfs-datanode.service exit 0 EOF chmod +x $SCRIPT_NAME sed -i \"s|null &|null \\&\\& bash $SCRIPT_NAME >> \\$STDOUT_LOG 2>> \\$STDERR_LOG 0</dev/null \\&|\" /usr/share/aws/emr/node-provisioner/bin/provision-node exit 0 Once done, we can specify the following HBase configuration in the hbase-site in order to store our WALs files on SSD disks only. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.wal.storage.policy\" : \"ALL_SSD\" } } ] By doing this, WALs will be allocated and persisted on the HDFS using disks that have been labeled as SSD. To verify the setup, you can run the following command from the EMR master node that will display the corresponding allocations on the blocks on the HDFS for WALs. # Describe block allocation for hbase root dir hdfs fsck /user/hbase/WALs -files -blocks -locations # Sample output /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746301597 135252162 bytes, replicated: replication = 1 , 1 block ( s ) : OK 0 . BP-581531277-172.31.3.43-1674746228762:blk_1073741836_1012 len = 135252162 Live_repl = 1 [ DatanodeInfoWithStorage [ 172 .31.3.138:9866,DS-5ef6e227-738d-4cb5-9fc9-4d636744674d,SSD ]] /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746426864 135213883 bytes, replicated: replication = 1 , 1 block ( s ) : OK 0 . BP-581531277-172.31.3.43-1674746228762:blk_1073742073_1255 len = 135213883 Live_repl = 1 [ DatanodeInfoWithStorage [ 172 .31.3.138:9866,DS-bf9acb8e-ad9f-4757-a8cb-59b9d1d0e659,SSD ]] Based on this example, you can create more complex scenarios depending on the volumes attached to the nodes. HBase also provides another useful feature called Heterogeneous Storage for Date Tiered Compaction to better handle cold and hot data separation. However, this feature has been introduced in the newer HBase 3.x versions only. Summary \u00b6 The following summarize a minimal set of configurations you can tune to improve the performance on an HDFS cluster. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.handler.count\" : \"120\" } }, { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.namenode.handler.count\" : \"64\" , \"dfs.datanode.handler.count\" : \"48\" , \"dfs.client.hedged.read.threadpool.size\" : \"20\" , \"dfs.client.hedged.read.threshold.millis\" : \"100\" , \"dfs.client.read.shortcircuit\" : \"true\" , \"dfs.client.socket-timeout\" : \"60000\" , \"dfs.domain.socket.path\" : \"/var/run/hadoop-hdfs/dn_socket\" } } ]","title":"Best Practice for HDFS"},{"location":"applications/hbase/best_practice_hdfs/#best-practice-for-hdfs","text":"This section highlights some of the features / best practice that you could use to improve the performance in your cluster when using HDFS as storage layer for HBase.","title":"Best Practice for HDFS"},{"location":"applications/hbase/best_practice_hdfs/#hdfs-name-node-memory","text":"When handling large cluster deployments, it\u2019s important to properly size the HDFS NameNode (NN) heap memory which Amazon EMR set accordingly to the instance used . The NN keeps in memory metadata for each file / block allocated in the HDFS, so it\u2019s important to properly size the memory to prevent failures that might create down-times in our services. To size the NN memory, we can consider that each HDFS block persisted in memory uses approximately 150 bytes. Using this value as reference, you can do a rough estimate of the memory required to store data in the HDFS, considering that a block is 128MB (please note that a file smaller than the HDFS block size will still count as a individual block in memory). As alternative, you can use a rule of thumb and specify 1GB of memory each 1 million blocks stored in the HDFS. To change the default NN memory, you can use the following EMR Configuration : [ { \"Classification\" : \"hadoop-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_NAMENODE_HEAPSIZE\" : \"8192\" } } ], \"Properties\" : {} } ]","title":"HDFS - Name Node memory"},{"location":"applications/hbase/best_practice_hdfs/#hdfs-service-threads","text":"Amazon EMR already configures most of the HDFS parameters that are required to get good HDFS performance for HBase. However, if you\u2019re using a large instance with several vCpu, you might benefit in increasing the number of service threads that are available for the HDFS DataNode service. Please note that if you\u2019re using HDFS - Short Circuit Reads you might not get any additional benefits from this parameter tuning, but this might still be handy if your HDFS is used by other applications. In this case, setting the dfs.datanode.handler.count to 3 times the number of vCpu available on the node can be a good starting point. In the same way we can also tune the number of dfs.namenode.handler.count for larger cluster installations. For this last parameter, you can use the following formula to determine a good value for your cluster 20 * log 2 ( number of CORE nodes ) Please note that this value might be useful to increase, if you have more than 20 CORE nodes provisioned in the cluster, otherwise you might stick to the default values set by the service. Also for both dfs.namenode.handler.count and dfs.datanode.handler.count you should not set a value higher than 200. [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.namenode.handler.count\" : \"64\" , \"dfs.datanode.handler.count\" : \"48\" } } ]","title":"HDFS - Service Threads"},{"location":"applications/hbase/best_practice_hdfs/#hdfs-short-circuit-reads","text":"In HDFS, reads normally go through the Data Node service. When the client asks the Data Node to read a file, the service reads that file off of the disk and sends the data to the client over a TCP socket. The \"short-circuit reads\" bypass the Data Node, allowing the client to read the file directly. This is only possible in cases where the client is co-located with the data. The following configurations allow HBase to directly read store files on the local node bypassing the HDFS service providing better performance while accessing data not cached. [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.client.read.shortcircuit\" : \"true\" , \"dfs.client.socket-timeout\" : \"60000\" , \"dfs.domain.socket.path\" : \"/var/run/hadoop-hdfs/dn_socket\" } } ] For additional details, see HDFS Short-Circuit Local Reads","title":"HDFS - Short Circuit Reads"},{"location":"applications/hbase/best_practice_hdfs/#hdfs-replication-factor","text":"As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API. If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation.","title":"HDFS - Replication Factor"},{"location":"applications/hbase/best_practice_hdfs/#hbase-hedged-reads","text":"Hadoop 2.4 introduced a new feature called Hedged Reads. If a read from a block is slow, the HDFS client starts up another parallel read against a different block replica. The result of whichever read returns first is used, and the outstanding read is cancelled. This feature helps in situations where a read occasionally takes a long time rather than when there is a systemic problem. Hedged reads can be enabled for HBase when the HFiles are stored in HDFS. This feature is disabled by default. To enable hedged reads, set dfs.client.hedged.read.threadpool.size to the number of threads to dedicate to running hedged threads, and dfs.client.hedged.read.threshold.millis to the number of milliseconds to wait before starting another read against a different block replica. The following is an example configuration to enable hedged reads using EMR Configurations: [ { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.client.hedged.read.threadpool.size\" : \"20\" , \"dfs.client.hedged.read.threshold.millis\" : \"100\" } } ]","title":"HBase - Hedged Reads"},{"location":"applications/hbase/best_practice_hdfs/#hbase-tiered-storage","text":"HBase can take advantage of the Heterogeneous Storage and Archival Storage feature available in the HDFS to store more efficiently data in different type of storage and provide better performance. One of the use case where this setup might be useful, is for write intensive clusters that have a high ingestion rate and trigger a lot of internal compaction operations. In this case we can define a policy to store HBase WALs on SSD disks present in our nodes (NVMe instance store volumes), while storing HFiles on additional EBS volumes attached to our instances. Please note that this is an advanced configuration that requires additional steps to be enabled on an EMR cluster and might not be beneficial for small clusters with simple ingestion patterns. Amazon EMR automatically configures both instances volumes stores and EBS disks that are defined while launching the cluster. However, we need to label the volumes attached to our node to specify the corresponding Storage Type for the corresponding volume. The first step is to attach a Bootstrap Action while launching the cluster to label NVMe disks. You can use the following script to label as SSD the NVMe disks attached to the cluster's nodes. #!/bin/bash #=============================================================================== #!# script: emr-ba-disk_labels.sh #!# version: v0.1 #!# #!# This Bootstrap Action can be attached to an EMR Cluster to automatically #!# tag NVMe Disks using the HDFS Storage Type SSD. #!# #=============================================================================== #?# #?# usage: ./emr-ba-disk_labels.sh #?# #=============================================================================== # Force the script to run as root if [ $( id -u ) ! = \"0\" ] then sudo \" $0 \" \" $@ \" exit $? fi ## Install nvme-cli yum install -y nvme-cli cd /tmp && wget -O epel.rpm \u2013nv https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y ./epel.rpm && yum -y install xmlstarlet ## List NVMe disks nvme_disks =( $( nvme list | grep \"Amazon EC2 NVMe Instance Storage\" | awk -F '[[:space:]][[:space:]]+' '{print $1}' ) ) ## If there's no nvme exit [[ ${# nvme_disks [@] } -eq 0 ]] && echo \"No EC2 NVMe Instance Storage found. End script...\" && exit 0 SCRIPT_NAME = \"/tmp/disk_labels.sh\" cat << 'EOF' > $SCRIPT_NAME #!/bin/bash # retrieve dfs.data.dir value HDFS_CORE_SITE=\"/etc/hadoop/conf/hdfs-site.xml\" nvme_disks=($(nvme list | grep \"Amazon EC2 NVMe Instance Storage\" | awk -F'[[:space:]][[:space:]]+' '{print $1}')) for disk in \"${nvme_disks[@]}\"; do # Find corresponding mounted partition mount_path=$(mount | grep \"$disk\" | awk -F'[[:space:]]' '{print $3}') echo \"Apply Hadoop Storaget Type Label [SSD] to $disk ($mount_path)\" curr_value=$(xmlstarlet sel -t -v '//configuration/property[name = \"dfs.data.dir\"]/value' $HDFS_CORE_SITE) echo \"current: $curr_value\" new_value=$(echo $curr_value | sed \"s|$mount_path|[SSD]$mount_path|g\") echo \"new: $new_value\" xmlstarlet ed -L -u \"/configuration/property[name='dfs.data.dir']/value\" -v \"$new_value\" $HDFS_CORE_SITE done systemctl restart hadoop-hdfs-datanode.service exit 0 EOF chmod +x $SCRIPT_NAME sed -i \"s|null &|null \\&\\& bash $SCRIPT_NAME >> \\$STDOUT_LOG 2>> \\$STDERR_LOG 0</dev/null \\&|\" /usr/share/aws/emr/node-provisioner/bin/provision-node exit 0 Once done, we can specify the following HBase configuration in the hbase-site in order to store our WALs files on SSD disks only. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.wal.storage.policy\" : \"ALL_SSD\" } } ] By doing this, WALs will be allocated and persisted on the HDFS using disks that have been labeled as SSD. To verify the setup, you can run the following command from the EMR master node that will display the corresponding allocations on the blocks on the HDFS for WALs. # Describe block allocation for hbase root dir hdfs fsck /user/hbase/WALs -files -blocks -locations # Sample output /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746301597 135252162 bytes, replicated: replication = 1 , 1 block ( s ) : OK 0 . BP-581531277-172.31.3.43-1674746228762:blk_1073741836_1012 len = 135252162 Live_repl = 1 [ DatanodeInfoWithStorage [ 172 .31.3.138:9866,DS-5ef6e227-738d-4cb5-9fc9-4d636744674d,SSD ]] /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746426864 135213883 bytes, replicated: replication = 1 , 1 block ( s ) : OK 0 . BP-581531277-172.31.3.43-1674746228762:blk_1073742073_1255 len = 135213883 Live_repl = 1 [ DatanodeInfoWithStorage [ 172 .31.3.138:9866,DS-bf9acb8e-ad9f-4757-a8cb-59b9d1d0e659,SSD ]] Based on this example, you can create more complex scenarios depending on the volumes attached to the nodes. HBase also provides another useful feature called Heterogeneous Storage for Date Tiered Compaction to better handle cold and hot data separation. However, this feature has been introduced in the newer HBase 3.x versions only.","title":"HBase - Tiered Storage"},{"location":"applications/hbase/best_practice_hdfs/#summary","text":"The following summarize a minimal set of configurations you can tune to improve the performance on an HDFS cluster. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.handler.count\" : \"120\" } }, { \"Classification\" : \"hdfs-site\" , \"Properties\" : { \"dfs.namenode.handler.count\" : \"64\" , \"dfs.datanode.handler.count\" : \"48\" , \"dfs.client.hedged.read.threadpool.size\" : \"20\" , \"dfs.client.hedged.read.threshold.millis\" : \"100\" , \"dfs.client.read.shortcircuit\" : \"true\" , \"dfs.client.socket-timeout\" : \"60000\" , \"dfs.domain.socket.path\" : \"/var/run/hadoop-hdfs/dn_socket\" } } ]","title":"Summary"},{"location":"applications/hbase/best_practice_s3/","text":"Best Practice for Amazon S3 \u00b6 This section highlights some of the features / best practice that you can use to improve the performance in your cluster when using Amazon S3 as storage layer for HBase. For additional best practice / tuning parameters, see Apache HBase on Amazon S3 configuration properties . HBase - Speed up region assignment / opening / closing \u00b6 HBase 1.x Set the below configurations to speed up region assignment, opening and closure on HBase 1.x clusters. These configurations specifically disable the use of zookeeper for the region assignment by setting to false the property hbase.assignment.usezk . Additionally, you can increase the thread pools the Region Servers use for opening the assigned regions. For Regions Servers handling many regions (in the order of thousands), you can set the thread pools up to 10 times the available number of vCpu on the Region Server. Below, an example EMR Configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.assignment.usezk\" : \"false\" , \"hbase.regionserver.executor.openregion.threads\" : \"120\" , \"hbase.regionserver.executor.closeregion.threads\" : \"120\" } } ] HBase 2.x HBase 2.x introduced a more robust and efficient workflow to manage regions transitions which leverage the ProcedureV2 introduced in HBASE-14614 . In this case, it is only sufficient to increase the default region server thread pools to speed up the initialization of the regions. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.executor.openregion.threads\" : \"120\" , \"hbase.regionserver.executor.closeregion.threads\" : \"120\" } } ] HBase - Bucket Cache \u00b6 When using Amazon S3 as storage layer for HBase, EMR configures the service to use a Bucket Cache for persisting data blocks on the L2 Cache of each region server. The default cache implementation used for Amazon S3 persists blocks on the local volumes of the node as defined by the hbase.bucketcache.ioengine property. This parameter defines the location of the files used to store the cached data. For example, the following snippet shows the default configurations for a node with 4 EBS volumes attached. <property> <name> hbase.bucketcache.ioengine </name> <value> files:/mnt1/hbase/bucketcache,/mnt2/hbase/bucketcache,/mnt3/hbase/bucketcache </value> </property> By default, EMR configures N - 1 volumes for caching data, so in our example only 3 volumes out of 4 will be used for the cache. This feature can be useful to persist HOT data on the local disks of the cluster to reduce the latency introduced when accessing HFiles stored on S3. However, by default the cache size is set as 8GB, so you might need to increase it depending on the amount of data you want to store on each node. To modify the default cache value, you can set the following property: hbase.bucketcache.size: 98304 # defined as MB In the above example, we set the cache size for each node to 98GB. In each volume only 32GB (98304 / 3) are used, as the total cache size will be evenly distributed across the volumes defined in the hbase.bucketcache.ioengine . Besides, when using S3 it might be convenient to pre-warm the cache during the region opening to avoid performance degradation when the cache is still not fully initialized. In this case to enable blocks prefetch, you should enable the following configuration. hbase.rs.prefetchblocksonopen: true This configuration can also be set for individual Column Family of an HBase table. In this case you should specify the configuration through the HBase shell using the following command: hbase> create 'MyTable', { NAME => 'myCF', PREFETCH_BLOCKS_ON_OPEN => 'true' } Finally, in write intensive use cases, it might be useful to also enable the following configurations to automatically persist blocks in the cache as they are written, and to repopulate the cache following a compaction (compaction operations invalidate cache blocks). In this case we can set the following additional properties: hbase.rs.cacheblocksonwrite: true hbase.rs.cachecompactedblocksonwrite: true Below a sample configuration to tune the Bucket Cache in an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.bucketcache.size\" : \"98304\" , \"hbase.rs.prefetchblocksonopen\" : \"true\" , \"hbase.rs.cacheblocksonwrite\" : \"true\" , \"hbase.rs.cachecompactedblocksonwrite\" : \"true\" } } ] HBase - Memstore flush size \u00b6 When using Amazon S3 in HBase, it might be convenient to increase the default memstore flush size to avoid performance degradation, or an excessive number of small compaction operations in write intensive clusters. This can be useful if you have manually disabled the HBase - Persistent File Tracking feature that is enabled on EMR greater than 6.2.0 or if you're using an EMR 5.x cluster. In this case, you can increase the memstore flush size to 256MB or 512MB (default 128MB). Below an example of how you can change this configuration in an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.hregion.memstore.flush.size\" : \"268435456\" # 256 * 1024 * 1024 } } ] HBase - Region Split Policy \u00b6 Depending on the HBase version that you\u2019re using, you will use different region split policies. By default, you\u2019ll have: HBase 1.x org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy HBase 2.x org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy These specific implementations aims to quickly increase the number of regions when you have a fresh new table that wasn\u2019t pre-partitioned. This might be a good strategy for new tables in a cluster. However, it might be more convenient for a cluster using S3 as storage layer to use the old split strategy org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy that performs a split operation only when the overall size of a region goes above a threshold as defined by the parameter: hbase.hregion.max.filesize (default: 10GB) This can help if you want to have more control on the number of regions, as it will allow you to control the growth of the number of regions by a fixed size that you specify. Additionally, this can also be handy in case you\u2019re leveraging Apache Phoenix to query HBase and you have a constant flow of new data. Setting a constant size region split policy will prevent excessive splitting operations. These operations can cause temporary region cache boundaries exceptions while using Phoenix, due to the time required to refresh internal metadata about regions boundaries. This problem might be more frequent when using S3 as storage layer than when using HDFS. Below an example to modify the Region Server split logic on an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.region.split.policy\" : \"org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy\" , \"hbase.hregion.max.filesize\" : \"10737418240\" } } ] HBase - Persistent File Tracking \u00b6 When using EMR versions greater than 6.2.0, EMR will enable a feature called Persistent File Tracking when using Amazon S3 as storage layer. This specific feature, is enabled by default and provides performance benefits as it avoids HFile rename operations that might delay write operations due to S3 latencies. However, please note that this feature does not support the native HBase replication feature. So if you want to use replication to implement a Highly Available setup when using Amazon S3, you\u2019ll have to disable this feature. This applies only to S3 and is not required when using HDFS as storage layer. For more details on this feature, see Persistent HFile tracking .","title":"Best Practice for Amazon S3"},{"location":"applications/hbase/best_practice_s3/#best-practice-for-amazon-s3","text":"This section highlights some of the features / best practice that you can use to improve the performance in your cluster when using Amazon S3 as storage layer for HBase. For additional best practice / tuning parameters, see Apache HBase on Amazon S3 configuration properties .","title":"Best Practice for Amazon S3"},{"location":"applications/hbase/best_practice_s3/#hbase-speed-up-region-assignment-opening-closing","text":"HBase 1.x Set the below configurations to speed up region assignment, opening and closure on HBase 1.x clusters. These configurations specifically disable the use of zookeeper for the region assignment by setting to false the property hbase.assignment.usezk . Additionally, you can increase the thread pools the Region Servers use for opening the assigned regions. For Regions Servers handling many regions (in the order of thousands), you can set the thread pools up to 10 times the available number of vCpu on the Region Server. Below, an example EMR Configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.assignment.usezk\" : \"false\" , \"hbase.regionserver.executor.openregion.threads\" : \"120\" , \"hbase.regionserver.executor.closeregion.threads\" : \"120\" } } ] HBase 2.x HBase 2.x introduced a more robust and efficient workflow to manage regions transitions which leverage the ProcedureV2 introduced in HBASE-14614 . In this case, it is only sufficient to increase the default region server thread pools to speed up the initialization of the regions. [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.executor.openregion.threads\" : \"120\" , \"hbase.regionserver.executor.closeregion.threads\" : \"120\" } } ]","title":"HBase - Speed up region assignment / opening / closing"},{"location":"applications/hbase/best_practice_s3/#hbase-bucket-cache","text":"When using Amazon S3 as storage layer for HBase, EMR configures the service to use a Bucket Cache for persisting data blocks on the L2 Cache of each region server. The default cache implementation used for Amazon S3 persists blocks on the local volumes of the node as defined by the hbase.bucketcache.ioengine property. This parameter defines the location of the files used to store the cached data. For example, the following snippet shows the default configurations for a node with 4 EBS volumes attached. <property> <name> hbase.bucketcache.ioengine </name> <value> files:/mnt1/hbase/bucketcache,/mnt2/hbase/bucketcache,/mnt3/hbase/bucketcache </value> </property> By default, EMR configures N - 1 volumes for caching data, so in our example only 3 volumes out of 4 will be used for the cache. This feature can be useful to persist HOT data on the local disks of the cluster to reduce the latency introduced when accessing HFiles stored on S3. However, by default the cache size is set as 8GB, so you might need to increase it depending on the amount of data you want to store on each node. To modify the default cache value, you can set the following property: hbase.bucketcache.size: 98304 # defined as MB In the above example, we set the cache size for each node to 98GB. In each volume only 32GB (98304 / 3) are used, as the total cache size will be evenly distributed across the volumes defined in the hbase.bucketcache.ioengine . Besides, when using S3 it might be convenient to pre-warm the cache during the region opening to avoid performance degradation when the cache is still not fully initialized. In this case to enable blocks prefetch, you should enable the following configuration. hbase.rs.prefetchblocksonopen: true This configuration can also be set for individual Column Family of an HBase table. In this case you should specify the configuration through the HBase shell using the following command: hbase> create 'MyTable', { NAME => 'myCF', PREFETCH_BLOCKS_ON_OPEN => 'true' } Finally, in write intensive use cases, it might be useful to also enable the following configurations to automatically persist blocks in the cache as they are written, and to repopulate the cache following a compaction (compaction operations invalidate cache blocks). In this case we can set the following additional properties: hbase.rs.cacheblocksonwrite: true hbase.rs.cachecompactedblocksonwrite: true Below a sample configuration to tune the Bucket Cache in an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.bucketcache.size\" : \"98304\" , \"hbase.rs.prefetchblocksonopen\" : \"true\" , \"hbase.rs.cacheblocksonwrite\" : \"true\" , \"hbase.rs.cachecompactedblocksonwrite\" : \"true\" } } ]","title":"HBase - Bucket Cache"},{"location":"applications/hbase/best_practice_s3/#hbase-memstore-flush-size","text":"When using Amazon S3 in HBase, it might be convenient to increase the default memstore flush size to avoid performance degradation, or an excessive number of small compaction operations in write intensive clusters. This can be useful if you have manually disabled the HBase - Persistent File Tracking feature that is enabled on EMR greater than 6.2.0 or if you're using an EMR 5.x cluster. In this case, you can increase the memstore flush size to 256MB or 512MB (default 128MB). Below an example of how you can change this configuration in an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.hregion.memstore.flush.size\" : \"268435456\" # 256 * 1024 * 1024 } } ]","title":"HBase - Memstore flush size"},{"location":"applications/hbase/best_practice_s3/#hbase-region-split-policy","text":"Depending on the HBase version that you\u2019re using, you will use different region split policies. By default, you\u2019ll have: HBase 1.x org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy HBase 2.x org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy These specific implementations aims to quickly increase the number of regions when you have a fresh new table that wasn\u2019t pre-partitioned. This might be a good strategy for new tables in a cluster. However, it might be more convenient for a cluster using S3 as storage layer to use the old split strategy org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy that performs a split operation only when the overall size of a region goes above a threshold as defined by the parameter: hbase.hregion.max.filesize (default: 10GB) This can help if you want to have more control on the number of regions, as it will allow you to control the growth of the number of regions by a fixed size that you specify. Additionally, this can also be handy in case you\u2019re leveraging Apache Phoenix to query HBase and you have a constant flow of new data. Setting a constant size region split policy will prevent excessive splitting operations. These operations can cause temporary region cache boundaries exceptions while using Phoenix, due to the time required to refresh internal metadata about regions boundaries. This problem might be more frequent when using S3 as storage layer than when using HDFS. Below an example to modify the Region Server split logic on an Amazon EMR cluster: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.regionserver.region.split.policy\" : \"org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy\" , \"hbase.hregion.max.filesize\" : \"10737418240\" } } ]","title":"HBase - Region Split Policy"},{"location":"applications/hbase/best_practice_s3/#hbase-persistent-file-tracking","text":"When using EMR versions greater than 6.2.0, EMR will enable a feature called Persistent File Tracking when using Amazon S3 as storage layer. This specific feature, is enabled by default and provides performance benefits as it avoids HFile rename operations that might delay write operations due to S3 latencies. However, please note that this feature does not support the native HBase replication feature. So if you want to use replication to implement a Highly Available setup when using Amazon S3, you\u2019ll have to disable this feature. This applies only to S3 and is not required when using HDFS as storage layer. For more details on this feature, see Persistent HFile tracking .","title":"HBase - Persistent File Tracking"},{"location":"applications/hbase/data_integrity/","text":"Data Integrity / Disaster Recovery / High Availability \u00b6 The following document illustrates architectures and service features that can be combined to implement Highly Available architectures and Disaster Recovery solutions for HBase clusters running on Amazon EMR. The document also describes additional best practices for data recovery in case of integrity issues you might face when using HBase. Best Practice \u00b6 When working with HBase on Amazon EMR, it is good practice to enable the EMR Multimaster feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down. Please note that EMR on EC2 launches all the nodes of the cluster within the same Availability Zone, so this solution is not sufficient to create a robust setup for high available clusters. Nevertheless, this functionality is highly recommended both when using HDFS or Amazon S3 as storage layer. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason, it will not be possible to recover any data from the HDFS storage of the cluster as the HDFS metadata will be lost after the termination of the EMR master. Moreover, it is also recommended to specify a SPREAD placement group strategy that places the master instances across separate underlying hardware to guard against the loss of multiple master nodes in the event of a hardware failure. For additional details see Amazon EMR integration with EC2 placement groups In terms of cluster scale in / out, it\u2019s not recommended to enable the EMR Managed Scaling or the EMR scaling with custom policies when using HBase. These features are designed to operate with YARN workloads, so they might cause data integrity issues if some nodes are terminated by the scaling policies. In case you need to scale your HBase cluster size, you can follow the below procedures: Scale Out - Use the EMR Web Console or API to increase the number of nodes. New nodes are automatically recognized once they join the cluster, and the HBase balancer will automatically spread regions across new nodes. Scale In - Disable the HBase tables and use the EMR Web Console or API to decrease the number of nodes in the cluster. HBase on HDFS \u00b6 Within this section you can find additional information to secure your data when launching an Amazon EMR cluster using HDFS as storage layer. As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API. If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation. HBase - Snapshots \u00b6 HBase snapshots should be considered the first line of defense against data integrity issues that might cause a service disruption in your HBase cluster. To prevent any kind of data loss, is highly recommended to perform daily snapshots of your HBase tables. Please note that taking a snapshot of a table does not involve any data copy operation, so this operation doesn\u2019t generate any additional data in your HDFS storage. However, it is not recommended to maintain a high number of snapshots for a table, especially if its data change frequently. Modified regions that are used by old snapshots are preserved in the archive folder within the HBase root directory, so this can have a significant impact on the amount of data retained in the HDFS. Besides, please note that HBase snapshots are by default persisted in the same storage layer configured when launching the cluster (in this case HDFS), so they should not be considered a strong disaster recovery mechanism if you want to protect your data in case of a cluster termination. In this case, you can export the snapshots in an Amazon S3 bucket to persist all your data on a reliable storage layer. Please note that periodic snapshots exports to S3 are recommended only if your tables have a small size (less than few TB) as an HBase export will copy all the data belonging to the snapshot in the S3 bucket using a Map Reduce job. For sample scripts and commands see the related examples in the Data Migration guide. Additional Tips Use a time identifier in the snapshot name that can help you identify when the snapshot was created. The creation time is also present in the snapshot metadata, but using this convention in the name can save some time while restoring an impaired cluster. HBase - Cluster Replication \u00b6 The HBase cluster replication allows you to keep one or more HBase clusters synchronized between each other. Depending on how you set up the replication and peering between the clusters you can achieve different configurations to establish both a Disaster Recovery or a Highly Available setup depending on your needs. The following sections describe typical architectures that can be achieved with this feature. Active - Active architecture \u00b6 This first approach describes a setup that is suitable to provide Highly Available clusters that can both serve read and write requests. In this case is required to set up a two-way replication peering between the Primary and Secondary cluster as described in the below figure. In this architecture both reads and writes will be routed across the two clusters by the Elastic Load Balancer and data written in a cluster will also be replicated in the other one. To setup this architecture you should performed the following steps: Create two HBase clusters in different AWS Availability Zones Create an Elastic Load Balancer using an EC2 Target Group configured with the following specifications: Target Type : IP addresses. EMR master IP for the Primary and Secondary cluster Protocol : TCP Port: 2181. Default port used by the Zookeeper service Establish a Two Way replication peering between the two clusters. To enable the replication, you can run the following commands on each master node. While running these commands, please make sure to replace MASTER_NODE_IP with the IP address of the other master node. For example, if running the commands on the Primary the MASTER_IP should be set with the Secondary IP address. HBASE_CMD = \"sudo -u hbase hbase\" MASTER_IP = \"MASTER_NODE_IP\" PEER_NAME = \"aws\" ## Create peering with the destination cluster echo \"add_peer ' $PEER_NAME ', CLUSTER_KEY => ' $MASTER_IP :2181:/hbase'\" | $HBASE_CMD shell ## List peers in the source cluster echo \"list_peers\" | $HBASE_CMD shell Make sure the HBase tables you want to replicate are already available in both EMR clusters, If your tables need to be initialized with the same data use the HBase snapshots to make sure they contain the same data. Enable the table replication using the following snippet for each table you want to replicate on both clusters ## Enable replication TABLE_NAME = \"YOUR_TABLE_NAME\" HBASE_CMD = \"sudo -u hbase hbase\" echo \"enable_table_replication ' $TABLE_NAME '\" | $HBASE_CMD shell To leverage this setup, specify the Network Load Balancer endpoint in the hbase.zookeeper.quorum property used by your client applications. This setup can tolerate impairments of an Availability Zone within the same Region and provides the best performance if you need milliseconds / sub milliseconds responses from your clusters. Please note that by default the HBase replication is an asynchronous process executed in background, and replicates WAL data across the clusters for which the replication is enabled. This means that this feature does not guarantee strong consistency when reading data. So carefully evaluate if this meet your business needs. In case one of the two cluster is terminated or needs to be upgraded, you have to re-create the HBase peering for the new cluster and restore the table\u2019s data and metadata in the new cluster. Active - Passive architecture \u00b6 In a similar way as before, you can set up an Active / Passive architecture that can serve DR purposes. This can be useful if you want to have a backup cluster you can switch to in case of issues on the Primary one. The following picture highlights the overall architecture setup and components. In order to implement the following architecture, you can perform the steps below: Create two EMR clusters in separate Availability Zones Create a Route53 Private Hosted Zone and create an A record pointing to the EMR Master you want to act as Primary. If using the EMR Multi-Master feature, it is recommended to add all the 3 Master nodes in the record set Establish a One Way HBase replication from the EMR Primary to EMR Secondary to replicate data to the Secondary cluster. In this case, you can use the commands previously shared and execute them on the EMR Primary cluster only. Once done, specify the Route53 A record previously defined to route your client applications to the EMR Primary cluster. This architecture serves mainly to implement a DR strategy for your HBase data. However, you can still leverage the Secondary cluster as a read replica of your data to reduce read requests on the Primary EMR Cluster. However, if you want to implement this scenario, please make sure that only client applications that have to perform READ operations (e.g. SCAN, GET) connect to the Secondary EMR cluster. In case of failures on the EMR Primary cluster, you\u2019ll be able to route your client application traffic to the Secondary EMR cluster by changing the IP address in the A record defined in the Route55 Private Hosted Zone. Please note that your client applications might face some failures while the A record update takes place. Multi Region architecture \u00b6 If you have a business requirement that requires to replicate HBase data in different AWS Regions, you can still leverage the HBase cluster replication feature to synchronize data between two clusters. The setup is very similar to what previously described, but requires to establish an inter-region VPC peering between the two AWS Regions, so that HBase clusters can exchange data between each other. An example multi region Active / Active setup is depicted in the below figure. HBase on Amazon S3 \u00b6 The following section provides architectures and best practices that you can use to implement Disaster Recovery (DR) strategies and Highly Available clusters when using Amazon S3 as storage layer for your HBase clusters. Storage Classes \u00b6 When using Amazon S3 as storage layer for your HBase cluster, all the objects created in the bucket by HBase will be created using the default S3 Standard storage class. In this case your data will be redundantly stored on a minimum of three Availability Zones within the same AWS Region. This ensures that your data is still available in your HBase cluster, either if there is an impairment in one Availability Zone. If you want to maintain this level of data availability in case of AZ failures, it is not recommended to set any S3 Lifecycle configuration that might transition HBase files in a storage class that will reduce the internal S3 data replication (e.g. S3 One Zone-IA). Additional Tips Always use dedicated S3 Buckets for your HBase on S3 clusters. This minimize chances of API throttling in case other processes or applications (e.g. Spark ETL jobs) are also using the same HBase bucket. HBase - Snapshots \u00b6 Although Amazon S3 already provides native functionalities to replicate objects across multiple Availability Zones, this doesn\u2019t protect you in case of application issues that might corrupt your HBase data. In this case, is good practice to leverage HBase existing capabilities to create periodic snapshots of your tables so that you can recover / restore tables in case of HBase inconsistencies or similar data integrity issues. Apache HBase stores snapshot data (store files and metadata) in the archive and .hbase-snapshot folders within the HBase root path. When using Amazon S3 as storage layer, this data will be replicated across multiple Availability Zones as well, as their content will be stored by default in the S3 bucket. We recommend to create HBase snapshots using the same S3 bucket used while launching the cluster (default behavior). In this way, snapshots will leverage incremental capabilities during the snapshot creation thus minimizing the footprint of data stored in the bucket. Please note that exporting a HBase snapshot in a different S3 bucket or prefix, will force HBase to copy all data required by the snapshot. For this reason, if you manage large clusters (hundred of TB or PB data), it\u2019s not recommended to export snapshots in different AWS Regions or S3 Buckets using this approach. HBase - Cluster Replication \u00b6 As previously described in the HDFS section, the HBase Cluster Replication can be used to create a Highly Available cluster, or to implement different DR solutions depending on your business requirements. When using Amazon S3 as storage layer, it\u2019s important to remember that two HBase clusters cannot share the same S3 root directory, when they both receive write requests, as this might lead to data inconsistencies. For this reason, you should always use separate buckets for each individual HBase cluster, or as alternative use different prefixes within the same S3 bucket. This latest solution however is not ideal as it might increase the chances to face S3 throttling issues. Amazon EMR - Read Replica \u00b6 The EMR HBase Read Replica feature can be used to provide High Available reads for your HBase clusters using S3 as storage layer. Although this feature does not provide additional benefits for a DR recovery mechanism, it can still be useful to serve HBase read requests, in case you want to perform a Blue / Green deployment to modify a cluster configuration on the primary Amazon EMR cluster that requires the termination of the cluster (e.g. EMR release version upgrade) For additional details see Using a read-replica cluster in the Amazon EMR documentation. Amazon S3 - Object Replication \u00b6 If you want to replicate your HBase data on a backup region for DR purposes, you might leverage the Amazon S3 Object Replication feature. This can be used to replicate objects within the same AWS Region (Same-Region Replication) or in a different region (Cross-Region Replication). This approach can be used to implement a DR mechanism that allows you to launch a Secondary cluster in a different AWS Region in case you have an impairment in your Primary one. The overall architecture is described in the below figure. This architecture requires you to use a DNS mechanism (for example using Route 53 hosted zones) so that you can switch between the AWS Regions in case of failures. This approach requires the following components: Primary Amazon EMR cluster with his own dedicated S3 Bucket. Secondary Amazon EMR cluster that will only be launched in case of failures, with his own dedicated S3 bucket. The secondary cluster can be launched in the same AWS Region as the primary or in a different one depending on the requirements. Active S3 Replication between the Primary and the Secondary S3 buckets to replicate S3 objects A DNS setup that allows you to switch your HBase clients from the primary to the secondary in case of failures. For example this might be achieved using a Route 53 private hosted zones As previously described, the Secondary cluster should only be launched in case of an impairment in the Primary Region or Availability Zone failure. The cluster can be launched with the same configurations and sizes as the primary, but should point to a different S3 bucket. We also recommend to launch the secondary cluster using the Amazon EMR HBase read replica feature to be sure that no new data will be written on the secondary cluster. This prevent the secondary cluster to receive new data, but simplify the recovery after an impairment. In order to enable the S3 Object Replication, you should follow the steps below: Create two Amazon S3 buckets that will be respectively used to store production and replicated data. Make sure to enable the Amazon S3 Versioning , as this functionality is required to enable the S3 replication . In the primary bucket, create a new S3 replication rule to replicate data generated by the primary cluster. You can follow the example in the Amazon S3 documentation to enable the replication in the bucket. While creating the replication rule, make sure to adhere the following best practices: Enable the S3 Object Replication only for the HBase root prefix specified when launching the cluster. This help mitigating delay problems that might occur if you also have objects outside the HBase root prefix that should be replicated. Enable the Replication Time Control (RTC) capabilities. This feature is designed to replicate 99.99% of objects within 15 minutes after upload. Enabling this feature will also automatically enable the S3 replication metrics that are review the pending replication objects. Enable the Delete Marker Replication Additionally, is also recommend to create a Lifecycle Rule to delete expired object delete markers, incomplete multipart uploads, and non current version of files. This architecture serves mainly to implement a cost effective DR strategy for your HBase data as only one active cluster will be running. In case of failover, before switching to the secondary cluster, check the S3 replication metrics to verify there are no pending objects to be replicated. Additional Considerations Amazon EMR implements internal features that prevents the clusters to be terminated in case of Availability Zone or service issues. If your primary cluster cannot be reached, you might want to launch another cluster pointing to the same Amazon S3 bucket in a different AZ. However, this might lead to inconsistencies in case your Primary HBase cluster is not terminated as you might end up in a situation where two active HBase clusters are pointing to the same S3 root bucket. For this reason, you might want to implement the following safety measures in case of service issues: If you only require to continue supporting HBase read operations, you can launch a backup cluster pointing to the same S3 root directory until we solve the problem. As alternative, if you\u2019re not able to determine if your cluster instances are terminated (e.g. the failure also impact your ability to use the EC2 service) you might contact our Support to verify if the cluster was terminated to decide launching a new active cluster instead of just launch a HBase read replica. If you want to continue to support HBase write requests within the same Region, you\u2019ll have to leverage a backup S3 bucket where data have been replicated using the S3 Object Replication or HBase cluster replication to avoid data inconsistencies if the Primary EMR cluster has not been yet terminated. As in the previous scenario, you can also contact our Support to determine if the Primary cluster was already terminated, but this might delay the recovery time. HBase WALs \u00b6 When using Amazon S3 as storage layer, HBase still stores WALs on the local HDFS of the cluster. WALs are used internally by HBase to replay mutate operations in case of a region failure. Please note that every mutation request on HBase is first written on a WAL file, then in the HBase memstore and only after a Memstore flush this data will be persisted on S3. If the Amazon EMR cluster is terminated due to an incident, you might lose the latest data not yet persisted on S3. In this case is a good practice to leverage a persistent event store solution, like Amazon MSK or Amazon Kinesis to retain the latest ingested data, so that you\u2019ll be able to replay any missing data from the moment of the service interruption. As alternative, you can configure your HBase cluster to store WALs on a persistent storage layer as an external HDFS cluster, or an Amazon EFS filesystem. This last solution might increase the latency of your write operations on HBase so you might want to verify if this solution met your SLA requirements. To configure WALs on Amazon EFS you can use the procedure described here . If you prefer to simply leverage another HDFS cluster, you can configure the new path using the following EMR Classification when launching your HBase Cluster. [ { \"classification\" : \"hbase-site\" , \"properties\" : { \"hbase.wal.dir\" : \"hdfs://HDFS_MASTER_NODE:8020/PATH\" } } ] Please note that the PATH used in the external HDFS cluster should be pre-created before launching the cluster and should be writable by the following user and group: hbase : hbase . Additionally, if your external HDFS cluster is secured with Kerberos authentication, you also need to configure your HBase cluster with Kerberos, and both clusters should leverage the same Kerberos REALM to be able to communicate between each other. For additional information, see External KDC in the Amazon EMR documentation.","title":"Data Integrity"},{"location":"applications/hbase/data_integrity/#data-integrity-disaster-recovery-high-availability","text":"The following document illustrates architectures and service features that can be combined to implement Highly Available architectures and Disaster Recovery solutions for HBase clusters running on Amazon EMR. The document also describes additional best practices for data recovery in case of integrity issues you might face when using HBase.","title":"Data Integrity / Disaster Recovery / High Availability"},{"location":"applications/hbase/data_integrity/#best-practice","text":"When working with HBase on Amazon EMR, it is good practice to enable the EMR Multimaster feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down. Please note that EMR on EC2 launches all the nodes of the cluster within the same Availability Zone, so this solution is not sufficient to create a robust setup for high available clusters. Nevertheless, this functionality is highly recommended both when using HDFS or Amazon S3 as storage layer. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason, it will not be possible to recover any data from the HDFS storage of the cluster as the HDFS metadata will be lost after the termination of the EMR master. Moreover, it is also recommended to specify a SPREAD placement group strategy that places the master instances across separate underlying hardware to guard against the loss of multiple master nodes in the event of a hardware failure. For additional details see Amazon EMR integration with EC2 placement groups In terms of cluster scale in / out, it\u2019s not recommended to enable the EMR Managed Scaling or the EMR scaling with custom policies when using HBase. These features are designed to operate with YARN workloads, so they might cause data integrity issues if some nodes are terminated by the scaling policies. In case you need to scale your HBase cluster size, you can follow the below procedures: Scale Out - Use the EMR Web Console or API to increase the number of nodes. New nodes are automatically recognized once they join the cluster, and the HBase balancer will automatically spread regions across new nodes. Scale In - Disable the HBase tables and use the EMR Web Console or API to decrease the number of nodes in the cluster.","title":"Best Practice"},{"location":"applications/hbase/data_integrity/#hbase-on-hdfs","text":"Within this section you can find additional information to secure your data when launching an Amazon EMR cluster using HDFS as storage layer. As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API. If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation.","title":"HBase on HDFS"},{"location":"applications/hbase/data_integrity/#hbase-snapshots","text":"HBase snapshots should be considered the first line of defense against data integrity issues that might cause a service disruption in your HBase cluster. To prevent any kind of data loss, is highly recommended to perform daily snapshots of your HBase tables. Please note that taking a snapshot of a table does not involve any data copy operation, so this operation doesn\u2019t generate any additional data in your HDFS storage. However, it is not recommended to maintain a high number of snapshots for a table, especially if its data change frequently. Modified regions that are used by old snapshots are preserved in the archive folder within the HBase root directory, so this can have a significant impact on the amount of data retained in the HDFS. Besides, please note that HBase snapshots are by default persisted in the same storage layer configured when launching the cluster (in this case HDFS), so they should not be considered a strong disaster recovery mechanism if you want to protect your data in case of a cluster termination. In this case, you can export the snapshots in an Amazon S3 bucket to persist all your data on a reliable storage layer. Please note that periodic snapshots exports to S3 are recommended only if your tables have a small size (less than few TB) as an HBase export will copy all the data belonging to the snapshot in the S3 bucket using a Map Reduce job. For sample scripts and commands see the related examples in the Data Migration guide. Additional Tips Use a time identifier in the snapshot name that can help you identify when the snapshot was created. The creation time is also present in the snapshot metadata, but using this convention in the name can save some time while restoring an impaired cluster.","title":"HBase - Snapshots"},{"location":"applications/hbase/data_integrity/#hbase-cluster-replication","text":"The HBase cluster replication allows you to keep one or more HBase clusters synchronized between each other. Depending on how you set up the replication and peering between the clusters you can achieve different configurations to establish both a Disaster Recovery or a Highly Available setup depending on your needs. The following sections describe typical architectures that can be achieved with this feature.","title":"HBase - Cluster Replication"},{"location":"applications/hbase/data_integrity/#active-active-architecture","text":"This first approach describes a setup that is suitable to provide Highly Available clusters that can both serve read and write requests. In this case is required to set up a two-way replication peering between the Primary and Secondary cluster as described in the below figure. In this architecture both reads and writes will be routed across the two clusters by the Elastic Load Balancer and data written in a cluster will also be replicated in the other one. To setup this architecture you should performed the following steps: Create two HBase clusters in different AWS Availability Zones Create an Elastic Load Balancer using an EC2 Target Group configured with the following specifications: Target Type : IP addresses. EMR master IP for the Primary and Secondary cluster Protocol : TCP Port: 2181. Default port used by the Zookeeper service Establish a Two Way replication peering between the two clusters. To enable the replication, you can run the following commands on each master node. While running these commands, please make sure to replace MASTER_NODE_IP with the IP address of the other master node. For example, if running the commands on the Primary the MASTER_IP should be set with the Secondary IP address. HBASE_CMD = \"sudo -u hbase hbase\" MASTER_IP = \"MASTER_NODE_IP\" PEER_NAME = \"aws\" ## Create peering with the destination cluster echo \"add_peer ' $PEER_NAME ', CLUSTER_KEY => ' $MASTER_IP :2181:/hbase'\" | $HBASE_CMD shell ## List peers in the source cluster echo \"list_peers\" | $HBASE_CMD shell Make sure the HBase tables you want to replicate are already available in both EMR clusters, If your tables need to be initialized with the same data use the HBase snapshots to make sure they contain the same data. Enable the table replication using the following snippet for each table you want to replicate on both clusters ## Enable replication TABLE_NAME = \"YOUR_TABLE_NAME\" HBASE_CMD = \"sudo -u hbase hbase\" echo \"enable_table_replication ' $TABLE_NAME '\" | $HBASE_CMD shell To leverage this setup, specify the Network Load Balancer endpoint in the hbase.zookeeper.quorum property used by your client applications. This setup can tolerate impairments of an Availability Zone within the same Region and provides the best performance if you need milliseconds / sub milliseconds responses from your clusters. Please note that by default the HBase replication is an asynchronous process executed in background, and replicates WAL data across the clusters for which the replication is enabled. This means that this feature does not guarantee strong consistency when reading data. So carefully evaluate if this meet your business needs. In case one of the two cluster is terminated or needs to be upgraded, you have to re-create the HBase peering for the new cluster and restore the table\u2019s data and metadata in the new cluster.","title":"Active - Active architecture"},{"location":"applications/hbase/data_integrity/#active-passive-architecture","text":"In a similar way as before, you can set up an Active / Passive architecture that can serve DR purposes. This can be useful if you want to have a backup cluster you can switch to in case of issues on the Primary one. The following picture highlights the overall architecture setup and components. In order to implement the following architecture, you can perform the steps below: Create two EMR clusters in separate Availability Zones Create a Route53 Private Hosted Zone and create an A record pointing to the EMR Master you want to act as Primary. If using the EMR Multi-Master feature, it is recommended to add all the 3 Master nodes in the record set Establish a One Way HBase replication from the EMR Primary to EMR Secondary to replicate data to the Secondary cluster. In this case, you can use the commands previously shared and execute them on the EMR Primary cluster only. Once done, specify the Route53 A record previously defined to route your client applications to the EMR Primary cluster. This architecture serves mainly to implement a DR strategy for your HBase data. However, you can still leverage the Secondary cluster as a read replica of your data to reduce read requests on the Primary EMR Cluster. However, if you want to implement this scenario, please make sure that only client applications that have to perform READ operations (e.g. SCAN, GET) connect to the Secondary EMR cluster. In case of failures on the EMR Primary cluster, you\u2019ll be able to route your client application traffic to the Secondary EMR cluster by changing the IP address in the A record defined in the Route55 Private Hosted Zone. Please note that your client applications might face some failures while the A record update takes place.","title":"Active - Passive architecture"},{"location":"applications/hbase/data_integrity/#multi-region-architecture","text":"If you have a business requirement that requires to replicate HBase data in different AWS Regions, you can still leverage the HBase cluster replication feature to synchronize data between two clusters. The setup is very similar to what previously described, but requires to establish an inter-region VPC peering between the two AWS Regions, so that HBase clusters can exchange data between each other. An example multi region Active / Active setup is depicted in the below figure.","title":"Multi Region architecture"},{"location":"applications/hbase/data_integrity/#hbase-on-amazon-s3","text":"The following section provides architectures and best practices that you can use to implement Disaster Recovery (DR) strategies and Highly Available clusters when using Amazon S3 as storage layer for your HBase clusters.","title":"HBase on Amazon S3"},{"location":"applications/hbase/data_integrity/#storage-classes","text":"When using Amazon S3 as storage layer for your HBase cluster, all the objects created in the bucket by HBase will be created using the default S3 Standard storage class. In this case your data will be redundantly stored on a minimum of three Availability Zones within the same AWS Region. This ensures that your data is still available in your HBase cluster, either if there is an impairment in one Availability Zone. If you want to maintain this level of data availability in case of AZ failures, it is not recommended to set any S3 Lifecycle configuration that might transition HBase files in a storage class that will reduce the internal S3 data replication (e.g. S3 One Zone-IA). Additional Tips Always use dedicated S3 Buckets for your HBase on S3 clusters. This minimize chances of API throttling in case other processes or applications (e.g. Spark ETL jobs) are also using the same HBase bucket.","title":"Storage Classes"},{"location":"applications/hbase/data_integrity/#hbase-snapshots_1","text":"Although Amazon S3 already provides native functionalities to replicate objects across multiple Availability Zones, this doesn\u2019t protect you in case of application issues that might corrupt your HBase data. In this case, is good practice to leverage HBase existing capabilities to create periodic snapshots of your tables so that you can recover / restore tables in case of HBase inconsistencies or similar data integrity issues. Apache HBase stores snapshot data (store files and metadata) in the archive and .hbase-snapshot folders within the HBase root path. When using Amazon S3 as storage layer, this data will be replicated across multiple Availability Zones as well, as their content will be stored by default in the S3 bucket. We recommend to create HBase snapshots using the same S3 bucket used while launching the cluster (default behavior). In this way, snapshots will leverage incremental capabilities during the snapshot creation thus minimizing the footprint of data stored in the bucket. Please note that exporting a HBase snapshot in a different S3 bucket or prefix, will force HBase to copy all data required by the snapshot. For this reason, if you manage large clusters (hundred of TB or PB data), it\u2019s not recommended to export snapshots in different AWS Regions or S3 Buckets using this approach.","title":"HBase - Snapshots"},{"location":"applications/hbase/data_integrity/#hbase-cluster-replication_1","text":"As previously described in the HDFS section, the HBase Cluster Replication can be used to create a Highly Available cluster, or to implement different DR solutions depending on your business requirements. When using Amazon S3 as storage layer, it\u2019s important to remember that two HBase clusters cannot share the same S3 root directory, when they both receive write requests, as this might lead to data inconsistencies. For this reason, you should always use separate buckets for each individual HBase cluster, or as alternative use different prefixes within the same S3 bucket. This latest solution however is not ideal as it might increase the chances to face S3 throttling issues.","title":"HBase - Cluster Replication"},{"location":"applications/hbase/data_integrity/#amazon-emr-read-replica","text":"The EMR HBase Read Replica feature can be used to provide High Available reads for your HBase clusters using S3 as storage layer. Although this feature does not provide additional benefits for a DR recovery mechanism, it can still be useful to serve HBase read requests, in case you want to perform a Blue / Green deployment to modify a cluster configuration on the primary Amazon EMR cluster that requires the termination of the cluster (e.g. EMR release version upgrade) For additional details see Using a read-replica cluster in the Amazon EMR documentation.","title":"Amazon EMR - Read Replica"},{"location":"applications/hbase/data_integrity/#amazon-s3-object-replication","text":"If you want to replicate your HBase data on a backup region for DR purposes, you might leverage the Amazon S3 Object Replication feature. This can be used to replicate objects within the same AWS Region (Same-Region Replication) or in a different region (Cross-Region Replication). This approach can be used to implement a DR mechanism that allows you to launch a Secondary cluster in a different AWS Region in case you have an impairment in your Primary one. The overall architecture is described in the below figure. This architecture requires you to use a DNS mechanism (for example using Route 53 hosted zones) so that you can switch between the AWS Regions in case of failures. This approach requires the following components: Primary Amazon EMR cluster with his own dedicated S3 Bucket. Secondary Amazon EMR cluster that will only be launched in case of failures, with his own dedicated S3 bucket. The secondary cluster can be launched in the same AWS Region as the primary or in a different one depending on the requirements. Active S3 Replication between the Primary and the Secondary S3 buckets to replicate S3 objects A DNS setup that allows you to switch your HBase clients from the primary to the secondary in case of failures. For example this might be achieved using a Route 53 private hosted zones As previously described, the Secondary cluster should only be launched in case of an impairment in the Primary Region or Availability Zone failure. The cluster can be launched with the same configurations and sizes as the primary, but should point to a different S3 bucket. We also recommend to launch the secondary cluster using the Amazon EMR HBase read replica feature to be sure that no new data will be written on the secondary cluster. This prevent the secondary cluster to receive new data, but simplify the recovery after an impairment. In order to enable the S3 Object Replication, you should follow the steps below: Create two Amazon S3 buckets that will be respectively used to store production and replicated data. Make sure to enable the Amazon S3 Versioning , as this functionality is required to enable the S3 replication . In the primary bucket, create a new S3 replication rule to replicate data generated by the primary cluster. You can follow the example in the Amazon S3 documentation to enable the replication in the bucket. While creating the replication rule, make sure to adhere the following best practices: Enable the S3 Object Replication only for the HBase root prefix specified when launching the cluster. This help mitigating delay problems that might occur if you also have objects outside the HBase root prefix that should be replicated. Enable the Replication Time Control (RTC) capabilities. This feature is designed to replicate 99.99% of objects within 15 minutes after upload. Enabling this feature will also automatically enable the S3 replication metrics that are review the pending replication objects. Enable the Delete Marker Replication Additionally, is also recommend to create a Lifecycle Rule to delete expired object delete markers, incomplete multipart uploads, and non current version of files. This architecture serves mainly to implement a cost effective DR strategy for your HBase data as only one active cluster will be running. In case of failover, before switching to the secondary cluster, check the S3 replication metrics to verify there are no pending objects to be replicated. Additional Considerations Amazon EMR implements internal features that prevents the clusters to be terminated in case of Availability Zone or service issues. If your primary cluster cannot be reached, you might want to launch another cluster pointing to the same Amazon S3 bucket in a different AZ. However, this might lead to inconsistencies in case your Primary HBase cluster is not terminated as you might end up in a situation where two active HBase clusters are pointing to the same S3 root bucket. For this reason, you might want to implement the following safety measures in case of service issues: If you only require to continue supporting HBase read operations, you can launch a backup cluster pointing to the same S3 root directory until we solve the problem. As alternative, if you\u2019re not able to determine if your cluster instances are terminated (e.g. the failure also impact your ability to use the EC2 service) you might contact our Support to verify if the cluster was terminated to decide launching a new active cluster instead of just launch a HBase read replica. If you want to continue to support HBase write requests within the same Region, you\u2019ll have to leverage a backup S3 bucket where data have been replicated using the S3 Object Replication or HBase cluster replication to avoid data inconsistencies if the Primary EMR cluster has not been yet terminated. As in the previous scenario, you can also contact our Support to determine if the Primary cluster was already terminated, but this might delay the recovery time.","title":"Amazon S3 - Object Replication"},{"location":"applications/hbase/data_integrity/#hbase-wals","text":"When using Amazon S3 as storage layer, HBase still stores WALs on the local HDFS of the cluster. WALs are used internally by HBase to replay mutate operations in case of a region failure. Please note that every mutation request on HBase is first written on a WAL file, then in the HBase memstore and only after a Memstore flush this data will be persisted on S3. If the Amazon EMR cluster is terminated due to an incident, you might lose the latest data not yet persisted on S3. In this case is a good practice to leverage a persistent event store solution, like Amazon MSK or Amazon Kinesis to retain the latest ingested data, so that you\u2019ll be able to replay any missing data from the moment of the service interruption. As alternative, you can configure your HBase cluster to store WALs on a persistent storage layer as an external HDFS cluster, or an Amazon EFS filesystem. This last solution might increase the latency of your write operations on HBase so you might want to verify if this solution met your SLA requirements. To configure WALs on Amazon EFS you can use the procedure described here . If you prefer to simply leverage another HDFS cluster, you can configure the new path using the following EMR Classification when launching your HBase Cluster. [ { \"classification\" : \"hbase-site\" , \"properties\" : { \"hbase.wal.dir\" : \"hdfs://HDFS_MASTER_NODE:8020/PATH\" } } ] Please note that the PATH used in the external HDFS cluster should be pre-created before launching the cluster and should be writable by the following user and group: hbase : hbase . Additionally, if your external HDFS cluster is secured with Kerberos authentication, you also need to configure your HBase cluster with Kerberos, and both clusters should leverage the same Kerberos REALM to be able to communicate between each other. For additional information, see External KDC in the Amazon EMR documentation.","title":"HBase WALs"},{"location":"applications/hbase/data_migration/","text":"Data Migration \u00b6 This document describes possible migrations paths you can follow when migrating data from an existing HBase cluster (e.g. on premise, or self-managed cluster on EC2) to Amazon EMR. HBase snapshots \u00b6 This is the most straight forward approach that doesn't require a complex setup and can easily be achieved using simple bash scripts. This approach is suitable if your data does not change frequently or when you can tolerate downtimes in your production systems to perform the data migration. Below a list of steps that can be used to create a HBase Snapshot and transfer it to an Amazon S3 bucket. Please note that you can use the same approach to store snapshots on an HDFS cluster. If this is the case, replace the S3 target path in the following commands with the destination HDFS path (e.g. hdfs://NN_TARGET:8020/user/hbase) where you want to store the snapshots. Create a snapshot of a single HBase table When creating a snapshot, it\u2019s good practice to also add an identifier in the snapshot name to have a reference date of when the snapshot was created. Before launching this command please replace the variable TABLE_NAME with the corresponding table you want to generate the snapshot for. If the table is in a namespace different from default use the following convention NAMESPACE:TABLE_NAME . From the SOURCE cluster submit the following commands: DATE = ` date + \"%Y%m%d\" ` TABLE_NAME = \"YOUR_TABLE_NAME\" hbase snapshot create -n \" ${ TABLE_NAME /:/_ } - $DATE \" -t ${ TABLE_NAME } To verify the snapshot just created, use the following command hbase snapshot info -list-snapshots Copy the snapshot to an Amazon S3 bucket Note When migrating from an on premise cluster, make sure that you have Hadoop YARN installed in your cluster, as the commands rely on MR jobs to perform the copy to S3. Besides, you need to make sure that your Hadoop installation provides the hadoop-aws module that is required to communicate with Amazon S3. Note If you're planning to use HBase with Amazon S3 as storage layer, you should use as TARGET_BUCKET the same S3 path that will be used as HBase S3 Root Directory while launching the EMR cluster. This minimize copies on S3 that are required when restoring the snapshots, thus reducing the restore time of your tables. To avoid any conflict during the snapshot copy, you should not start the EMR cluster (if using Amazon S3 as storage layer) before the end of the snapshot copy. TARGET_BUCKET = \"s3://BUCKET/PREFIX/\" hbase snapshot export -snapshot ${ TABLE_NAME /:/_ } - $DATE -copy-to $TARGET_BUCKET Restore Table when using Amazon S3 as storage layer for HBase If you followed the notes in the previous step, you'll find the snapshot already available in HBase after launching the cluster. Note If your snapshot was created from a namespace different from the default one, make sure to pre create it, to avoid failures while restoring the snapshot. From the EMR master node: # Verify snapshot availability HBASE_CMD = \"sudo -u hbase hbase\" $HBASE_CMD snapshot info -list-snapshots # Review snapshot info and details SNAPSHOT_NAME = \"YOUR_SNAPSHOT_NAME\" $HBASE_CMD snapshot info -snapshot $SNAPSHOT_NAME -size-in-bytes -files -stats -schema # Optional - Create namespaces required by the snapshot echo \"create_namespace \\\" $NAMESPACE_NAME \\\"\" | $HBASE_CMD shell # Restore table from snapshot echo \"restore_snapshot \\\" $SNAPSHOT_NAME \\\"\" | $HBASE_CMD shell Scripts The following scripts allows you to migrate and restore HBase tables an namespaces using the snapshot procedure previously described. Snapshot export - Generate HBase snapshots for all the tables stored in all the namespaces, and copy them on an Amazon S3 bucket. Snapshot import - Restore all the snapshots stored in an Amazon S3 bucket. Snapshots with Incremental Export \u00b6 This approach might help in those situations where you want to migrate your data but at the same time you cannot tolerate much downtime in your production system. This approach helps to perform an initial bulk migration using the HBase snapshot procedure previously described, and then reconcile data received after the HBase snapshot generating incremental exports from the SOURCE table. This approach works when the volume of ingested data is not high, as the procedure to reconcile the data in the DESTINATION cluster might require multiple iterations to synchronize the two clusters, along with the fact that might be error prone. The following highlights the overall migration procedure. In the SOURCE cluster: Create a snapshot of the HBase table you want to migrate. Collect the epoch time when the snapshot was taken, as this will be used to determine new data ingested in the cluster. Export the snapshot on Amazon S3 org.apache.hadoop.hbase.snapshot.ExportSnapshot In the DESTINATION cluster: Import the snapshot in the cluster and restore the table In the SOURCE cluster: Generate an incremental export to S3 for data arrived in the cluster after taking the snapshot using the HBase utility org.apache.hadoop.hbase.mapreduce.Export In the DESTINATION cluster: Restore the missing data in the destination cluster using the HBase utility org.apache.hadoop.hbase.mapreduce.Import Example Export Commands ## Configurations HBASE_CMD = \"sudo -u hbase hbase\" BUCKET_NAME = \"YOUR_BUCKET_NAME\" SNAPSHOT_PATH = \"s3:// $BUCKET_NAME /hbase-snapshots/\" TABLE_NAME = \"TestTable\" # ============================================================================== # (Simulate) Create TestTable with 1000 rows # ============================================================================== $HBASE_CMD pe --table = $TABLE_NAME --rows = 1000 --nomapred sequentialWrite 1 # ============================================================================== # Take initial table snapshot and copy it to S3 # ============================================================================== DATE = ` date + \"%Y%m%d\" ` EPOCH_MS = ` date +%s%N | cut -b1-13 ` LABEL = \" $DATE - $EPOCH_MS \" # snapshot creation # Note: HBase performs a FLUSH by default when creating a snapshot # You can change this behaviour specifying the -s parameter $HBASE_CMD snapshot create -n \" ${ LABEL } - ${ TABLE_NAME } \" -t $TABLE_NAME # copy to S3 $HBASE_CMD org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot \" ${ LABEL } - ${ TABLE_NAME } \" -copy-to $SNAPSHOT_PATH # ============================================================================== # (Simulate) Data mutations to simulate data arrived after taking the snapshot # ============================================================================== # overwrite the first 100 elements of the table $HBASE_CMD pe --rows = 100 --nomapred sequentialWrite 1 # check first 100 rows will have an higher timestamp compared to the 101 element echo \"scan ' $TABLE_NAME ', {LIMIT => 101}\" | $HBASE_CMD shell # ============================================================================== # Generate incremental data export # ============================================================================== # Retrieve the epoch time from the snapshot name that was previously created. # This allow us to only export data modified since that moment in time. $HBASE_CMD snapshot info -list-snapshots # Incremental updates LATEST_SNAPSHOT_EPOCH = \" $EPOCH_MS \" NEW_EPOCH_MS = ` date +%s%N | cut -b1-13 ` INCREMENTAL_PATH = \"s3:// $BUCKET_NAME /hbase-delta/ ${ TABLE_NAME } / ${ NEW_EPOCH_MS } \" $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Export ${ TABLE_NAME } $INCREMENTAL_PATH 1 $LATEST_SNAPSHOT_EPOCH Example Import Commands ## Configurations HBASE_CMD = \"sudo -u hbase hbase\" BUCKET_NAME = \"YOUR_BUCKET_NAME\" SNAPSHOT_PATH = \"s3:// $BUCKET_NAME /hbase-snapshots/\" HBASE_CONF = \"/etc/hbase/conf/hbase-site.xml\" HBASE_ROOT = $( xmllint --xpath \"//configuration/property/*[text()='hbase.rootdir']/../value/text()\" $HBASE_CONF ) # ============================================================================== # Import and Restore HBase snapshot # ============================================================================== ## List Snapshots on S3 and take note of the snapshot you want to restore $HBASE_CMD snapshot info -list-snapshots -remote-dir $SNAPSHOT_PATH SNAPSHOT_NAME = \"SNAPSHOT_NAME\" # e.g. \"20220817-1660726018359-TestTable\" ## Copy snapshot on the cluster $HBASE_CMD snapshot export \\ -D hbase.rootdir = $SNAPSHOT_PATH \\ -snapshot $SNAPSHOT_NAME \\ -copy-to $HBASE_ROOT # Restore initial snapshot echo \"restore_snapshot ' $SNAPSHOT_NAME '\" | $HBASE_CMD shell # ============================================================================== # Replay incremental updates # ============================================================================== TABLE_NAME = $( echo $SNAPSHOT_NAME | awk -F- '{print $3}' ) INCREMENTAL_PATH = \"s3:// $BUCKET_NAME /hbase-delta/ ${ TABLE_NAME } / ${ NEW_EPOCH_MS } \" $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Import ${ TABLE_NAME } ${ INCREMENTAL_PATH } Snapshots with HBase Replication \u00b6 This approach describes how to migrate data using the HBase cluster replication feature that allows you to establish a peering between two (or more) HBase clusters so that they can replicate incoming data depending on how the peering was established. In order to use this approach, a network connection between the SOURCE and DESTINATION cluster should be present. If you're transferring data from an on premise cluster and you have large volumes of data to replicate, you might establish the connection between the two clusters using AWS Direct Connect or you can establish a VPN connection if this is a one time migration. The below section highlight the overall procedure to establish the replication. In the SOURCE cluster, create a HBase peering with the DESTINATION cluster and then disable the peering so that data is accumulated in the HBase WALs. In the SOURCE cluster, take a snapshot of the table you want to migrate and export it to S3. In the DESTINATION cluster, import and restore the snapshot. This creates the metadata (table description) required for the replication and also restore the data present in the snapshot. In the SOURCE cluster, re-enable the HBase peering with the DESTINATION cluster, so that data modified up to that moment will start to be replicated in the DESTINATION cluster. Monitor the replication process from the HBase shell to verify the lag of replication before completely switch on the DESTINATION cluster, and shutdown the SOURCE cluster. Create one-way peering: SOURCE \u2192 DESTINATION Note The configuration for the replication should be enabled by default in HBase. To double check, verify hbase.replication is set to true in the hbase-site.xml in the SOURCE cluster. To create the HBase peering, you need to know the DESTINATION ip or hostname of the node where the Zookeeper ensemble used by HBase is located. If the destination cluster is an Amazon EMR cluster this coincides with the EMR master node. Once collected this information, from the SOURCE cluster execute the following commands to enable the peering with the destination cluster and start accumulating new data in the HBase WALs: # The HBase command might be different in your Hadoop environment depending on # how HBase was installed and which user is used to properly launch the cli. # In most installations, it's sufficient to use the `hbase` command only. HBASE_CMD = \"sudo -u hbase hbase\" MASTER_IP = \"**YOUR_MASTER_IP**\" # e.g. ip-xxx-xx-x-xx.eu-west-1.compute.internal PEER_NAME = \"aws\" TABLE_NAME = \"**YOUR_TABLE_NAME**\" ## Create peering with the destination cluster echo \"add_peer ' $PEER_NAME ', CLUSTER_KEY => ' $MASTER_IP :2181:/hbase'\" | $HBASE_CMD shell ## List peers in the source cluster echo \"list_peers\" | $HBASE_CMD shell ## Disable the peer just created, so that we can keep new data in the LOG (HBase WALs) until the snapshots are restored in the DESTINATION cluster echo \"disable_peer ' $PEER_NAME '\" | $HBASE_CMD shell ## enable replication for the tables to replicate echo \"enable_table_replication ' $TABLE_NAME '\" | $HBASE_CMD shell Now you can switch to the DESTINATION cluster and restore the initial snapshot taken for the table. Once the restore is complete, switch again on the SOURCE cluster and enable the HBase peering to start replicating new data ingested in the SOURCE cluster since the initial SNAPSHOT was taken. HBASE_CMD = \"sudo -u hbase hbase\" PEER_NAME = \"aws\" echo \"enable_peer ' $PEER_NAME '\" | $HBASE_CMD shell To monitor the replication status you could use the hbase command status 'replication' from the HBase shell on the SOURCE cluster. Migrate HBase 1.x to HBase 2.x \u00b6 When using HDFS \u00b6 The migration path from HBase 1.x to HBase 2.x, can be accomplished using HBase snapshots if you're using HDFS as storage layer. In this case you can take a snapshot on the HBase 1.x cluster and then restore it on the HBase 2.x one. Although it is highly recommended to migrate to the latest version of HBase 1.4.x before migrating to HBase 2.x, it is still possible to migrate from older version of the 1.x branch (1.0.x, 1.1.x, 1.2.x, etc). When using Amazon S3 \u00b6 If you're using Amazon S3 as storage layer for HBase, you can directly migrate any EMR cluster using an HBase version >= 1.x to an Amazon EMR release using HBase <= 2.2.x. Note If you try to update to a more recent version of HBase (e.g. HBase 2.4.4 from HBase 1.x), the HBase master will fail to correctly start due to some breaking changes in the way HBase load the meta table information in newest releases. You might see a similar error in your HMaster logs: Caused by: org . apache . hadoop . hbase . ipc . RemoteWithExtrasException ( org . apache . hadoop . hbase . regionserver . NoSuchColumnFamilyException ) : org . apache . hadoop . hbase . regionserver . NoSuchColumnFamilyException: Column family table does not exist in region hbase: meta ,, 1.1588230740 in table 'hba se: meta ', { TABLE_ATTRIBUTES => { IS_META => ' true ', coprocessor $ 1 => ' | org . apache . hadoop . hbase . coprocessor . MultiRowMutationEndpoint | 536870911 | '}}, { NAME => ' info ', VERSIONS => '3' , KEEP_DELETED_CELLS => ' FALSE ', DATA_BLOCK_ENCODING => ' NONE ', TTL => ' FOREVER ', MIN_VERSIONS => '0' , REPLICATION_SCOPE => '0' , BLOOMFILTER => ' NONE ', IN_MEMORY => ' true ', COMPRESSION => ' NONE ', BLOCKCACHE => ' true ', BLOCKSIZE => ' 8192 ', METADATA => {' CACHE_DATA_IN_L1 ' => ' true '}} at org . apache . hadoop . hbase . regionserver . HRegion . checkFamily ( HRegion . java: 8685 ) at org . apache . hadoop . hbase . regionserver . HRegion . getScanner ( HRegion . java: 3125 ) at org . apache . hadoop . hbase . regionserver . HRegion . getScanner ( HRegion . java: 3110 ) In this case to migrate to the latest version, you can perform a two step migration: First, disable all your HBase tables in the Amazon EMR cluster using HBase 1.x. Once all the tables are disabled, terminate this cluster. Launch a new Amazon EMR cluster using EMR 6.3.0 as release and wait for all the tables/regions to be assigned. Once completed, disable all the tables again and shutdown the cluster. Finally, launch the latest EMR Version you want to use. Summary \u00b6 Approach When to use? Complexity Batch - HBase Snapshots Data doesn't change frequently or when you can tolerate high service downtime Easy Incremental - HBase Snapshots + Export The data doesn't change frequently and you have large tables Medium Online - HBase Snapshots + Replication Data changes frequently and high service downtime cannot be tolerated Advanced","title":"Data Migration"},{"location":"applications/hbase/data_migration/#data-migration","text":"This document describes possible migrations paths you can follow when migrating data from an existing HBase cluster (e.g. on premise, or self-managed cluster on EC2) to Amazon EMR.","title":"Data Migration"},{"location":"applications/hbase/data_migration/#hbase-snapshots","text":"This is the most straight forward approach that doesn't require a complex setup and can easily be achieved using simple bash scripts. This approach is suitable if your data does not change frequently or when you can tolerate downtimes in your production systems to perform the data migration. Below a list of steps that can be used to create a HBase Snapshot and transfer it to an Amazon S3 bucket. Please note that you can use the same approach to store snapshots on an HDFS cluster. If this is the case, replace the S3 target path in the following commands with the destination HDFS path (e.g. hdfs://NN_TARGET:8020/user/hbase) where you want to store the snapshots. Create a snapshot of a single HBase table When creating a snapshot, it\u2019s good practice to also add an identifier in the snapshot name to have a reference date of when the snapshot was created. Before launching this command please replace the variable TABLE_NAME with the corresponding table you want to generate the snapshot for. If the table is in a namespace different from default use the following convention NAMESPACE:TABLE_NAME . From the SOURCE cluster submit the following commands: DATE = ` date + \"%Y%m%d\" ` TABLE_NAME = \"YOUR_TABLE_NAME\" hbase snapshot create -n \" ${ TABLE_NAME /:/_ } - $DATE \" -t ${ TABLE_NAME } To verify the snapshot just created, use the following command hbase snapshot info -list-snapshots Copy the snapshot to an Amazon S3 bucket Note When migrating from an on premise cluster, make sure that you have Hadoop YARN installed in your cluster, as the commands rely on MR jobs to perform the copy to S3. Besides, you need to make sure that your Hadoop installation provides the hadoop-aws module that is required to communicate with Amazon S3. Note If you're planning to use HBase with Amazon S3 as storage layer, you should use as TARGET_BUCKET the same S3 path that will be used as HBase S3 Root Directory while launching the EMR cluster. This minimize copies on S3 that are required when restoring the snapshots, thus reducing the restore time of your tables. To avoid any conflict during the snapshot copy, you should not start the EMR cluster (if using Amazon S3 as storage layer) before the end of the snapshot copy. TARGET_BUCKET = \"s3://BUCKET/PREFIX/\" hbase snapshot export -snapshot ${ TABLE_NAME /:/_ } - $DATE -copy-to $TARGET_BUCKET Restore Table when using Amazon S3 as storage layer for HBase If you followed the notes in the previous step, you'll find the snapshot already available in HBase after launching the cluster. Note If your snapshot was created from a namespace different from the default one, make sure to pre create it, to avoid failures while restoring the snapshot. From the EMR master node: # Verify snapshot availability HBASE_CMD = \"sudo -u hbase hbase\" $HBASE_CMD snapshot info -list-snapshots # Review snapshot info and details SNAPSHOT_NAME = \"YOUR_SNAPSHOT_NAME\" $HBASE_CMD snapshot info -snapshot $SNAPSHOT_NAME -size-in-bytes -files -stats -schema # Optional - Create namespaces required by the snapshot echo \"create_namespace \\\" $NAMESPACE_NAME \\\"\" | $HBASE_CMD shell # Restore table from snapshot echo \"restore_snapshot \\\" $SNAPSHOT_NAME \\\"\" | $HBASE_CMD shell Scripts The following scripts allows you to migrate and restore HBase tables an namespaces using the snapshot procedure previously described. Snapshot export - Generate HBase snapshots for all the tables stored in all the namespaces, and copy them on an Amazon S3 bucket. Snapshot import - Restore all the snapshots stored in an Amazon S3 bucket.","title":"HBase snapshots"},{"location":"applications/hbase/data_migration/#snapshots-with-incremental-export","text":"This approach might help in those situations where you want to migrate your data but at the same time you cannot tolerate much downtime in your production system. This approach helps to perform an initial bulk migration using the HBase snapshot procedure previously described, and then reconcile data received after the HBase snapshot generating incremental exports from the SOURCE table. This approach works when the volume of ingested data is not high, as the procedure to reconcile the data in the DESTINATION cluster might require multiple iterations to synchronize the two clusters, along with the fact that might be error prone. The following highlights the overall migration procedure. In the SOURCE cluster: Create a snapshot of the HBase table you want to migrate. Collect the epoch time when the snapshot was taken, as this will be used to determine new data ingested in the cluster. Export the snapshot on Amazon S3 org.apache.hadoop.hbase.snapshot.ExportSnapshot In the DESTINATION cluster: Import the snapshot in the cluster and restore the table In the SOURCE cluster: Generate an incremental export to S3 for data arrived in the cluster after taking the snapshot using the HBase utility org.apache.hadoop.hbase.mapreduce.Export In the DESTINATION cluster: Restore the missing data in the destination cluster using the HBase utility org.apache.hadoop.hbase.mapreduce.Import Example Export Commands ## Configurations HBASE_CMD = \"sudo -u hbase hbase\" BUCKET_NAME = \"YOUR_BUCKET_NAME\" SNAPSHOT_PATH = \"s3:// $BUCKET_NAME /hbase-snapshots/\" TABLE_NAME = \"TestTable\" # ============================================================================== # (Simulate) Create TestTable with 1000 rows # ============================================================================== $HBASE_CMD pe --table = $TABLE_NAME --rows = 1000 --nomapred sequentialWrite 1 # ============================================================================== # Take initial table snapshot and copy it to S3 # ============================================================================== DATE = ` date + \"%Y%m%d\" ` EPOCH_MS = ` date +%s%N | cut -b1-13 ` LABEL = \" $DATE - $EPOCH_MS \" # snapshot creation # Note: HBase performs a FLUSH by default when creating a snapshot # You can change this behaviour specifying the -s parameter $HBASE_CMD snapshot create -n \" ${ LABEL } - ${ TABLE_NAME } \" -t $TABLE_NAME # copy to S3 $HBASE_CMD org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot \" ${ LABEL } - ${ TABLE_NAME } \" -copy-to $SNAPSHOT_PATH # ============================================================================== # (Simulate) Data mutations to simulate data arrived after taking the snapshot # ============================================================================== # overwrite the first 100 elements of the table $HBASE_CMD pe --rows = 100 --nomapred sequentialWrite 1 # check first 100 rows will have an higher timestamp compared to the 101 element echo \"scan ' $TABLE_NAME ', {LIMIT => 101}\" | $HBASE_CMD shell # ============================================================================== # Generate incremental data export # ============================================================================== # Retrieve the epoch time from the snapshot name that was previously created. # This allow us to only export data modified since that moment in time. $HBASE_CMD snapshot info -list-snapshots # Incremental updates LATEST_SNAPSHOT_EPOCH = \" $EPOCH_MS \" NEW_EPOCH_MS = ` date +%s%N | cut -b1-13 ` INCREMENTAL_PATH = \"s3:// $BUCKET_NAME /hbase-delta/ ${ TABLE_NAME } / ${ NEW_EPOCH_MS } \" $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Export ${ TABLE_NAME } $INCREMENTAL_PATH 1 $LATEST_SNAPSHOT_EPOCH Example Import Commands ## Configurations HBASE_CMD = \"sudo -u hbase hbase\" BUCKET_NAME = \"YOUR_BUCKET_NAME\" SNAPSHOT_PATH = \"s3:// $BUCKET_NAME /hbase-snapshots/\" HBASE_CONF = \"/etc/hbase/conf/hbase-site.xml\" HBASE_ROOT = $( xmllint --xpath \"//configuration/property/*[text()='hbase.rootdir']/../value/text()\" $HBASE_CONF ) # ============================================================================== # Import and Restore HBase snapshot # ============================================================================== ## List Snapshots on S3 and take note of the snapshot you want to restore $HBASE_CMD snapshot info -list-snapshots -remote-dir $SNAPSHOT_PATH SNAPSHOT_NAME = \"SNAPSHOT_NAME\" # e.g. \"20220817-1660726018359-TestTable\" ## Copy snapshot on the cluster $HBASE_CMD snapshot export \\ -D hbase.rootdir = $SNAPSHOT_PATH \\ -snapshot $SNAPSHOT_NAME \\ -copy-to $HBASE_ROOT # Restore initial snapshot echo \"restore_snapshot ' $SNAPSHOT_NAME '\" | $HBASE_CMD shell # ============================================================================== # Replay incremental updates # ============================================================================== TABLE_NAME = $( echo $SNAPSHOT_NAME | awk -F- '{print $3}' ) INCREMENTAL_PATH = \"s3:// $BUCKET_NAME /hbase-delta/ ${ TABLE_NAME } / ${ NEW_EPOCH_MS } \" $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Import ${ TABLE_NAME } ${ INCREMENTAL_PATH }","title":"Snapshots with Incremental Export"},{"location":"applications/hbase/data_migration/#snapshots-with-hbase-replication","text":"This approach describes how to migrate data using the HBase cluster replication feature that allows you to establish a peering between two (or more) HBase clusters so that they can replicate incoming data depending on how the peering was established. In order to use this approach, a network connection between the SOURCE and DESTINATION cluster should be present. If you're transferring data from an on premise cluster and you have large volumes of data to replicate, you might establish the connection between the two clusters using AWS Direct Connect or you can establish a VPN connection if this is a one time migration. The below section highlight the overall procedure to establish the replication. In the SOURCE cluster, create a HBase peering with the DESTINATION cluster and then disable the peering so that data is accumulated in the HBase WALs. In the SOURCE cluster, take a snapshot of the table you want to migrate and export it to S3. In the DESTINATION cluster, import and restore the snapshot. This creates the metadata (table description) required for the replication and also restore the data present in the snapshot. In the SOURCE cluster, re-enable the HBase peering with the DESTINATION cluster, so that data modified up to that moment will start to be replicated in the DESTINATION cluster. Monitor the replication process from the HBase shell to verify the lag of replication before completely switch on the DESTINATION cluster, and shutdown the SOURCE cluster. Create one-way peering: SOURCE \u2192 DESTINATION Note The configuration for the replication should be enabled by default in HBase. To double check, verify hbase.replication is set to true in the hbase-site.xml in the SOURCE cluster. To create the HBase peering, you need to know the DESTINATION ip or hostname of the node where the Zookeeper ensemble used by HBase is located. If the destination cluster is an Amazon EMR cluster this coincides with the EMR master node. Once collected this information, from the SOURCE cluster execute the following commands to enable the peering with the destination cluster and start accumulating new data in the HBase WALs: # The HBase command might be different in your Hadoop environment depending on # how HBase was installed and which user is used to properly launch the cli. # In most installations, it's sufficient to use the `hbase` command only. HBASE_CMD = \"sudo -u hbase hbase\" MASTER_IP = \"**YOUR_MASTER_IP**\" # e.g. ip-xxx-xx-x-xx.eu-west-1.compute.internal PEER_NAME = \"aws\" TABLE_NAME = \"**YOUR_TABLE_NAME**\" ## Create peering with the destination cluster echo \"add_peer ' $PEER_NAME ', CLUSTER_KEY => ' $MASTER_IP :2181:/hbase'\" | $HBASE_CMD shell ## List peers in the source cluster echo \"list_peers\" | $HBASE_CMD shell ## Disable the peer just created, so that we can keep new data in the LOG (HBase WALs) until the snapshots are restored in the DESTINATION cluster echo \"disable_peer ' $PEER_NAME '\" | $HBASE_CMD shell ## enable replication for the tables to replicate echo \"enable_table_replication ' $TABLE_NAME '\" | $HBASE_CMD shell Now you can switch to the DESTINATION cluster and restore the initial snapshot taken for the table. Once the restore is complete, switch again on the SOURCE cluster and enable the HBase peering to start replicating new data ingested in the SOURCE cluster since the initial SNAPSHOT was taken. HBASE_CMD = \"sudo -u hbase hbase\" PEER_NAME = \"aws\" echo \"enable_peer ' $PEER_NAME '\" | $HBASE_CMD shell To monitor the replication status you could use the hbase command status 'replication' from the HBase shell on the SOURCE cluster.","title":"Snapshots with HBase Replication"},{"location":"applications/hbase/data_migration/#migrate-hbase-1x-to-hbase-2x","text":"","title":"Migrate HBase 1.x to HBase 2.x"},{"location":"applications/hbase/data_migration/#when-using-hdfs","text":"The migration path from HBase 1.x to HBase 2.x, can be accomplished using HBase snapshots if you're using HDFS as storage layer. In this case you can take a snapshot on the HBase 1.x cluster and then restore it on the HBase 2.x one. Although it is highly recommended to migrate to the latest version of HBase 1.4.x before migrating to HBase 2.x, it is still possible to migrate from older version of the 1.x branch (1.0.x, 1.1.x, 1.2.x, etc).","title":"When using HDFS"},{"location":"applications/hbase/data_migration/#when-using-amazon-s3","text":"If you're using Amazon S3 as storage layer for HBase, you can directly migrate any EMR cluster using an HBase version >= 1.x to an Amazon EMR release using HBase <= 2.2.x. Note If you try to update to a more recent version of HBase (e.g. HBase 2.4.4 from HBase 1.x), the HBase master will fail to correctly start due to some breaking changes in the way HBase load the meta table information in newest releases. You might see a similar error in your HMaster logs: Caused by: org . apache . hadoop . hbase . ipc . RemoteWithExtrasException ( org . apache . hadoop . hbase . regionserver . NoSuchColumnFamilyException ) : org . apache . hadoop . hbase . regionserver . NoSuchColumnFamilyException: Column family table does not exist in region hbase: meta ,, 1.1588230740 in table 'hba se: meta ', { TABLE_ATTRIBUTES => { IS_META => ' true ', coprocessor $ 1 => ' | org . apache . hadoop . hbase . coprocessor . MultiRowMutationEndpoint | 536870911 | '}}, { NAME => ' info ', VERSIONS => '3' , KEEP_DELETED_CELLS => ' FALSE ', DATA_BLOCK_ENCODING => ' NONE ', TTL => ' FOREVER ', MIN_VERSIONS => '0' , REPLICATION_SCOPE => '0' , BLOOMFILTER => ' NONE ', IN_MEMORY => ' true ', COMPRESSION => ' NONE ', BLOCKCACHE => ' true ', BLOCKSIZE => ' 8192 ', METADATA => {' CACHE_DATA_IN_L1 ' => ' true '}} at org . apache . hadoop . hbase . regionserver . HRegion . checkFamily ( HRegion . java: 8685 ) at org . apache . hadoop . hbase . regionserver . HRegion . getScanner ( HRegion . java: 3125 ) at org . apache . hadoop . hbase . regionserver . HRegion . getScanner ( HRegion . java: 3110 ) In this case to migrate to the latest version, you can perform a two step migration: First, disable all your HBase tables in the Amazon EMR cluster using HBase 1.x. Once all the tables are disabled, terminate this cluster. Launch a new Amazon EMR cluster using EMR 6.3.0 as release and wait for all the tables/regions to be assigned. Once completed, disable all the tables again and shutdown the cluster. Finally, launch the latest EMR Version you want to use.","title":"When using Amazon S3"},{"location":"applications/hbase/data_migration/#summary","text":"Approach When to use? Complexity Batch - HBase Snapshots Data doesn't change frequently or when you can tolerate high service downtime Easy Incremental - HBase Snapshots + Export The data doesn't change frequently and you have large tables Medium Online - HBase Snapshots + Replication Data changes frequently and high service downtime cannot be tolerated Advanced","title":"Summary"},{"location":"applications/hbase/introduction/","text":"Introduction \u00b6 When working with Amazon EMR on EC2, you have the ability to choose between two deployment options for the underlying storage layer used by HBase: the Hadoop HDFS or Amazon S3 . Although there are no restrictions in the use of these storage options, they serve different purposes, and they both have pros and cons with related performance implications. In this document, we are going to review the main aspects of each storage option. Which storage layer should I use? \u00b6 Typically, to understand which storage layer you should use in your HBase cluster, you must determine what are your application requirements and decide what is most important between these two main decision drivers: performance or costs. Generally speaking, on a large cluster setup, HDFS provides better performance in most cases, while Amazon S3 provides better cost savings due to the reduced amount of storage required to persist all your data, and is the right option when you want to decouple your storage from compute. Using HDFS allows you to achieve the best performance for latency responses. This is true if you need milliseconds / sub-milliseconds read responses from HBase. You can also achieve similar results using Amazon S3 as storage layer, but this will require to rely on HBase caching features. Depending on your tables sizes, this can increase costs when provisioning resources for cache, as you\u2019ll have to provision more EBS volumes or use bigger instances to cache your data locally on the nodes, thus losing the main advantages of using Amazon S3. This requires to fine tune HBase to find the right balance between performance and cost for your workload. Another common use case to choose HDFS over S3 is a data migration from an on premise cluster. This is typically recommended as first migration step, as this solution provides similar performance compared to your existing cluster. You can more easily migrate your infrastructure to the cloud, and later decide if it makes sense to use Amazon S3. Besides, using the HDFS for a data migration can be a requirement before moving to Amazon S3. Specifically this can help to optimize the underlying layout of your HBase tables if they have a considerable amount of small HBase regions, and you want to merge them. This operation can be more quickly be performed on an HDFS cluster, and you can later migrate the data to Amazon S3. For more details, see the sections Reduce number of Regions and Data Migration . Finally, using HDFS is also the right choice if you have a cluster that is mostly used for write intensive workloads. This is because write intensive clusters are subject to intensive compaction and region splitting operations that are performed internally by HBase to manage the underlying data storage. In these cases, using Amazon S3 might not be the right option, because of data movements that occur between Amazon S3 and the cluster to perform compaction processes. This increases the time required to perform such operations, thus impacting the overall cluster performance resulting in higher latencies. On the other side, Amazon S3 is a good option for read-intensive HBase clusters. One of the best use cases where S3 excels is when the data that is most frequently accessed (read or modified) is the most recent, while old data is rarely modified. You can use the pre-configured bucket cache, to store a hot copy of the most recent data on local disks of your cluster, thus maintaining a good compromise in terms of costs and performance. For more details, see Bucket Cache . Another good use case for using Amazon S3 is when you have tables that rarely change over time, and you need to serve a large amount of read requests. In this case, you can opt for Amazon S3 in combination with the EMR HBase read-replica , to distribute your read requests across multiple clusters. For more details about this approach kindly see Data Integrity . Moreover, Amazon S3 provides stronger SLA for data durability and availability transparently at the storage level and will not be impacted by failures on EMR instances. Finally, one major benefit of relying on S3 for storage is cost saving. If you have significant costs in your cluster due to large amount of data stored on EBS volumes, moving to S3 can reduce costs drastically. Moreover, HDFS uses block replication to provide fault tolerance, which increases the footprint of data stored locally in your cluster. In Amazon EMR, the default HDFS replication factor is defined automatically when launching the cluster (or you can override it manually using the EMR configuration API ). For large tables size this can drastically increase EBS storage costs, so you might want to leverage S3 where replication is handled natively by the service for a more convenient cost. Which instance should I use? \u00b6 When talking about hardware requirements for HBase, it is very important to choose the right EC2 instance type when using HDFS as storage layer, as it might be prohibitive to change it once you have a live production cluster. On the other side, changing instances for an HBase cluster running on Amazon S3 is much easier as data is persisted on S3. This allows us to more easily terminate an EMR cluster without losing data and launch a new one using a different instance type. Below you can find some details that can help you to choose the right instances based on your use case / workloads requirements. HBase typically performs better with small instances and when you spread the overall requests across multiple instances. This is because there are some limitations in the number of HBase regions a single Region Server can handle, and having a huge amount of regions on a single node can lead to issues and unexpected behavior. For more details on determining the right number of regions for a specific instance, see the section Number of HBase Regions . Generally speaking, if you want to achieve the best possible performance in your HBase cluster, it\u2019s highly recommended to use EC2 instances powered with an Instance Store volume. This is especially true for write intensive / mixed (50% writes 50% reads) workloads. For such use cases, if you have significant write requests, you\u2019ll need disks that can provide a large amount of IOPS in order to accommodate all background operations performed by HBase (compaction, WAL writes). Using disk optimized instances allows you to sustain high volumes of write operations either if HBase is performing compaction or other background operations on disks. Some example of instances that are recommended for such workloads are: i3 / i3en provide dense SSD storage for data-intensive workloads. They provide the best performance for write intensive workloads but can be prohibitive depending on the amount of storage you want to use. They are recommended if you want to achieve the best possible performance, and if you want to cache several data in memory. m5d / r5d / c5d all these families provide NVMe SSD disks to deliver high random I/O performance. They can be used in different ways to exploit HBase features. For example, r5d can be used in combination with HBase off heap caching to maintain a significant amount of data cached in a performant memory (instead of reading data from the disks). On the other side, c5d comes with a higher proportion of vCPU compared to the memory, so they can be a better match if you need to serve huge volumes of requests on a single region server. To decide the right instance size, it\u2019s important to understand how many regions you\u2019re going to serve on a single region server. As general rule however, for large HBase tables, it\u2019s recommended to choose an instance type that can provide at least 32GB of memory dedicated for the HBase services (HMaster and Region Servers). Please note that by default Amazon EMR split the available memory of an instance between the YARN Node Manager and the HBase Region Server. For a list of default memory settings, see Default values for task configuration settings . You can always override the default EMR behavior using the EMR Configuration API. For more details see Modify Heap Memory . Number of HBase Regions \u00b6 As described in the HBase documentation , you can use the following formula to compute the number of HBase regions that should be hosted on a single region server. You should note that this is gives more of guideline about number of regions, but you should investigate and experiment on your workload to tune the number of regions: (REGION_SERVER_MEM_SIZE * MEMSTORE_FRACTION) / (MEMSTORE_SIZE * NUM_COLUMN_FAMILIES) REGION_SERVER_MEM_SIZE Memory allocated for the Region Server, as defined by the parameter -Xmx in hbase-env.sh MEMSTORE_FRACTION Memstore memory fraction, defined by hbase.regionserver.global.memstore.size (default 0.4) MEMSTORE_SIZE Memstore flush size (default 128MB) NUM_COLUMN_FAMILIES Number of column families defined for the table For example for a Region Server configured with 32GB of Heap memory and hosting a table with a single column family with the default HBase settings, we'll have an ideal allocation of regions equals to: # Number Recommended Regions (32GB * 0.4) / (128MB * 1) = 100 As previosly mentioned, this is a recommended setting that you can use as a starting point. For example, is not unfrequent to have a region server with 3 / 4 times the recommended value. However, to avoid impacting the performance it\u2019s better that you\u2019re not extensively using these extra regions for write operations to avoid extensive GC operations that might degrade performance or in worst cases failures that will force a Region Server restart.","title":"Introduction"},{"location":"applications/hbase/introduction/#introduction","text":"When working with Amazon EMR on EC2, you have the ability to choose between two deployment options for the underlying storage layer used by HBase: the Hadoop HDFS or Amazon S3 . Although there are no restrictions in the use of these storage options, they serve different purposes, and they both have pros and cons with related performance implications. In this document, we are going to review the main aspects of each storage option.","title":"Introduction"},{"location":"applications/hbase/introduction/#which-storage-layer-should-i-use","text":"Typically, to understand which storage layer you should use in your HBase cluster, you must determine what are your application requirements and decide what is most important between these two main decision drivers: performance or costs. Generally speaking, on a large cluster setup, HDFS provides better performance in most cases, while Amazon S3 provides better cost savings due to the reduced amount of storage required to persist all your data, and is the right option when you want to decouple your storage from compute. Using HDFS allows you to achieve the best performance for latency responses. This is true if you need milliseconds / sub-milliseconds read responses from HBase. You can also achieve similar results using Amazon S3 as storage layer, but this will require to rely on HBase caching features. Depending on your tables sizes, this can increase costs when provisioning resources for cache, as you\u2019ll have to provision more EBS volumes or use bigger instances to cache your data locally on the nodes, thus losing the main advantages of using Amazon S3. This requires to fine tune HBase to find the right balance between performance and cost for your workload. Another common use case to choose HDFS over S3 is a data migration from an on premise cluster. This is typically recommended as first migration step, as this solution provides similar performance compared to your existing cluster. You can more easily migrate your infrastructure to the cloud, and later decide if it makes sense to use Amazon S3. Besides, using the HDFS for a data migration can be a requirement before moving to Amazon S3. Specifically this can help to optimize the underlying layout of your HBase tables if they have a considerable amount of small HBase regions, and you want to merge them. This operation can be more quickly be performed on an HDFS cluster, and you can later migrate the data to Amazon S3. For more details, see the sections Reduce number of Regions and Data Migration . Finally, using HDFS is also the right choice if you have a cluster that is mostly used for write intensive workloads. This is because write intensive clusters are subject to intensive compaction and region splitting operations that are performed internally by HBase to manage the underlying data storage. In these cases, using Amazon S3 might not be the right option, because of data movements that occur between Amazon S3 and the cluster to perform compaction processes. This increases the time required to perform such operations, thus impacting the overall cluster performance resulting in higher latencies. On the other side, Amazon S3 is a good option for read-intensive HBase clusters. One of the best use cases where S3 excels is when the data that is most frequently accessed (read or modified) is the most recent, while old data is rarely modified. You can use the pre-configured bucket cache, to store a hot copy of the most recent data on local disks of your cluster, thus maintaining a good compromise in terms of costs and performance. For more details, see Bucket Cache . Another good use case for using Amazon S3 is when you have tables that rarely change over time, and you need to serve a large amount of read requests. In this case, you can opt for Amazon S3 in combination with the EMR HBase read-replica , to distribute your read requests across multiple clusters. For more details about this approach kindly see Data Integrity . Moreover, Amazon S3 provides stronger SLA for data durability and availability transparently at the storage level and will not be impacted by failures on EMR instances. Finally, one major benefit of relying on S3 for storage is cost saving. If you have significant costs in your cluster due to large amount of data stored on EBS volumes, moving to S3 can reduce costs drastically. Moreover, HDFS uses block replication to provide fault tolerance, which increases the footprint of data stored locally in your cluster. In Amazon EMR, the default HDFS replication factor is defined automatically when launching the cluster (or you can override it manually using the EMR configuration API ). For large tables size this can drastically increase EBS storage costs, so you might want to leverage S3 where replication is handled natively by the service for a more convenient cost.","title":"Which storage layer should I use?"},{"location":"applications/hbase/introduction/#which-instance-should-i-use","text":"When talking about hardware requirements for HBase, it is very important to choose the right EC2 instance type when using HDFS as storage layer, as it might be prohibitive to change it once you have a live production cluster. On the other side, changing instances for an HBase cluster running on Amazon S3 is much easier as data is persisted on S3. This allows us to more easily terminate an EMR cluster without losing data and launch a new one using a different instance type. Below you can find some details that can help you to choose the right instances based on your use case / workloads requirements. HBase typically performs better with small instances and when you spread the overall requests across multiple instances. This is because there are some limitations in the number of HBase regions a single Region Server can handle, and having a huge amount of regions on a single node can lead to issues and unexpected behavior. For more details on determining the right number of regions for a specific instance, see the section Number of HBase Regions . Generally speaking, if you want to achieve the best possible performance in your HBase cluster, it\u2019s highly recommended to use EC2 instances powered with an Instance Store volume. This is especially true for write intensive / mixed (50% writes 50% reads) workloads. For such use cases, if you have significant write requests, you\u2019ll need disks that can provide a large amount of IOPS in order to accommodate all background operations performed by HBase (compaction, WAL writes). Using disk optimized instances allows you to sustain high volumes of write operations either if HBase is performing compaction or other background operations on disks. Some example of instances that are recommended for such workloads are: i3 / i3en provide dense SSD storage for data-intensive workloads. They provide the best performance for write intensive workloads but can be prohibitive depending on the amount of storage you want to use. They are recommended if you want to achieve the best possible performance, and if you want to cache several data in memory. m5d / r5d / c5d all these families provide NVMe SSD disks to deliver high random I/O performance. They can be used in different ways to exploit HBase features. For example, r5d can be used in combination with HBase off heap caching to maintain a significant amount of data cached in a performant memory (instead of reading data from the disks). On the other side, c5d comes with a higher proportion of vCPU compared to the memory, so they can be a better match if you need to serve huge volumes of requests on a single region server. To decide the right instance size, it\u2019s important to understand how many regions you\u2019re going to serve on a single region server. As general rule however, for large HBase tables, it\u2019s recommended to choose an instance type that can provide at least 32GB of memory dedicated for the HBase services (HMaster and Region Servers). Please note that by default Amazon EMR split the available memory of an instance between the YARN Node Manager and the HBase Region Server. For a list of default memory settings, see Default values for task configuration settings . You can always override the default EMR behavior using the EMR Configuration API. For more details see Modify Heap Memory .","title":"Which instance should I use?"},{"location":"applications/hbase/introduction/#number-of-hbase-regions","text":"As described in the HBase documentation , you can use the following formula to compute the number of HBase regions that should be hosted on a single region server. You should note that this is gives more of guideline about number of regions, but you should investigate and experiment on your workload to tune the number of regions: (REGION_SERVER_MEM_SIZE * MEMSTORE_FRACTION) / (MEMSTORE_SIZE * NUM_COLUMN_FAMILIES) REGION_SERVER_MEM_SIZE Memory allocated for the Region Server, as defined by the parameter -Xmx in hbase-env.sh MEMSTORE_FRACTION Memstore memory fraction, defined by hbase.regionserver.global.memstore.size (default 0.4) MEMSTORE_SIZE Memstore flush size (default 128MB) NUM_COLUMN_FAMILIES Number of column families defined for the table For example for a Region Server configured with 32GB of Heap memory and hosting a table with a single column family with the default HBase settings, we'll have an ideal allocation of regions equals to: # Number Recommended Regions (32GB * 0.4) / (128MB * 1) = 100 As previosly mentioned, this is a recommended setting that you can use as a starting point. For example, is not unfrequent to have a region server with 3 / 4 times the recommended value. However, to avoid impacting the performance it\u2019s better that you\u2019re not extensively using these extra regions for write operations to avoid extensive GC operations that might degrade performance or in worst cases failures that will force a Region Server restart.","title":"Number of HBase Regions"},{"location":"applications/hbase/management/","text":"Management \u00b6 This section highlights some commands and best practices, that can help you to manage your HBase clusters on Amazon EMR. Create command alias \u00b6 If you administrate your HBase cluster mainly from the shell of the EMR master, it might be convenient to define a command alias to avoid permission issues you might incur by typing erroneous commands using a different user (e.g. root). As best practice, you should always run HBase commands as hbase user. In order to do that, you can add the following alias in the ~/.bashrc profile for the user that you use to administrate your cluster (e.g. hadoop) alias hbase = 'sudo -u hbase hbase' Once done, you can safely run HBase commands as usual hbase shell Determine average row size \u00b6 If you want to determine the average size of a row stored in a HBase table, you can use the following commands to retrieve the payload from a storefile of the table. For example: # Simple notation hbase hfile -m -f $HBASE_PATH # Extended notation hbase org.apache.hadoop.hbase.io.hfile.HFile -m -f $HBASE_PATH The class org.apache.hadoop.hbase.io.hfile.HFile allows you to analyze HBase store files that are persisted on HDFS or S3. The option -m returns the metadata for the file analyzed that reports the average size (bytes) of the Row Key in that particular file, and the average size (bytes) of the values stored in that file. To get a rough estimation of the average payload of a single row, you can sum the parameters avgKeyLen and avgValueLen that are returned in the previous command, to get the average size in bytes of a row. For example: # row_avg_size = avgKeyLen + avgValueLen row_avg_size = 19 + 7 = 26 This command might be useful to get a rough estimate of your data payload when you are not sure about it. You can later on use this value to fine-tuning your cluster (e.g. increase/decrease RPC Listeners ) Reduce number of Regions \u00b6 The HBase community introduced a new utility in the hbase-tools package that helps to reduce the number of regions for tables stored in HBase. This utility is available in the class org.apache.hbase.RegionsMerger and can help you to automatically merge the number of regions to a value that you define, if you have a high count in your cluster (e.g. wrong table pre-split, or high split rate due to incorrect settings) # copy library in classpath sudo cp /usr/lib/hbase-operator-tools/hbase-tools-*.jar /usr/lib/hbase/lib/ # merge regions hbase org.apache.hbase.RegionsMerger <TABLE_NAME> <TARGET_NUMBER_OF_REGIONS> This tool is available in HBase versions >= 2.x.x and should only be used with these versions.","title":"Management"},{"location":"applications/hbase/management/#management","text":"This section highlights some commands and best practices, that can help you to manage your HBase clusters on Amazon EMR.","title":"Management"},{"location":"applications/hbase/management/#create-command-alias","text":"If you administrate your HBase cluster mainly from the shell of the EMR master, it might be convenient to define a command alias to avoid permission issues you might incur by typing erroneous commands using a different user (e.g. root). As best practice, you should always run HBase commands as hbase user. In order to do that, you can add the following alias in the ~/.bashrc profile for the user that you use to administrate your cluster (e.g. hadoop) alias hbase = 'sudo -u hbase hbase' Once done, you can safely run HBase commands as usual hbase shell","title":"Create command alias"},{"location":"applications/hbase/management/#determine-average-row-size","text":"If you want to determine the average size of a row stored in a HBase table, you can use the following commands to retrieve the payload from a storefile of the table. For example: # Simple notation hbase hfile -m -f $HBASE_PATH # Extended notation hbase org.apache.hadoop.hbase.io.hfile.HFile -m -f $HBASE_PATH The class org.apache.hadoop.hbase.io.hfile.HFile allows you to analyze HBase store files that are persisted on HDFS or S3. The option -m returns the metadata for the file analyzed that reports the average size (bytes) of the Row Key in that particular file, and the average size (bytes) of the values stored in that file. To get a rough estimation of the average payload of a single row, you can sum the parameters avgKeyLen and avgValueLen that are returned in the previous command, to get the average size in bytes of a row. For example: # row_avg_size = avgKeyLen + avgValueLen row_avg_size = 19 + 7 = 26 This command might be useful to get a rough estimate of your data payload when you are not sure about it. You can later on use this value to fine-tuning your cluster (e.g. increase/decrease RPC Listeners )","title":"Determine average row size"},{"location":"applications/hbase/management/#reduce-number-of-regions","text":"The HBase community introduced a new utility in the hbase-tools package that helps to reduce the number of regions for tables stored in HBase. This utility is available in the class org.apache.hbase.RegionsMerger and can help you to automatically merge the number of regions to a value that you define, if you have a high count in your cluster (e.g. wrong table pre-split, or high split rate due to incorrect settings) # copy library in classpath sudo cp /usr/lib/hbase-operator-tools/hbase-tools-*.jar /usr/lib/hbase/lib/ # merge regions hbase org.apache.hbase.RegionsMerger <TABLE_NAME> <TARGET_NUMBER_OF_REGIONS> This tool is available in HBase versions >= 2.x.x and should only be used with these versions.","title":"Reduce number of Regions"},{"location":"applications/hbase/observability/","text":"Observability \u00b6 This section describes how you can leverage existing monitoring tools and features available in Amazon EMR to monitor and troubleshoot issues that might occur when working with HBase. Logs \u00b6 By default, Amazon EMR stores HBase logs under the following directory: /var/log/hbase . Depending on the role of the node you are connected to (e.g. MASTER, CORE or TASK) you might see different types of log files stored within this folder. Below a list of log files generated by HBase: hbase-hbase-master-ip-xxx-xxx-xxx-xxx.log This file is present on the EMR master node only and provides details about operations performed by the HBase master service. In an impaired cluster, this is typically the first file to look at to identify the origin of the problem. hbase-hbase-rest-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about REST API requests. If you\u2019re not using the HTTP REST API to communicate with HBase, then you can skip this file. By default, Amazon EMR exposes HTTP REST API for HBase on port 8070. hbase-hbase-thrift-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about connections instantiated through the thrift interface of HBase. By default, Amazon EMR exposes the Thrift server on port 9090. hbase-hbase-regionserver-ip-xxx-xxx-xxx-xxx.log Available on CORE and TASK nodes only. It collects log entries generated by the region server running on the node. Should be reviewed in case of issues on the specific node. hbase.log This file logs the output of client commands issued by end users. For example, if you launch the command hbase hbck -details on the EMR master node, this file will trace the default logger of the hbck utility (which does not correspond to the stdout messages generated by the command). SecurityAuth.audit This file records user accesses to HBase tables, and it\u2019s mainly used for audit purposes. In order to collect useful logs, it\u2019s required to enable Kerberos authentication and enable the Access Controller capabilities of the HBase co-processor to provide more details in terms of tables accessed by the users. For an example configuration, see Security/Authorization In order to modify the default log level of HBase, you can modify the hbase-log4j classification when launching an EMR cluster. For example, the below configuration will enable TRACE logging to have a more granular audit of user accesses to HBase tables (requires to enable Security/Authorization ). [ { \"Classification\" : \"hbase-log4j\" , \"Properties\" : { \"log4j.category.SecurityLogger\" : \"TRACE,RFAS\" , \"log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController\" : \"TRACE\" , \"log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.visibility.VisibilityController\" : \"TRACE\" } } ] If you enabled the EMR logging on Amazon S3 these files will also be periodically collected on the S3 bucket that you choose when launching the cluster. Monitoring HBase \u00b6 Web UI \u00b6 By default, HBase provides out of the box a Web UI that you can leverage to quickly check the status of your cluster and review detailed metrics of cluster utilization on a friendly interface. Amazon EMR exposes the HBase Web UIs at the following addresses: HBase Master http://master-dns-name:16010/ HBase Region http://region-dns-name:16030/ The HBase Master Web UI is typically the first entry point to check the status of your cluster. It provides details about the current number of tables hosted on the cluster, details about regions servers, cache utilization and much more. In HBase 2.x the Web UI was further improved, thus making it a valuable and quick tool to check the health and status of your HBase cluster. Below a sample image of the HBase master Web UI. Ganglia \u00b6 Another way to monitor HBase is with the help of Ganglia, which can be installed while provisioning your EMR cluster. Ganglia automatically collects HBase metrics and allows you to create reports and custom visualizations for the metrics of interests. The project has been discontinued in the last years, but it can still provide you with detailed information on your HBase utilization. This software should be installed if you need a quick and easy way for monitoring you HBase cluster. To install Ganglia in your cluster, please see Monitor HBase with Ganglia . Grafana & Prometheus \u00b6 Grafana in combination with Prometheus provides a better and more rich experience to monitor your HBase cluster that can be customized based on your needs. In order to monitor your HBase cluster using a Grafana dashboard, you should install the prometheus node_exporter utility on each node to collect additional details from the nodes (CPU, Memory, Disk, Network metrics) and the Prometheus JMX exporter to prepare HBase JMX metrics for prometheus scraping. Please refer to the following scripts for additional information on the setup and configurations of the components previously described: emr-ba-prometheus_exporter.sh Node Exporter and Prometheus JMX exporter setup emr-step-monitoring_apps.sh Prometheus & Grafana setup Finally, in order to expose HBase JMX metrics from the HBase Master and Region Servers it is required to set the following variables in the /etc/hbase/conf/hbase-env.sh script: export HBASE_MASTER_OPTS = \" $HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\" export HBASE_REGIONSERVER_OPTS = \" $HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\" This can easily be achieved using the following EMR Configurations [ { \"Classification\" : \"hbase-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_REGIONSERVER_OPTS\" : \"\\\"$HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\\"\" , \"HBASE_MASTER_OPTS\" : \"\\\"$HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\\"\" } } ], \"Properties\" : {} } ] A sample CloudFormation template with related dashboard can be found here . JMX Metrics \u00b6 By default, HBase generates a significant amount of metrics that can be used to monitor the status and utilization of your cluster. To retrieve a list of all the metrics exposed by the cluster with related descriptions, you can directly invoke the HBase REST API using the following addresses: HBase Master http://master-dns-name:16010/jmx?description=true HBase Region http://region-dns-name:16030/jmx?description=true For more details on JMX metrics, see HBase Metrics in the official HBase documentation. Monitoring HDFS \u00b6 Amazon EMR pushes metrics to Amazon CloudWatch , a service that monitors your Amazon Web Services (AWS) resources, where you can define alarms for Amazon EMR metrics. For example, you can configure an alarm to receive a notification any time the HDFS utilization rises above 80%. This can help you to detect problems that might occur in your HBase cluster due to the full utilization of the HDFS space. Typically, it\u2019s useful to define an alarm for the following metrics: HDFSUtilization - The percentage of HDFS storage currently used MissingBlocks - The number of blocks in which HDFS has no replicas UnderReplicatedBlocks - The number of blocks that need to be replicated one or more times. For a list of all the metrics published by Amazon EMR, see Monitor metrics with CloudWatch in the EMR documentation. To configure an alarm for the previously mentioned metrics, see Create or edit a CloudWatch alarm in the Amazon CloudWatch User Guide. Monitoring Amazon S3 \u00b6 Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. Amazon S3 provides different tools that you can use to both monitor and audit the operations that are performed on the service. When using Amazon S3 as storage layer for HBase, there are typically two main information that you might want to get out from the S3 service: size of the data stored in the bucket that can help you to estimate costs, and number of 5xx errors that might indicate throttling operations for S3 calls. In this case, you can use S3 Storage Lens that can provide visibility into object-storage usage and activity. Once enabled, the service generates summary insights that you can review using a pre-built service dashboard. The dashboards can be generated for individual buckets or a list of buckets within the same region. When monitoring HBase, we recommend to enable S3 Storage Lens with the following configurations: Dashboard Scope This mainly depends on how you want to monitor S3. You might generate individual dashboards for each bucket or a more comprehensive dashboard for a single region if you have multiple HBase clusters using S3 as storage layer. Metrics Selection Enable the \u2018\u201cAdvanced Metrics and Recommendations\u201d and select Activity Metrics and Detailed Status Code Metrics. Please note that the following metrics are available with additional costs, however they provide a more granular visibility about issues that you might experience on a specific bucket. Amazon S3 also provides native integrations with Amazon CloudWatch and CloudTrail for a more granular control on the operations performed against S3 buckets. However, if you don\u2019t have strong auditing requirements, the S3 Storage Lens capabilities are usually sufficient to monitor S3 when using HBase. For additional details on S3 monitoring tools available on the service, see Monitoring Amazon S3 in the Amazon S3 documentation. Finally, in some cases it might be useful to show in the HBase logs extended details for S3 calls performed by the HBase master and Region Servers. This information is useful to troubleshoot unusual behavior in the cluster or in case of issues. In this specific cases, you might want change the default logging for S3 calls using the following EMR configurations: [ { \"Classification\" : \"hbase-log4j\" , \"Properties\" : { \"log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.request\" : \"DEBUG\" , \"log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.latency\" : \"ERROR\" } } ]","title":"Observability"},{"location":"applications/hbase/observability/#observability","text":"This section describes how you can leverage existing monitoring tools and features available in Amazon EMR to monitor and troubleshoot issues that might occur when working with HBase.","title":"Observability"},{"location":"applications/hbase/observability/#logs","text":"By default, Amazon EMR stores HBase logs under the following directory: /var/log/hbase . Depending on the role of the node you are connected to (e.g. MASTER, CORE or TASK) you might see different types of log files stored within this folder. Below a list of log files generated by HBase: hbase-hbase-master-ip-xxx-xxx-xxx-xxx.log This file is present on the EMR master node only and provides details about operations performed by the HBase master service. In an impaired cluster, this is typically the first file to look at to identify the origin of the problem. hbase-hbase-rest-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about REST API requests. If you\u2019re not using the HTTP REST API to communicate with HBase, then you can skip this file. By default, Amazon EMR exposes HTTP REST API for HBase on port 8070. hbase-hbase-thrift-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about connections instantiated through the thrift interface of HBase. By default, Amazon EMR exposes the Thrift server on port 9090. hbase-hbase-regionserver-ip-xxx-xxx-xxx-xxx.log Available on CORE and TASK nodes only. It collects log entries generated by the region server running on the node. Should be reviewed in case of issues on the specific node. hbase.log This file logs the output of client commands issued by end users. For example, if you launch the command hbase hbck -details on the EMR master node, this file will trace the default logger of the hbck utility (which does not correspond to the stdout messages generated by the command). SecurityAuth.audit This file records user accesses to HBase tables, and it\u2019s mainly used for audit purposes. In order to collect useful logs, it\u2019s required to enable Kerberos authentication and enable the Access Controller capabilities of the HBase co-processor to provide more details in terms of tables accessed by the users. For an example configuration, see Security/Authorization In order to modify the default log level of HBase, you can modify the hbase-log4j classification when launching an EMR cluster. For example, the below configuration will enable TRACE logging to have a more granular audit of user accesses to HBase tables (requires to enable Security/Authorization ). [ { \"Classification\" : \"hbase-log4j\" , \"Properties\" : { \"log4j.category.SecurityLogger\" : \"TRACE,RFAS\" , \"log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController\" : \"TRACE\" , \"log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.visibility.VisibilityController\" : \"TRACE\" } } ] If you enabled the EMR logging on Amazon S3 these files will also be periodically collected on the S3 bucket that you choose when launching the cluster.","title":"Logs"},{"location":"applications/hbase/observability/#monitoring-hbase","text":"","title":"Monitoring HBase"},{"location":"applications/hbase/observability/#web-ui","text":"By default, HBase provides out of the box a Web UI that you can leverage to quickly check the status of your cluster and review detailed metrics of cluster utilization on a friendly interface. Amazon EMR exposes the HBase Web UIs at the following addresses: HBase Master http://master-dns-name:16010/ HBase Region http://region-dns-name:16030/ The HBase Master Web UI is typically the first entry point to check the status of your cluster. It provides details about the current number of tables hosted on the cluster, details about regions servers, cache utilization and much more. In HBase 2.x the Web UI was further improved, thus making it a valuable and quick tool to check the health and status of your HBase cluster. Below a sample image of the HBase master Web UI.","title":"Web UI"},{"location":"applications/hbase/observability/#ganglia","text":"Another way to monitor HBase is with the help of Ganglia, which can be installed while provisioning your EMR cluster. Ganglia automatically collects HBase metrics and allows you to create reports and custom visualizations for the metrics of interests. The project has been discontinued in the last years, but it can still provide you with detailed information on your HBase utilization. This software should be installed if you need a quick and easy way for monitoring you HBase cluster. To install Ganglia in your cluster, please see Monitor HBase with Ganglia .","title":"Ganglia"},{"location":"applications/hbase/observability/#grafana-prometheus","text":"Grafana in combination with Prometheus provides a better and more rich experience to monitor your HBase cluster that can be customized based on your needs. In order to monitor your HBase cluster using a Grafana dashboard, you should install the prometheus node_exporter utility on each node to collect additional details from the nodes (CPU, Memory, Disk, Network metrics) and the Prometheus JMX exporter to prepare HBase JMX metrics for prometheus scraping. Please refer to the following scripts for additional information on the setup and configurations of the components previously described: emr-ba-prometheus_exporter.sh Node Exporter and Prometheus JMX exporter setup emr-step-monitoring_apps.sh Prometheus & Grafana setup Finally, in order to expose HBase JMX metrics from the HBase Master and Region Servers it is required to set the following variables in the /etc/hbase/conf/hbase-env.sh script: export HBASE_MASTER_OPTS = \" $HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\" export HBASE_REGIONSERVER_OPTS = \" $HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\" This can easily be achieved using the following EMR Configurations [ { \"Classification\" : \"hbase-env\" , \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HBASE_REGIONSERVER_OPTS\" : \"\\\"$HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\\"\" , \"HBASE_MASTER_OPTS\" : \"\\\"$HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\\"\" } } ], \"Properties\" : {} } ] A sample CloudFormation template with related dashboard can be found here .","title":"Grafana &amp; Prometheus"},{"location":"applications/hbase/observability/#jmx-metrics","text":"By default, HBase generates a significant amount of metrics that can be used to monitor the status and utilization of your cluster. To retrieve a list of all the metrics exposed by the cluster with related descriptions, you can directly invoke the HBase REST API using the following addresses: HBase Master http://master-dns-name:16010/jmx?description=true HBase Region http://region-dns-name:16030/jmx?description=true For more details on JMX metrics, see HBase Metrics in the official HBase documentation.","title":"JMX Metrics"},{"location":"applications/hbase/observability/#monitoring-hdfs","text":"Amazon EMR pushes metrics to Amazon CloudWatch , a service that monitors your Amazon Web Services (AWS) resources, where you can define alarms for Amazon EMR metrics. For example, you can configure an alarm to receive a notification any time the HDFS utilization rises above 80%. This can help you to detect problems that might occur in your HBase cluster due to the full utilization of the HDFS space. Typically, it\u2019s useful to define an alarm for the following metrics: HDFSUtilization - The percentage of HDFS storage currently used MissingBlocks - The number of blocks in which HDFS has no replicas UnderReplicatedBlocks - The number of blocks that need to be replicated one or more times. For a list of all the metrics published by Amazon EMR, see Monitor metrics with CloudWatch in the EMR documentation. To configure an alarm for the previously mentioned metrics, see Create or edit a CloudWatch alarm in the Amazon CloudWatch User Guide.","title":"Monitoring HDFS"},{"location":"applications/hbase/observability/#monitoring-amazon-s3","text":"Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. Amazon S3 provides different tools that you can use to both monitor and audit the operations that are performed on the service. When using Amazon S3 as storage layer for HBase, there are typically two main information that you might want to get out from the S3 service: size of the data stored in the bucket that can help you to estimate costs, and number of 5xx errors that might indicate throttling operations for S3 calls. In this case, you can use S3 Storage Lens that can provide visibility into object-storage usage and activity. Once enabled, the service generates summary insights that you can review using a pre-built service dashboard. The dashboards can be generated for individual buckets or a list of buckets within the same region. When monitoring HBase, we recommend to enable S3 Storage Lens with the following configurations: Dashboard Scope This mainly depends on how you want to monitor S3. You might generate individual dashboards for each bucket or a more comprehensive dashboard for a single region if you have multiple HBase clusters using S3 as storage layer. Metrics Selection Enable the \u2018\u201cAdvanced Metrics and Recommendations\u201d and select Activity Metrics and Detailed Status Code Metrics. Please note that the following metrics are available with additional costs, however they provide a more granular visibility about issues that you might experience on a specific bucket. Amazon S3 also provides native integrations with Amazon CloudWatch and CloudTrail for a more granular control on the operations performed against S3 buckets. However, if you don\u2019t have strong auditing requirements, the S3 Storage Lens capabilities are usually sufficient to monitor S3 when using HBase. For additional details on S3 monitoring tools available on the service, see Monitoring Amazon S3 in the Amazon S3 documentation. Finally, in some cases it might be useful to show in the HBase logs extended details for S3 calls performed by the HBase master and Region Servers. This information is useful to troubleshoot unusual behavior in the cluster or in case of issues. In this specific cases, you might want change the default logging for S3 calls using the following EMR configurations: [ { \"Classification\" : \"hbase-log4j\" , \"Properties\" : { \"log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.request\" : \"DEBUG\" , \"log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.latency\" : \"ERROR\" } } ]","title":"Monitoring Amazon S3"},{"location":"applications/hbase/performance_tests/","text":"Performance Tests \u00b6 One of the most important operations to perform before start using an HBase cluster is to perform a stress test to verify if the provisioned infrastructure meets the requirements in terms of latency and throughput for your applications. In this section, we\u2019re going to explore some tools that can help to validate the provisioned infrastructure in terms of performance. For optimal results, it\u2019s recommended to setup a monitoring tool as described in the Observability section to collect advanced metrics from the tests. Evaluation Framework \u00b6 Typically, there are different aspects you want to check depending on how your cluster will be used. However, there are two major metrics that are important to define a baseline for the cluster performance: operation throughput (number of requests we can serve for a specific operation in a given period of time, e.g. GET) and operation latency (time required to acknowledge a client request). It\u2019s very important to baseline these metrics in a production cluster. They will give you hints on when to scale nodes based on clients requests during the day, and they can suggest configuration tuning if it is not matching expected performance. Typically, you can perform a benchmark in an HBase cluster following the steps below: Write / data load This is always the first step in the process as you should populate some tables with mock data to perform read tests or simply to evaluate the maximum throughput you can achieve during write operations. For this test, it is important to mimic as much as possible the average payload size of the data that will be ingested in the cluster. This can help to evaluate the number of compactions performed with the ingested volume and see the performance degradation that you might expect during these operations. Besides, this will also give you an idea of the maximum number of write requests you can serve with the specified cluster topology. Read / latency / cache This is the next step to define our baseline. The major aim of this test should be to verify the max throughput that the cluster can serve and understand how well you are leveraging the HBase cache to improve response latency. As best practice for running these tests, you can follow the following rules: Separate the clients from the HBase cluster. The goal is to collect metrics without having to care about resources used in our cluster. So as best practice, you should run your client fleet on a separate cluster. If your clients are on a separate cluster, make sure that your fleet is co-located on the same subnet of the cluster. This will improve response latency and avoid extra costs you might incur for data trasfer across Availability Zones. Use EMR Configurations defined as JSON files that are stored on an Amazon S3 bucket to launch your test clusters. This will help you to more easily export configurations used in your QA environment to production. Moreover, it will be easier to track specific configurations used in a test cluster, rather than setting them manually while launching the cluster. The following section describes some tools that can be used to baseline an HBase cluster. Performance Evaluation Tool \u00b6 The first tool we\u2019re going to use is the HBase Performance Evaluation utility that is already available in your Amazon EMR cluster. This utility can be invoked using the following syntax from the EMR master node: hbase pe <OPTIONS> <command> <nclients> The tool allow us to perform both write and read operations specifying different options to control several aspects of our tests (e.g. create a partitioned table, disable WAL flush, etc.) For example, the following command allows us to create a table called MyWriteTest that will be pre-partitioned with 200 regions (--presplit) and we\u2019re going to write 2GB (--size) of data using a single client. We also enable the latency parameter to report operation latencies that help us to identify if the response time met our requirements. hbase pe --table = MyWriteTest --presplit = 200 --size = 2 --latency --nomapred randomWrite 1 As described in the Log section, the log output will be stored in the /var/log/hbase/hbase.log file. Please make sure to run the previous command as hbase user, or you\u2019ll not have the permissions to modify this file using the standard hadoop user. The following shows a sample output for the previous command: INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Latency ( us ) : mean = 21.45 , min = 1.00 , max = 480941.00 , stdDev = 992.53 , 50 th = 2.00 , 75 th = 2.00 , 95 th = 2.00 , 99 th = 3.00 , 99.9 th = 24.00 , 99.99 th = 37550.00 , 99.999 th = 46364.23 INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Num measures ( latency ) : 2097151 INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Mean = 21.45 ... INFO [ TestClient -0 ] hbase . PerformanceEvaluation : No valueSize statistics available INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Finished class org . apache . hadoop . hbase . PerformanceEvaluation$RandomWriteTest in 42448 ms at offset 0 for 2097152 rows ( 48.62 MB / s ) INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Finished TestClient -0 in 42448 ms over 2097152 rows INFO [ main ] hbase . PerformanceEvaluation : [ RandomWriteTest ] Summary of timings ( ms ) : [ 42448 ] INFO [ main ] hbase . PerformanceEvaluation : [ RandomWriteTest duration ] Min : 42448 ms Max : 42448 ms Avg : 42448 ms INFO [ main ] hbase . PerformanceEvaluation : [ Avg latency ( us )] 21 INFO [ main ] hbase . PerformanceEvaluation : [ Avg TPS / QPS ] 49405 row per second As you can see this will report min, max and avg response latency for our write requests, along with throughput information about the max number of calls served by the cluster. Please note that in our example we used the nomapred parameter that will use a local thread to perform the test (in this case client resides on the EMR master node). If we want to generate a higher number of requests is better to remove this option, so that the utility will use a Map Reduce (MR) job to perform the test. In this last scenario, it might be convenient to run the MR job on a separate cluster, to avoid using resources (cpu, network bandwidth) from our HBase cluster and gather more realistic results. For example, the same tests can performed from a separate EMR cluster adding the following parameter: -Dhbase.zookeeper.quorum=TARGET_HBASE_MASTER_DNS , and replacing TARGET_HBASE_MASTER_DNS with the EMR master hostname we want to test. hbase pe -Dhbase.zookeeper.quorum = ip-xxx-xx-x-xxx.compute.internal --table = MyWriteTestTwo --presplit = 200 --size = 2 --latency randomWrite 1 In the same way we can perform Read test operations. For a detailed list of all options and tests available in the utility, please check the help section of the tool from ther command line. YCSB \u00b6 Another popular tool to benchmark your HBase cluster is YCSB (Yahoo Cloud Serving Benchmark). This utility is not available on Amazon EMR, so it should be manually installed on the EMR master itself, or on a separate EC2 instance. This tool, unlike the previous one, is more focused on testing workloads patterns. In fact, in doesn\u2019t provide several options as the HBase PE utility, but allows you to define different types of workloads (typically called workload A,B,C,D, etc.) where you can mix different volumes of write/read/mutate operations, along with sizes of the data that are going to be read or modified. By default, the tool comes with pre-defined templates to tests some standard workloads patterns. For example, workload A performs 50% of read operations and 50% of update operations using 1KB payloads for each row. This tool is especially useful, when you know exactly your workloads patterns, and you want to simulate more realistic use cases. However, please note that the tool can only launch multithreaded clients on the same node. So if you have a large cluster that you want to test, you\u2019ll have to configure a fleet of EC2 instances and run the clients from each node using some automation scripts.","title":"Performance Tests"},{"location":"applications/hbase/performance_tests/#performance-tests","text":"One of the most important operations to perform before start using an HBase cluster is to perform a stress test to verify if the provisioned infrastructure meets the requirements in terms of latency and throughput for your applications. In this section, we\u2019re going to explore some tools that can help to validate the provisioned infrastructure in terms of performance. For optimal results, it\u2019s recommended to setup a monitoring tool as described in the Observability section to collect advanced metrics from the tests.","title":"Performance Tests"},{"location":"applications/hbase/performance_tests/#evaluation-framework","text":"Typically, there are different aspects you want to check depending on how your cluster will be used. However, there are two major metrics that are important to define a baseline for the cluster performance: operation throughput (number of requests we can serve for a specific operation in a given period of time, e.g. GET) and operation latency (time required to acknowledge a client request). It\u2019s very important to baseline these metrics in a production cluster. They will give you hints on when to scale nodes based on clients requests during the day, and they can suggest configuration tuning if it is not matching expected performance. Typically, you can perform a benchmark in an HBase cluster following the steps below: Write / data load This is always the first step in the process as you should populate some tables with mock data to perform read tests or simply to evaluate the maximum throughput you can achieve during write operations. For this test, it is important to mimic as much as possible the average payload size of the data that will be ingested in the cluster. This can help to evaluate the number of compactions performed with the ingested volume and see the performance degradation that you might expect during these operations. Besides, this will also give you an idea of the maximum number of write requests you can serve with the specified cluster topology. Read / latency / cache This is the next step to define our baseline. The major aim of this test should be to verify the max throughput that the cluster can serve and understand how well you are leveraging the HBase cache to improve response latency. As best practice for running these tests, you can follow the following rules: Separate the clients from the HBase cluster. The goal is to collect metrics without having to care about resources used in our cluster. So as best practice, you should run your client fleet on a separate cluster. If your clients are on a separate cluster, make sure that your fleet is co-located on the same subnet of the cluster. This will improve response latency and avoid extra costs you might incur for data trasfer across Availability Zones. Use EMR Configurations defined as JSON files that are stored on an Amazon S3 bucket to launch your test clusters. This will help you to more easily export configurations used in your QA environment to production. Moreover, it will be easier to track specific configurations used in a test cluster, rather than setting them manually while launching the cluster. The following section describes some tools that can be used to baseline an HBase cluster.","title":"Evaluation Framework"},{"location":"applications/hbase/performance_tests/#performance-evaluation-tool","text":"The first tool we\u2019re going to use is the HBase Performance Evaluation utility that is already available in your Amazon EMR cluster. This utility can be invoked using the following syntax from the EMR master node: hbase pe <OPTIONS> <command> <nclients> The tool allow us to perform both write and read operations specifying different options to control several aspects of our tests (e.g. create a partitioned table, disable WAL flush, etc.) For example, the following command allows us to create a table called MyWriteTest that will be pre-partitioned with 200 regions (--presplit) and we\u2019re going to write 2GB (--size) of data using a single client. We also enable the latency parameter to report operation latencies that help us to identify if the response time met our requirements. hbase pe --table = MyWriteTest --presplit = 200 --size = 2 --latency --nomapred randomWrite 1 As described in the Log section, the log output will be stored in the /var/log/hbase/hbase.log file. Please make sure to run the previous command as hbase user, or you\u2019ll not have the permissions to modify this file using the standard hadoop user. The following shows a sample output for the previous command: INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Latency ( us ) : mean = 21.45 , min = 1.00 , max = 480941.00 , stdDev = 992.53 , 50 th = 2.00 , 75 th = 2.00 , 95 th = 2.00 , 99 th = 3.00 , 99.9 th = 24.00 , 99.99 th = 37550.00 , 99.999 th = 46364.23 INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Num measures ( latency ) : 2097151 INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Mean = 21.45 ... INFO [ TestClient -0 ] hbase . PerformanceEvaluation : No valueSize statistics available INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Finished class org . apache . hadoop . hbase . PerformanceEvaluation$RandomWriteTest in 42448 ms at offset 0 for 2097152 rows ( 48.62 MB / s ) INFO [ TestClient -0 ] hbase . PerformanceEvaluation : Finished TestClient -0 in 42448 ms over 2097152 rows INFO [ main ] hbase . PerformanceEvaluation : [ RandomWriteTest ] Summary of timings ( ms ) : [ 42448 ] INFO [ main ] hbase . PerformanceEvaluation : [ RandomWriteTest duration ] Min : 42448 ms Max : 42448 ms Avg : 42448 ms INFO [ main ] hbase . PerformanceEvaluation : [ Avg latency ( us )] 21 INFO [ main ] hbase . PerformanceEvaluation : [ Avg TPS / QPS ] 49405 row per second As you can see this will report min, max and avg response latency for our write requests, along with throughput information about the max number of calls served by the cluster. Please note that in our example we used the nomapred parameter that will use a local thread to perform the test (in this case client resides on the EMR master node). If we want to generate a higher number of requests is better to remove this option, so that the utility will use a Map Reduce (MR) job to perform the test. In this last scenario, it might be convenient to run the MR job on a separate cluster, to avoid using resources (cpu, network bandwidth) from our HBase cluster and gather more realistic results. For example, the same tests can performed from a separate EMR cluster adding the following parameter: -Dhbase.zookeeper.quorum=TARGET_HBASE_MASTER_DNS , and replacing TARGET_HBASE_MASTER_DNS with the EMR master hostname we want to test. hbase pe -Dhbase.zookeeper.quorum = ip-xxx-xx-x-xxx.compute.internal --table = MyWriteTestTwo --presplit = 200 --size = 2 --latency randomWrite 1 In the same way we can perform Read test operations. For a detailed list of all options and tests available in the utility, please check the help section of the tool from ther command line.","title":"Performance Evaluation Tool"},{"location":"applications/hbase/performance_tests/#ycsb","text":"Another popular tool to benchmark your HBase cluster is YCSB (Yahoo Cloud Serving Benchmark). This utility is not available on Amazon EMR, so it should be manually installed on the EMR master itself, or on a separate EC2 instance. This tool, unlike the previous one, is more focused on testing workloads patterns. In fact, in doesn\u2019t provide several options as the HBase PE utility, but allows you to define different types of workloads (typically called workload A,B,C,D, etc.) where you can mix different volumes of write/read/mutate operations, along with sizes of the data that are going to be read or modified. By default, the tool comes with pre-defined templates to tests some standard workloads patterns. For example, workload A performs 50% of read operations and 50% of update operations using 1KB payloads for each row. This tool is especially useful, when you know exactly your workloads patterns, and you want to simulate more realistic use cases. However, please note that the tool can only launch multithreaded clients on the same node. So if you have a large cluster that you want to test, you\u2019ll have to configure a fleet of EC2 instances and run the clients from each node using some automation scripts.","title":"YCSB"},{"location":"applications/hbase/security/","text":"Security \u00b6 The following section describes the main security aspects that can help you to secure an HBase cluster running on Amazon EMR. Authentication \u00b6 By default when launching an Amazon EMR cluster with HBase installed, the service will configure HBase without enabling any type of authentication. This allows every client connecting to HBase to read / write tables stored in the cluster without the need to provide any credentials. In this context it is a best practice to limit access to the cluster by scoping access to the cluster using firewalls or EMR Security Groups attached to the cluster. For more details see Networking However, if you require to enable a strong authentication system, you can use Kerberos authentication to secure your cluster. HBase implements the Simple Authentication and Security Layer (SASL) at the RPC level, that will handle authentication and encryption negotiation for each connection established with the service. Amazon EMR automatically configures HBase with the required configurations when you launch a cluster with a Security Configuration where Kerberos authentication is enabled. The following highlights the main HBase configurations set by the service when launching an EMR cluster with Kerberos enabled (generated using Amazon EMR 6.9.0): Configuration Value hbase.security.authentication kerberos hbase.security.authorization true hbase.master.kerberos.principal hbase/_HOST@ hbase.master.keytab.file /etc/hbase.keytab hbase.regionserver.kerberos.principal hbase/_HOST@ hbase.regionserver.keytab.file /etc/hbase.keytab hbase.thrift.kerberos.principal hbase/_HOST@ hbase.thrift.keytab.file /etc/hbase.keytab hbase.thrift.security.qop auth hbase.rest.authentication.type kerberos hbase.rest.authentication.kerberos.principal HTTP/_HOST@ hbase.rest.authentication.kerberos.keytab /etc/hbase.keytab hbase.rest.kerberos.principal hbase/_HOST@ hbase.rest.keytab.file /etc/hbase.keytab hbase.rest.support.proxyuser true hadoop.proxyuser.hbase.groups * hadoop.proxyuser.hbase.hosts * To launch an EMR cluster with Kerberos Authentication, please refer to Configuring Kerberos on Amazon EMR Authorization \u00b6 Once the users are authenticated through Kerberos, we can now implement our Authorization policies to allow restricted access for specific user to our tables. To enable this functionality, it\u2019s required to enable the Access Controller Coprocessor, by adding additional configurations when launching the EMR cluster. Below an example EMR configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.coprocessor.master.classes\" : \"org.apache.hadoop.hbase.security.access.AccessController\" , \"hbase.coprocessor.region.classes\" : \"org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController\" , \"hbase.security.authorization\" : \"true\" , \"hbase.security.exec.permission.checks\" : \"true\" } } ] In order to grant permissions to specific users in the cluster, you must define the ACL policies using the hbase admin user. For example, the below command add the READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A') permissions to the hadoop user: sudo -s kdestroy kinit hbase/ ` hostname -f ` @YOUR_KERBEROS_REALM -k -t /etc/hbase.keytab hbase shell grant 'hadoop' , 'RWXCA' For additional details, please see the Administration section in official HBase documentation. Networking \u00b6 It\u2019s always a good practice to restrict network access to the cluster to reduce the exposure of the services to external attacks. When using Amazon EMR, you can specify additional Security Groups attached to the cluster to enable network communication with the cluster from pre-defined ranges of IPs or other AWS Security Groups. The tables below provides HBase ports you can control in the EMR Security Groups to allow interactions with trusted parties. For additional information see Control network traffic with security groups in the EMR documentation. HBase Services Port Security Group Description 2181 / TCP Master Zookeeper client port 16000 / TCP Master HMaster 16020 / TCP Core & Task Region Server 8070 / TCP Master REST server 9090 / TCP Master Thrift Server HBase Web UI Port Security Group Description 16010 / TCP Master HMaster Web UI 16030 / TCP Core & Task Region Server Web UI 8085 / TCP Master REST Server UI 9095 / TCP Master Thrift Server UI","title":"Security"},{"location":"applications/hbase/security/#security","text":"The following section describes the main security aspects that can help you to secure an HBase cluster running on Amazon EMR.","title":"Security"},{"location":"applications/hbase/security/#authentication","text":"By default when launching an Amazon EMR cluster with HBase installed, the service will configure HBase without enabling any type of authentication. This allows every client connecting to HBase to read / write tables stored in the cluster without the need to provide any credentials. In this context it is a best practice to limit access to the cluster by scoping access to the cluster using firewalls or EMR Security Groups attached to the cluster. For more details see Networking However, if you require to enable a strong authentication system, you can use Kerberos authentication to secure your cluster. HBase implements the Simple Authentication and Security Layer (SASL) at the RPC level, that will handle authentication and encryption negotiation for each connection established with the service. Amazon EMR automatically configures HBase with the required configurations when you launch a cluster with a Security Configuration where Kerberos authentication is enabled. The following highlights the main HBase configurations set by the service when launching an EMR cluster with Kerberos enabled (generated using Amazon EMR 6.9.0): Configuration Value hbase.security.authentication kerberos hbase.security.authorization true hbase.master.kerberos.principal hbase/_HOST@ hbase.master.keytab.file /etc/hbase.keytab hbase.regionserver.kerberos.principal hbase/_HOST@ hbase.regionserver.keytab.file /etc/hbase.keytab hbase.thrift.kerberos.principal hbase/_HOST@ hbase.thrift.keytab.file /etc/hbase.keytab hbase.thrift.security.qop auth hbase.rest.authentication.type kerberos hbase.rest.authentication.kerberos.principal HTTP/_HOST@ hbase.rest.authentication.kerberos.keytab /etc/hbase.keytab hbase.rest.kerberos.principal hbase/_HOST@ hbase.rest.keytab.file /etc/hbase.keytab hbase.rest.support.proxyuser true hadoop.proxyuser.hbase.groups * hadoop.proxyuser.hbase.hosts * To launch an EMR cluster with Kerberos Authentication, please refer to Configuring Kerberos on Amazon EMR","title":"Authentication"},{"location":"applications/hbase/security/#authorization","text":"Once the users are authenticated through Kerberos, we can now implement our Authorization policies to allow restricted access for specific user to our tables. To enable this functionality, it\u2019s required to enable the Access Controller Coprocessor, by adding additional configurations when launching the EMR cluster. Below an example EMR configuration: [ { \"Classification\" : \"hbase-site\" , \"Properties\" : { \"hbase.coprocessor.master.classes\" : \"org.apache.hadoop.hbase.security.access.AccessController\" , \"hbase.coprocessor.region.classes\" : \"org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController\" , \"hbase.security.authorization\" : \"true\" , \"hbase.security.exec.permission.checks\" : \"true\" } } ] In order to grant permissions to specific users in the cluster, you must define the ACL policies using the hbase admin user. For example, the below command add the READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A') permissions to the hadoop user: sudo -s kdestroy kinit hbase/ ` hostname -f ` @YOUR_KERBEROS_REALM -k -t /etc/hbase.keytab hbase shell grant 'hadoop' , 'RWXCA' For additional details, please see the Administration section in official HBase documentation.","title":"Authorization"},{"location":"applications/hbase/security/#networking","text":"It\u2019s always a good practice to restrict network access to the cluster to reduce the exposure of the services to external attacks. When using Amazon EMR, you can specify additional Security Groups attached to the cluster to enable network communication with the cluster from pre-defined ranges of IPs or other AWS Security Groups. The tables below provides HBase ports you can control in the EMR Security Groups to allow interactions with trusted parties. For additional information see Control network traffic with security groups in the EMR documentation. HBase Services Port Security Group Description 2181 / TCP Master Zookeeper client port 16000 / TCP Master HMaster 16020 / TCP Core & Task Region Server 8070 / TCP Master REST server 9090 / TCP Master Thrift Server HBase Web UI Port Security Group Description 16010 / TCP Master HMaster Web UI 16030 / TCP Core & Task Region Server Web UI 8085 / TCP Master REST Server UI 9095 / TCP Master Thrift Server UI","title":"Networking"},{"location":"applications/hive/best_practices/","text":"5.2 - Hive \u00b6 BP 5.2.1 - Upgrading Hive Metastore \u00b6 When upgrading from EMR 5.x (hive 2.x) to EMR 6.x (hive 3.x), hive metastore requires a schema upgrade to support changes and new features in Hive 3.x The following steps assumes hive metastore is running on Amazon RDS. Upgrading hive metastore is backwards compatible. Once hive metastore is upgraded, both hive 2x and hive 3.x clients/clusters can use the same hive metastore. 1. Take a snapshot of current Hive Metastore on Amazon RDS 2. Provision a new Amazon RDS with the snapshot that was created in step 1 3. Provision target EMR 6.x version without configuring an external hive metastore 4. SSH into EMR 6 cluster and update the below in hive-site.xml to point to new RDS from the previous step \"javax.jdo.option.ConnectionURL\": \"jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true\", \"javax.jdo.option.ConnectionDriverName\": \"org.mariadb.jdbc.Driver\", \"javax.jdo.option.ConnectionUserName\": \"username\", \"javax.jdo.option.ConnectionPassword\": \"password\" 5. Run the following to check current hms schema version: hive --service schemaTool -dbType mysql -info You should see the below. Make note of the current metastore schema version (2.3.0 in this case) Hive distribution version: 3.1.0 Metastore schema version: 2.3.0 org.apache.hadoop.hive.metastore.HiveMetaException: Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0 Use --verbose for detailed stacktrace. *** schemaTool failed \\*** 6. Change directory to the hive metastore upgrade scripts location cd /usr/lib/hive/scripts/metastore/upgrade/mysql Doing these steps on EMR 6.x is required because you need need the target hive distribution version (hive 3.1) and the upgrade scripts inorder to ugprade the schema. 7. Connect to mysql using below command and upgrade the schema as per the hive version. For example, if you are upgrading from 2.3.0 to 3.1.0, you would need to source the 2 scripts. Scripts can also be found in this location: https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql mysql - u < HIVEUSER > - h < ENDPOINT - RDS > - p 'PASSWORD' mysql > use hive ; mysql > source upgrade - 2.3 . 0 - to - 3.0 . 0 . mysql . sql ; mysql > source upgrade - 3.0 . 0 - to - 3.1 . 0 . mysql . sql ; 8. Verify the upgrade was succesful / usr / lib / hive / bin / schematool - dbType mysql - info Metastore connection URL : jdbc : mysql : // hostname : 3306 / hive ? createDatabaseIfNotExist = true Metastore Connection Driver : org . mariadb . jdbc . Driver Metastore connection User : admin Hive distribution version : 3.1 . 0 Metastore schema version : 3.1 . 0 schemaTool completed 9. After all commands are run, terminate the cluster. 10. Further validation: Provision new 5.x and 6.x cluster with updated hive-site.xml that points to new RDS. In both version, you can run hive --service schemaTool -dbType mysql -info In 5.x you'll see Hive distribution version: 2.3.0 Metastore schema version: 3.1.0 and in 6.x, you'll see Hive distribution version: 3.1.0 Metastore schema version: 3.1.0","title":"Best Practices"},{"location":"applications/hive/best_practices/#52-hive","text":"","title":"5.2 - Hive"},{"location":"applications/hive/best_practices/#bp-521-upgrading-hive-metastore","text":"When upgrading from EMR 5.x (hive 2.x) to EMR 6.x (hive 3.x), hive metastore requires a schema upgrade to support changes and new features in Hive 3.x The following steps assumes hive metastore is running on Amazon RDS. Upgrading hive metastore is backwards compatible. Once hive metastore is upgraded, both hive 2x and hive 3.x clients/clusters can use the same hive metastore. 1. Take a snapshot of current Hive Metastore on Amazon RDS 2. Provision a new Amazon RDS with the snapshot that was created in step 1 3. Provision target EMR 6.x version without configuring an external hive metastore 4. SSH into EMR 6 cluster and update the below in hive-site.xml to point to new RDS from the previous step \"javax.jdo.option.ConnectionURL\": \"jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true\", \"javax.jdo.option.ConnectionDriverName\": \"org.mariadb.jdbc.Driver\", \"javax.jdo.option.ConnectionUserName\": \"username\", \"javax.jdo.option.ConnectionPassword\": \"password\" 5. Run the following to check current hms schema version: hive --service schemaTool -dbType mysql -info You should see the below. Make note of the current metastore schema version (2.3.0 in this case) Hive distribution version: 3.1.0 Metastore schema version: 2.3.0 org.apache.hadoop.hive.metastore.HiveMetaException: Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0 Use --verbose for detailed stacktrace. *** schemaTool failed \\*** 6. Change directory to the hive metastore upgrade scripts location cd /usr/lib/hive/scripts/metastore/upgrade/mysql Doing these steps on EMR 6.x is required because you need need the target hive distribution version (hive 3.1) and the upgrade scripts inorder to ugprade the schema. 7. Connect to mysql using below command and upgrade the schema as per the hive version. For example, if you are upgrading from 2.3.0 to 3.1.0, you would need to source the 2 scripts. Scripts can also be found in this location: https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql mysql - u < HIVEUSER > - h < ENDPOINT - RDS > - p 'PASSWORD' mysql > use hive ; mysql > source upgrade - 2.3 . 0 - to - 3.0 . 0 . mysql . sql ; mysql > source upgrade - 3.0 . 0 - to - 3.1 . 0 . mysql . sql ; 8. Verify the upgrade was succesful / usr / lib / hive / bin / schematool - dbType mysql - info Metastore connection URL : jdbc : mysql : // hostname : 3306 / hive ? createDatabaseIfNotExist = true Metastore Connection Driver : org . mariadb . jdbc . Driver Metastore connection User : admin Hive distribution version : 3.1 . 0 Metastore schema version : 3.1 . 0 schemaTool completed 9. After all commands are run, terminate the cluster. 10. Further validation: Provision new 5.x and 6.x cluster with updated hive-site.xml that points to new RDS. In both version, you can run hive --service schemaTool -dbType mysql -info In 5.x you'll see Hive distribution version: 2.3.0 Metastore schema version: 3.1.0 and in 6.x, you'll see Hive distribution version: 3.1.0 Metastore schema version: 3.1.0","title":"BP 5.2.1  -  Upgrading Hive Metastore"},{"location":"applications/hive/introduction/","text":"Introduction \u00b6 This section offers best practices and tuning guidance for running Apache Hive workloads on Amazon EMR.","title":"Introduction"},{"location":"applications/hive/introduction/#introduction","text":"This section offers best practices and tuning guidance for running Apache Hive workloads on Amazon EMR.","title":"Introduction"},{"location":"applications/spark/best_practices/","text":"5.1 - Spark \u00b6 BP 5.1.1 - Use the most recent version of EMR \u00b6 Amazon EMR provides several Spark optimizations out of the box with EMR Spark runtime which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to improve the performance of this Spark runtime engine for new releases. Several optimizations such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference). As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits. Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of EMR Serverless (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code. aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- name spark - 6.5.0 - demo - application Detailed documentation for running Spark jobs using EMR Serverless can be found here . Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available. BP 5.1.2 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements. Memory-optimized \u00b6 Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized \u00b6 CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose \u00b6 General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU:memory ratio based on your workload needs. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute c5.18xlarge $3.06 $0.27 72 144 2 Memory r5.12xlarge $3.02 $0.27 48 384 8 General m5.16xlarge $3.07 $0.27 64 256 4 Storage-optimized \u00b6 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors. GPU instances \u00b6 GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Graviton instances \u00b6 Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance. BP 5.1.3 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application's Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications. * You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application. * You want to write output to the console i.e., send the results back to the client program where you submitted your application. * Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes. * There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors. * You are saving results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property spark.yarn.maxAppAttempts . You can increase this value further if needed. * You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property spark.yarn.submit.waitAppCompletion is set to \"false\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.1.4 - Use right file formats and compression type \u00b6 Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support. When writing Parquet files to S3, EMR Spark will use EMRFSOutputCommitter which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc. Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults. You can also apply columnar encryption on Parquet files using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.1.5 - Partitioning \u00b6 Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans. You can choose one or more partition fields from your dataset or table columns based on :- Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field. Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions). Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields. File sizes per partition. It is recommended that your individual file sizes within each partition are >=128 MB. The number of shuffle partitions will determine the number of output files per table partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property spark.sql.shuffle.partitions . You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property spark.sql.files.maxPartitionBytes . Partitioning ensures that dynamic partition pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark. For example, following query will push partition filters for better performance. l_shipdate and l_shipmode are partition fields of the table \"testdb.lineitem_shipmodesuppkey_part\". val df = spark.sql(\"select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\") df.queryExecution.toString Printing the query execution plan where we can see pushed filters for the two partition fields in where clause: == Physical Plan == AdaptiveSparkPlan isFinalPlan = true +- == Final Plan == * ( 2 ) HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- ShuffleQueryStage 0 +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 198 ] +- * ( 1 ) HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- * ( 1 ) Project +- * ( 1 ) ColumnarToRow +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> +- == Initial Plan == HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 179 ] +- HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- Project +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> BP 5.1.6 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning. For a starting point, generally, it is advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory , you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory ). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb ). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + ( spark.executor.memory * spark.yarn.executor.memoryOverheadFactor ) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor =0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to \"true\" will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property spark.yarn.heterogeneousExecutors.enabled and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties spark.executor.maxMemory and spark.executor.maxCores . Minimum resources are calculated with spark.executor.cores and spark.executor.memory . For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting spark.yarn.heterogeneousExecutors.enabled to \"false\" and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn't matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions: Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune spark.driver.maxResultSize . You can use smaller driver memory (or use the default spark.driver.memory ) if you are running multiple jobs in parallel. Now, coming to spark.sql.shuffle.partitions for Dataframes and Datasets and spark.default.parallelism for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce spark.sql.shuffle.partitions to increase partition size during the exchange. Apart from this, if you want to use tools to receive tuning suggestions, consider using Sparklens and Dr. Elephant with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application. BP 5.1.7 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below: val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also optionally fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet). spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m. spark.kryoserializer.buffer - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.1.8 - Tune Garbage Collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can monitor GC performance using Spark UI. The GC time should be ideally <= 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance. BP 5.1.9 - Use optimal APIs wherever possible \u00b6 When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes. BP 5.1.10 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads. If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks. BP 5.1.11 - For workloads with predictable pattern, consider disabling dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature for following cases: Workloads processing variable amount of data When your cluster uses autoscaling Dynamic processing requirements or unpredictable workload patterns Streaming and ad-hoc workloads When your cluster runs multiple concurrent applications Your cluster is long-running The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where: Workloads have a very predicatable pattern Amount of data processed is predictable and consistent throughout the application Cluster runs Spark application in batch mode Clusters are transient and are of fixed size (no autoscaling) Application processing is relatively uniform. Workload is not spikey in nature. For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs. For such use cases, you can consider disabling dynamic allocation along with setting the precise number and size of executors and cores like below. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"8G\", \"spark.executor.cores\": \"4\" }, \"configurations\": [] }] Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well. However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources. BP 5.1.12 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes. For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors. Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations. Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput. BP 5.1.13 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, spark.speculation is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp Using HDFS as storage (not recommended) Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data. You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation =\"true\"). [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.speculation\": \"true\" }, \"configurations\": [] }] Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter. BP 5.1.14 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running. In order to check data integrity, consider using Deequ for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on \"Status\" and check for error message of failed tasks. If you are seeing too many random unexpected errors such as \"ArrayIndexOutOfBounds\" or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job. BP 5.1.15 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes tries to skip. Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort. BP 5.1.16 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level. Salting \u00b6 Salting is one of the most common skew mitigation techniques where you add a \"salt\" to the skewed column say \"col1\". You can split it into multiple columns like \"col1_0\",\"col1_1\",\"col1_2\" and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved. Original data Salted 4 times Salted 8 times A typical Salting workflow looks like below: For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms). n = 100 salted_df = df.withColumn(\"salt\", (rand * n).cast(IntegerType)) Now, aggregation is performed on this salt column and the results are reduced by keys unsalted_df = salted_df . groupBy ( \"salt\" , groupByFields ). agg ( aggregateFields ). groupBy ( groupByFields ). agg ( aggregateFields ) Similar logic can be applied for windowing functions as well. A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance. Isolated Salting \u00b6 In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results. Isolated Salting workflow looks like below: Example code looks like below: val count = 4 val salted = df . withColumn ( \" salt \" , when ( ' col === \"A\", rand(1) * count cast IntegerType) otherwise 0) val replicaDF = skewDF . withColumn ( \" replica \" , when ( ' col === \"A\", (0 until count) toArray) otherwise Array(0)) . withColumn ( \" salt \" , explode ( ' replica ' )) . drop ( ' replica ' ) val merged = salted . join ( replicaDF , joinColumns : + \" salt \" ) Isolated broadcast join \u00b6 In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the \u201dnormal\u201d keys and perform map-side join on isolated \u201dskewed\u201d keys. Finally, we can merge the results of skewed and normal joins Isolated map-side join workflow looks like below: Example code looks like below: val count = 8 val salted = skewDF.withColumn(\"salt\", when('col === \"A\", rand(1) * count cast IntegerType) otherwise 0).repartition('col', 'salt') // Re-partition to remove skew val broadcastDF = salted.join(broadcast(sourceDF), \"symbol\") Hashing for SparkSQL queries \u00b6 While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes. Following could be an example query working on top of a skewed dataset. select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes. select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 ) limit 10; If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written. BP 5.1.17 - Choose the right type of join \u00b6 There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join \"hints\" as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively. SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; Broadcast Join \u00b6 Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (=) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is <=10 MB, this join strategy is chosen. This is based on the parameter spark.sql.autoBroadcastJoinThreshold which is defaulted to 10 MB. If one of your join tables are larger than 10 MB, you can either modify spark.sql.autoBroadcastJoinThreshold or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI. Please note that you should not use this join if your \"small\" table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is <1 GB. Sort Merge Join \u00b6 This is the most common join used by Spark. If you are joining two large tables (>10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI. Spark configuration spark.sql.join.preferSortMergeJoin is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets. Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is < 1 GB. All join types are supported. Shuffle Hash Join \u00b6 Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless spark.sql.join.preferSortMergeJoin is set to \"false\" or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join. Broadcast Nested Loop Join \u00b6 Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions (<,>,<=,>=,like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice. Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable. For example, below code leads to Broadcast Nested Loop Join. val df1 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\" ) val df2 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-04' and l_shipmode='SHIP'\" ) val nestedLoopDF = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" ) || df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) Instead, you can change the code like below: val result1 = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" )) val result2 = df1 . join ( df2 , df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) val resultDF = result1 . union ( result2 ) The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met spark.sql.autoBroadcastJoinThreshold . Cartesian Join \u00b6 Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running \"inner like\" queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks. for l_key in lhs_table : for r_key in rhs_table : # Execute join condition If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance. Following code will lead to a Cartesian product provided the tables do not meet spark.sql.autoBroadcastJoinThreshold . val crossJoinDF = df1.join(df2, df1(\"l_partkey\") >= df2(\"l_partkey\")) Now, passing a broadcast hint which leads to Broadcast Nested Loop Join val crossJoinDF = df1.join(broadcast(df2), df1(\"l_partkey\") >= df2(\"l_partkey\")) BP 5.1.18 - Consider Spark Blacklisting for large clusters \u00b6 Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14. This blacklisting is enabled by default in Amazon EMR with the spark.blacklist.decommissioning.enabled property set to true. You can control the time for which the node is blacklisted using spark.blacklist.decommissioning.timeout property , which is set to 1 hour by default, equal to the default value for yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs . It is recommended to set spark.blacklist.decommissioning.timeout to a value equal to or greater than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs to make sure that Amazon EMR blacklists the node for the entire decommissioning period. Following are some experimental blacklisting properties. spark.blacklist.task.maxTaskAttemptsPerExecutor determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2. spark.blacklist.task.maxTaskAttemptsPerNode determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2. spark.blacklist.stage.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire stage. spark.blacklist.stage.maxFailedExecutorsPerNode determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2. spark.blacklist.application.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire application. spark.blacklist.application.maxFailedExecutorsPerNode is same as spark.blacklist.stage.maxFailedExecutorsPerNode but the worker node is blacklisted for the entire application. spark.blacklist.killBlacklistedExecutors when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring. spark.blacklist.application.fetchFailure.enabled when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring. The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.blacklist.killBlacklistedExecutors\": \"true\", \"spark.blacklist.application.fetchFailure.enabled\": \"true\" }, \"configurations\": [] }] You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs. When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by spark.stage.maxConsecutiveAttempts . This is determined by the setting spark.stage.attempt.ignoreOnDecommissionFetchFailure being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions. BP 5.1.19 - Debugging and monitoring Spark applications \u00b6 EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR. Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates. Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated. EMR Persistent UI - Amazon EMR provides Persistent User Interface for Spark . This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active. Spark UI options are also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on \"stderr\" or \"stdout\" from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs. Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step's stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live. If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property yarn.log-aggregation.retain-seconds . BP 5.1.20 - Spark Observability Platforms \u00b6 Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed here . You can choose an observability platform based on your requirements. Following are some example native options. Amazon Managed Services for Prometheus and Grafana \u00b6 AWS offers Amazon Managed Prometheus (AMP) which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. Amazon Managed Grafana (AMG) is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the deployment instructions available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using PrometheusServlet and prometheus/jmx_exporter . However, some bootstrapping is necessary for this integration. Amazon Opensearch \u00b6 Amazon Opensearch is a community-driven open source fork of Elasticsearch and Kibana . It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs. Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic. BP 5.1.21 - Potential resolutions for not-so-common errors \u00b6 Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions. Potential strategies to mitigate S3 throttling errors \u00b6 For mitigating S3 throttling errors (503: Slow Down), consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg format ObjectStoreLocationProvider to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job's processing requirements increase and thus help mitigate the S3 throttling errors. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProvider here . If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ /. Precautions to take while running too many executors \u00b6 If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs. ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970 For this issue, you can increase spark.scheduler.listenerbus.eventqueue.size from default of 10000 to 2x or more until you do not see dropped events anymore. Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing spark.executor.heartbeatInterval from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources. If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing spark.yarn.containerLauncherMaxThreads which is defaulted to 25. You may also want to increase spark.yarn.am.memory (default: 512 MB) and spark.yarn.am.cores (default: 1). Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows \u00b6 You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster. In hadoop-env.sh, you can see heap sizes for HDFS daemons. export HADOOP_OPTS = \"$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError\" export HADOOP_NAMENODE_HEAPSIZE = 25190 export HADOOP_DATANODE_HEAPSIZE = 4096 In yarn-env.sh, you can see heap sizes for YARN daemons. export YARN_NODEMANAGER_HEAPSIZE = 2048 export YARN_RESOURCEMANAGER_HEAPSIZE = 7086 Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like \"MissingBlocksException\" in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below: OpenJDK 64-Bit Server VM warning : INFO : os :: commit_memory ( 0x00007f0beb662000 , 12288 , 0 ) failed ; error = 'Cannot allocate memory' ( errno = 12 ) # # There is insufficient memory for the Java Runtime Environment to continue . # Native memory allocation ( mmap ) failed to map 12288 bytes for committing reserved memory . # An error report file with more information is saved as : # / tmp / hs_err_pid14730 . log In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well: [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_OPTS\" : \"-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS\" \u201c HADOOP_DATANODE_HEAPSIZE \u201d : \"4096\" }, \"Configurations\" : [] } ] } ] Additionally, you can also consider reducing yarn.nodemanager.resource.memory-mb by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from yarn.nodemanager.resource.memory-mb for your instance types. Precautions to take for highly concurrent workloads \u00b6 When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like \u201cCaused by: java.util.zip.ZipException: error in opening zip file\u201d or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files. 22 / 02 / 25 21 : 39 : 45 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 39 : 45 WARN Client : Neither spark . yarn . jars nor spark . yarn . archive is set , falling back to uploading libraries under SPARK_HOME . 22 / 02 / 25 21 : 39 : 48 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_libs__3016570917637060246 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_libs__3016570917637060246 . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / hive - site . xml 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / pyspark . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 39 : 50 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_conf__7549408525505552236 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_conf__ . zip To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set spark.yarn.archive to that location. Below is an example: zip -r spark-dependencies.zip /mnt/jars/ hdfs dfs -mkdir /user/hadoop/deps/ hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/ /mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults. spark.yarn.archive hdfs:///user/hadoop/deps/spark-dependencies.zip You can see that this file size is large. hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip -rw-r--r-- 1 hadoop hdfsadmingroup 287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node's CPU to ensure that the utilization has been brought down. 22 / 02 / 25 21 : 56 : 08 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 56 : 08 INFO Client : Source and destination file systems are the same . Not copying hdfs : / user / hadoop / deps / spark - dependencies . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / hive - site . xml 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / pyspark . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / mnt / tmp / spark - 0 fbfb5a9 - 7 c0c - 4 f9f - befd - 3 c8f56bc4688 / __spark_conf__5472705335503774914 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / __spark_conf__ . zip If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop\u2019s ShutdownHook . If this happens, increase hadoop.service.shutdown.timeout property from 30s to to a larger value. Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR. BP 5.1.22 - How the number of partitions are determined when reading a raw file \u00b6 When reading a raw file, that can be a text file, csv, etc. the count behind the number of partitions created from Spark depends from many variables as the methods used to read the file, the default parallelism and so on. Following an overview of how these factors are related between each other so to better understand how files are processed. Here a brief summary of relationship between core nodes - executors - tasks: each File is composed by blocks that will be parsed according to the InputFormat corresponding to the specific data format, and generally combines several blocks into one input slice, called InputSplit InputSplit and Task are one-to-one correspondence relationship each of these specific Tasks will be assigned to one executor of the nodes on the cluster each node can have one or more Executors, depending on the node resources and executor settings each Executor consists of cores and memory whose default is based on the node type. Each executor can only execute one task at time. So based on that, the number of threads/tasks will be based on the number of partitions while reading. Please note that the S3 connector takes some configuration option (e.g. s3a: fs.s3a.block.size) to simulate blocks in Hadoop services, but the concept of blocks in S3 does not really exists. Unlike HDFS that is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior, EMRFS is an object store, not a file system. For more information, see Hadoop documentation for Object Stores vs. Filesystems . Now, there are several factors that dictate how a dataset or file is mapped to a partition. First is the method used to read the file (e.g. text file), that changes if you're working with rdds or dataframes: sc . textFile (...) returns a RDD [ String ] textFile ( String path , int minPartitions ) Read a text file from HDFS , a local file system ( available on all nodes ), or any Hadoop - supported file system URI , and return it as an RDD of Strings . spark . read . text (...) returns a DataSet [ Row ] or a DataFrame text ( String path ) Loads text files and returns a DataFrame whose schema starts with a string column named \"value\" , and followed by partitioned columns if there are any . Spark Core API (RDDs) \u00b6 When using sc.textFile Spark uses the block size set for the filesysytem protocol it's reading from, to calculate the number of partitions in input: SparkContext.scala /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file */ def textFile ( path : String , minPartitions : Int = defaultMinPartitions ) : RDD [ String ] = withScope { assertNotStopped () hadoopFile ( path , classOf [ TextInputFormat ] , classOf [ LongWritable ] , classOf [ Text ] , minPartitions ). map ( pair => pair . _2 . toString ). setName ( path ) } FileInputFormat.java if ( isSplitable ( fs , path )) { long blockSize = file . getBlockSize () ; long splitSize = computeSplitSize ( goalSize , minSize , blockSize ) ; When using the S3A protocol the block size is set through the fs.s3a.block.size parameter (default 32M), and when using S3 protocol through fs.s3n.block.size (default 64M). Important to notice here is that with S3 protocol the parameter used is fs.s3n.block.size and not fs.s3.block.size as you would expect. In EMR indeed, when using EMRFS, which means using s3 with s3:// prefix, fs.s3.block.size will not have any affect on the EMRFS configration. Following some testing results using these parameters: CONF Input: 1 file, total size 336 MB TEST 1 (default) S3A protocol - fs.s3a.block.size = 32M (default) - Spark no. partitions: 336/32 = 11 S3 protocol - fs.s3n.block.size = 64M (default) - Spark no. partitions: 336/64 = 6 TEST 2 (modified) S3A protocol - fs.s3a.block.size = 64M (modified) - Spark no. partitions: 336/64 = 6 S3protocol - fs.s3n.block.size = 128M (modified) - Spark no. partitions: 336/128 = 3 Spark SQL (DATAFRAMEs) \u00b6 When using spark.read.text no. of spark tasks/partitions depends on default parallelism: DataSourceScanExec.scala val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism val totalBytes = selectedPartitions.flatMap(_.files.map (_.getLen + openCostInBytes)).sum val bytesPerCore = totalBytes / defaultParallelism val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) The default Parallelism is determined via: CoarseGrainedSchedulerBackend.scala override def defaultParallelism(): Int = { conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2)) } If defaultParallelism is too large, bytesPerCore will be small, and maxSplitBytes can be small, which can result in more no. of spark tasks/partitions. So if there're more cores, spark.default.parallelism can be large, defaultMaxSplitBytes can be small, and no. of spark tasks/partitions can be large. In order to tweak the input no. of partitions the following parameters need to be set: Classification Property Description spark-default spark.default.parallelism default: max(total number of vCores, 2) spark-default spark.sql.files.maxPartitionBytes default: 128MB If these parameters are modified, maximizeResourceAllocation need to be disabled, as it would override spark.default.parallelism parameter . Following some testing results using these parameters: CONF - Total number of vCores = 16 -> spark . default . parallelism = 16 - spark . sql . files . maxPartitionBytes = 128 MB TEST 1 - Input : 1 CSV file , total size 352 , 3 MB - Spark no . partitions : 16 - Partition size = 352 , 3 / 16 = \u223c 22 , 09 MB TEST 2 - Input : 10 CSV files , total size 3523 MB - Spark no . partitions : 30 - Partition size = 3523 / 30 = \u223c 117 , 43 MB Disclaimer When writing a file the number of partitions in output will depends from the number of partitions in input that will be maintained if no shuffle operations are applied on the data processed, changed otherwise based on spark.default.parallelism for RDDs and spark.sql.shuffle.partitions for dataframes.","title":"Best Practices"},{"location":"applications/spark/best_practices/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices/#bp-511-use-the-most-recent-version-of-emr","text":"Amazon EMR provides several Spark optimizations out of the box with EMR Spark runtime which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to improve the performance of this Spark runtime engine for new releases. Several optimizations such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference). As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits. Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of EMR Serverless (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code. aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- name spark - 6.5.0 - demo - application Detailed documentation for running Spark jobs using EMR Serverless can be found here . Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available.","title":"BP 5.1.1  -  Use the most recent version of EMR"},{"location":"applications/spark/best_practices/#bp-512-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements.","title":"BP 5.1.2  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices/#memory-optimized","text":"Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively.","title":"Memory-optimized"},{"location":"applications/spark/best_practices/#cpu-optimized","text":"CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia.","title":"CPU-optimized"},{"location":"applications/spark/best_practices/#general-purpose","text":"General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU:memory ratio based on your workload needs. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute c5.18xlarge $3.06 $0.27 72 144 2 Memory r5.12xlarge $3.02 $0.27 48 384 8 General m5.16xlarge $3.07 $0.27 64 256 4","title":"General purpose"},{"location":"applications/spark/best_practices/#storage-optimized","text":"Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors.","title":"Storage-optimized"},{"location":"applications/spark/best_practices/#gpu-instances","text":"GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines.","title":"GPU instances"},{"location":"applications/spark/best_practices/#graviton-instances","text":"Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance.","title":"Graviton instances"},{"location":"applications/spark/best_practices/#bp-513-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application's Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.1.3  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications. * You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application. * You want to write output to the console i.e., send the results back to the client program where you submitted your application. * Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode.","title":"Client deploy mode"},{"location":"applications/spark/best_practices/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes. * There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors. * You are saving results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property spark.yarn.maxAppAttempts . You can increase this value further if needed. * You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property spark.yarn.submit.waitAppCompletion is set to \"false\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices/#bp-514-use-right-file-formats-and-compression-type","text":"Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support. When writing Parquet files to S3, EMR Spark will use EMRFSOutputCommitter which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc. Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults. You can also apply columnar encryption on Parquet files using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.1.4  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices/#bp-515-partitioning","text":"Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans. You can choose one or more partition fields from your dataset or table columns based on :- Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field. Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions). Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields. File sizes per partition. It is recommended that your individual file sizes within each partition are >=128 MB. The number of shuffle partitions will determine the number of output files per table partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property spark.sql.shuffle.partitions . You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property spark.sql.files.maxPartitionBytes . Partitioning ensures that dynamic partition pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark. For example, following query will push partition filters for better performance. l_shipdate and l_shipmode are partition fields of the table \"testdb.lineitem_shipmodesuppkey_part\". val df = spark.sql(\"select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\") df.queryExecution.toString Printing the query execution plan where we can see pushed filters for the two partition fields in where clause: == Physical Plan == AdaptiveSparkPlan isFinalPlan = true +- == Final Plan == * ( 2 ) HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- ShuffleQueryStage 0 +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 198 ] +- * ( 1 ) HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- * ( 1 ) Project +- * ( 1 ) ColumnarToRow +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<> +- == Initial Plan == HashAggregate ( keys = [], functions = [ count ( 1 )], output = [ count ( 1 )# 320 ]) +- Exchange SinglePartition , ENSURE_REQUIREMENTS , [ id = # 179 ] +- HashAggregate ( keys = [], functions = [ partial_count ( 1 )], output = [ count # 318 L ]) +- Project +- FileScan orc testdb . lineitem_shipmodesuppkey_part [ l_shipdate # 313 , l_shipmode # 314 ] Batched: true , DataFilters: [], Format: ORC , Location: InMemoryFileIndex [ s3: //vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct<>","title":"BP 5.1.5  -  Partitioning"},{"location":"applications/spark/best_practices/#bp-516-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning. For a starting point, generally, it is advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory , you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory ). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb ). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + ( spark.executor.memory * spark.yarn.executor.memoryOverheadFactor ) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor =0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to \"true\" will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property spark.yarn.heterogeneousExecutors.enabled and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties spark.executor.maxMemory and spark.executor.maxCores . Minimum resources are calculated with spark.executor.cores and spark.executor.memory . For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting spark.yarn.heterogeneousExecutors.enabled to \"false\" and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn't matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions: Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune spark.driver.maxResultSize . You can use smaller driver memory (or use the default spark.driver.memory ) if you are running multiple jobs in parallel. Now, coming to spark.sql.shuffle.partitions for Dataframes and Datasets and spark.default.parallelism for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce spark.sql.shuffle.partitions to increase partition size during the exchange. Apart from this, if you want to use tools to receive tuning suggestions, consider using Sparklens and Dr. Elephant with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application.","title":"BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices/#bp-517-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below: val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also optionally fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet). spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m. spark.kryoserializer.buffer - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices/#bp-518-tune-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can monitor GC performance using Spark UI. The GC time should be ideally <= 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance.","title":"BP 5.1.8  -   Tune Garbage Collector"},{"location":"applications/spark/best_practices/#bp-519-use-optimal-apis-wherever-possible","text":"When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples.","title":"BP 5.1.9  -   Use optimal APIs wherever possible"},{"location":"applications/spark/best_practices/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices/#bp-5110-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads. If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks.","title":"BP 5.1.10 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices/#bp-5111-for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature for following cases: Workloads processing variable amount of data When your cluster uses autoscaling Dynamic processing requirements or unpredictable workload patterns Streaming and ad-hoc workloads When your cluster runs multiple concurrent applications Your cluster is long-running The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where: Workloads have a very predicatable pattern Amount of data processed is predictable and consistent throughout the application Cluster runs Spark application in batch mode Clusters are transient and are of fixed size (no autoscaling) Application processing is relatively uniform. Workload is not spikey in nature. For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs. For such use cases, you can consider disabling dynamic allocation along with setting the precise number and size of executors and cores like below. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"8G\", \"spark.executor.cores\": \"4\" }, \"configurations\": [] }] Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well. However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.","title":"BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation"},{"location":"applications/spark/best_practices/#bp-5112-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes. For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors. Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations. Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.","title":"BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices/#bp-5113-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, spark.speculation is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp Using HDFS as storage (not recommended) Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data. You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation =\"true\"). [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.speculation\": \"true\" }, \"configurations\": [] }] Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.","title":"BP 5.1.13  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices/#bp-5114-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running. In order to check data integrity, consider using Deequ for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on \"Status\" and check for error message of failed tasks. If you are seeing too many random unexpected errors such as \"ArrayIndexOutOfBounds\" or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job.","title":"BP 5.1.14 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices/#bp-5115-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes tries to skip. Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort.","title":"BP 5.1.15 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices/#bp-5116-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level.","title":"BP 5.1.16  -   Data Skew"},{"location":"applications/spark/best_practices/#salting","text":"Salting is one of the most common skew mitigation techniques where you add a \"salt\" to the skewed column say \"col1\". You can split it into multiple columns like \"col1_0\",\"col1_1\",\"col1_2\" and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved. Original data Salted 4 times Salted 8 times A typical Salting workflow looks like below: For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms). n = 100 salted_df = df.withColumn(\"salt\", (rand * n).cast(IntegerType)) Now, aggregation is performed on this salt column and the results are reduced by keys unsalted_df = salted_df . groupBy ( \"salt\" , groupByFields ). agg ( aggregateFields ). groupBy ( groupByFields ). agg ( aggregateFields ) Similar logic can be applied for windowing functions as well. A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance.","title":"Salting"},{"location":"applications/spark/best_practices/#isolated-salting","text":"In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results. Isolated Salting workflow looks like below: Example code looks like below: val count = 4 val salted = df . withColumn ( \" salt \" , when ( ' col === \"A\", rand(1) * count cast IntegerType) otherwise 0) val replicaDF = skewDF . withColumn ( \" replica \" , when ( ' col === \"A\", (0 until count) toArray) otherwise Array(0)) . withColumn ( \" salt \" , explode ( ' replica ' )) . drop ( ' replica ' ) val merged = salted . join ( replicaDF , joinColumns : + \" salt \" )","title":"Isolated Salting"},{"location":"applications/spark/best_practices/#isolated-broadcast-join","text":"In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the \u201dnormal\u201d keys and perform map-side join on isolated \u201dskewed\u201d keys. Finally, we can merge the results of skewed and normal joins Isolated map-side join workflow looks like below: Example code looks like below: val count = 8 val salted = skewDF.withColumn(\"salt\", when('col === \"A\", rand(1) * count cast IntegerType) otherwise 0).repartition('col', 'salt') // Re-partition to remove skew val broadcastDF = salted.join(broadcast(sourceDF), \"symbol\")","title":"Isolated broadcast join"},{"location":"applications/spark/best_practices/#hashing-for-sparksql-queries","text":"While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes. Following could be an example query working on top of a skewed dataset. select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes. select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 ) limit 10; If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written.","title":"Hashing for SparkSQL queries"},{"location":"applications/spark/best_practices/#bp-5117-choose-the-right-type-of-join","text":"There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join \"hints\" as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively. SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;","title":"BP 5.1.17  -  Choose the right type of join"},{"location":"applications/spark/best_practices/#broadcast-join","text":"Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (=) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is <=10 MB, this join strategy is chosen. This is based on the parameter spark.sql.autoBroadcastJoinThreshold which is defaulted to 10 MB. If one of your join tables are larger than 10 MB, you can either modify spark.sql.autoBroadcastJoinThreshold or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI. Please note that you should not use this join if your \"small\" table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is <1 GB.","title":"Broadcast Join"},{"location":"applications/spark/best_practices/#sort-merge-join","text":"This is the most common join used by Spark. If you are joining two large tables (>10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI. Spark configuration spark.sql.join.preferSortMergeJoin is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets. Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is < 1 GB. All join types are supported.","title":"Sort Merge Join"},{"location":"applications/spark/best_practices/#shuffle-hash-join","text":"Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless spark.sql.join.preferSortMergeJoin is set to \"false\" or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join.","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices/#broadcast-nested-loop-join","text":"Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions (<,>,<=,>=,like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice. Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable. For example, below code leads to Broadcast Nested Loop Join. val df1 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'\" ) val df2 = spark . sql ( \"select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-04' and l_shipmode='SHIP'\" ) val nestedLoopDF = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" ) || df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) Instead, you can change the code like below: val result1 = df1 . join ( df2 , df1 ( \"l_partkey\" ) === df2 ( \"l_partkey\" )) val result2 = df1 . join ( df2 , df1 ( \"l_linenumber\" ) === df2 ( \"l_linenumber\" )) val resultDF = result1 . union ( result2 ) The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met spark.sql.autoBroadcastJoinThreshold .","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices/#cartesian-join","text":"Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running \"inner like\" queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks. for l_key in lhs_table : for r_key in rhs_table : # Execute join condition If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance. Following code will lead to a Cartesian product provided the tables do not meet spark.sql.autoBroadcastJoinThreshold . val crossJoinDF = df1.join(df2, df1(\"l_partkey\") >= df2(\"l_partkey\")) Now, passing a broadcast hint which leads to Broadcast Nested Loop Join val crossJoinDF = df1.join(broadcast(df2), df1(\"l_partkey\") >= df2(\"l_partkey\"))","title":"Cartesian Join"},{"location":"applications/spark/best_practices/#bp-5118-consider-spark-blacklisting-for-large-clusters","text":"Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14. This blacklisting is enabled by default in Amazon EMR with the spark.blacklist.decommissioning.enabled property set to true. You can control the time for which the node is blacklisted using spark.blacklist.decommissioning.timeout property , which is set to 1 hour by default, equal to the default value for yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs . It is recommended to set spark.blacklist.decommissioning.timeout to a value equal to or greater than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs to make sure that Amazon EMR blacklists the node for the entire decommissioning period. Following are some experimental blacklisting properties. spark.blacklist.task.maxTaskAttemptsPerExecutor determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2. spark.blacklist.task.maxTaskAttemptsPerNode determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2. spark.blacklist.stage.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire stage. spark.blacklist.stage.maxFailedExecutorsPerNode determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2. spark.blacklist.application.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire application. spark.blacklist.application.maxFailedExecutorsPerNode is same as spark.blacklist.stage.maxFailedExecutorsPerNode but the worker node is blacklisted for the entire application. spark.blacklist.killBlacklistedExecutors when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring. spark.blacklist.application.fetchFailure.enabled when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring. The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application. [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.blacklist.killBlacklistedExecutors\": \"true\", \"spark.blacklist.application.fetchFailure.enabled\": \"true\" }, \"configurations\": [] }] You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs. When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by spark.stage.maxConsecutiveAttempts . This is determined by the setting spark.stage.attempt.ignoreOnDecommissionFetchFailure being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions.","title":"BP 5.1.18  - Consider Spark Blacklisting for large clusters"},{"location":"applications/spark/best_practices/#bp-5119-debugging-and-monitoring-spark-applications","text":"EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR. Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates. Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated. EMR Persistent UI - Amazon EMR provides Persistent User Interface for Spark . This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active. Spark UI options are also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on \"stderr\" or \"stdout\" from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs. Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step's stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live. If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property yarn.log-aggregation.retain-seconds .","title":"BP 5.1.19  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices/#bp-5120-spark-observability-platforms","text":"Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed here . You can choose an observability platform based on your requirements. Following are some example native options.","title":"BP 5.1.20  -  Spark Observability Platforms"},{"location":"applications/spark/best_practices/#amazon-managed-services-for-prometheus-and-grafana","text":"AWS offers Amazon Managed Prometheus (AMP) which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. Amazon Managed Grafana (AMG) is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the deployment instructions available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using PrometheusServlet and prometheus/jmx_exporter . However, some bootstrapping is necessary for this integration.","title":"Amazon Managed Services for Prometheus and Grafana"},{"location":"applications/spark/best_practices/#amazon-opensearch","text":"Amazon Opensearch is a community-driven open source fork of Elasticsearch and Kibana . It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs. Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic.","title":"Amazon Opensearch"},{"location":"applications/spark/best_practices/#bp-5121-potential-resolutions-for-not-so-common-errors","text":"Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions.","title":"BP 5.1.21  -  Potential resolutions for not-so-common errors"},{"location":"applications/spark/best_practices/#potential-strategies-to-mitigate-s3-throttling-errors","text":"For mitigating S3 throttling errors (503: Slow Down), consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg format ObjectStoreLocationProvider to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job's processing requirements increase and thus help mitigate the S3 throttling errors. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProvider here . If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ /.","title":"Potential strategies to mitigate S3 throttling errors"},{"location":"applications/spark/best_practices/#precautions-to-take-while-running-too-many-executors","text":"If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs. ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970 For this issue, you can increase spark.scheduler.listenerbus.eventqueue.size from default of 10000 to 2x or more until you do not see dropped events anymore. Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing spark.executor.heartbeatInterval from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources. If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing spark.yarn.containerLauncherMaxThreads which is defaulted to 25. You may also want to increase spark.yarn.am.memory (default: 512 MB) and spark.yarn.am.cores (default: 1).","title":"Precautions to take while running too many executors"},{"location":"applications/spark/best_practices/#adjust-hadoop-yarn-and-hdfs-heap-sizes-for-intensive-workflows","text":"You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster. In hadoop-env.sh, you can see heap sizes for HDFS daemons. export HADOOP_OPTS = \"$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError\" export HADOOP_NAMENODE_HEAPSIZE = 25190 export HADOOP_DATANODE_HEAPSIZE = 4096 In yarn-env.sh, you can see heap sizes for YARN daemons. export YARN_NODEMANAGER_HEAPSIZE = 2048 export YARN_RESOURCEMANAGER_HEAPSIZE = 7086 Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like \"MissingBlocksException\" in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below: OpenJDK 64-Bit Server VM warning : INFO : os :: commit_memory ( 0x00007f0beb662000 , 12288 , 0 ) failed ; error = 'Cannot allocate memory' ( errno = 12 ) # # There is insufficient memory for the Java Runtime Environment to continue . # Native memory allocation ( mmap ) failed to map 12288 bytes for committing reserved memory . # An error report file with more information is saved as : # / tmp / hs_err_pid14730 . log In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well: [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_OPTS\" : \"-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS\" \u201c HADOOP_DATANODE_HEAPSIZE \u201d : \"4096\" }, \"Configurations\" : [] } ] } ] Additionally, you can also consider reducing yarn.nodemanager.resource.memory-mb by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from yarn.nodemanager.resource.memory-mb for your instance types.","title":"Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows"},{"location":"applications/spark/best_practices/#precautions-to-take-for-highly-concurrent-workloads","text":"When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like \u201cCaused by: java.util.zip.ZipException: error in opening zip file\u201d or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files. 22 / 02 / 25 21 : 39 : 45 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 39 : 45 WARN Client : Neither spark . yarn . jars nor spark . yarn . archive is set , falling back to uploading libraries under SPARK_HOME . 22 / 02 / 25 21 : 39 : 48 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_libs__3016570917637060246 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_libs__3016570917637060246 . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / hive - site . xml 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / pyspark . zip 22 / 02 / 25 21 : 39 : 49 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 39 : 50 INFO Client : Uploading resource file : / mnt / tmp / spark - b0fe28f9 - 17e5 - 42 da - ab8a - 5 c861d81e25b / __spark_conf__7549408525505552236 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0003 / __spark_conf__ . zip To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set spark.yarn.archive to that location. Below is an example: zip -r spark-dependencies.zip /mnt/jars/ hdfs dfs -mkdir /user/hadoop/deps/ hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/ /mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults. spark.yarn.archive hdfs:///user/hadoop/deps/spark-dependencies.zip You can see that this file size is large. hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip -rw-r--r-- 1 hadoop hdfsadmingroup 287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node's CPU to ensure that the utilization has been brought down. 22 / 02 / 25 21 : 56 : 08 INFO Client : Preparing resources for our AM container 22 / 02 / 25 21 : 56 : 08 INFO Client : Source and destination file systems are the same . Not copying hdfs : / user / hadoop / deps / spark - dependencies . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / etc / spark / conf / hive - site . xml -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / hive - site . xml 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / pyspark . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / pyspark . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / usr / lib / spark / python / lib / py4j - 0.10 . 9 - src . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / py4j - 0.10 . 9 - src . zip 22 / 02 / 25 21 : 56 : 08 INFO Client : Uploading resource file : / mnt / tmp / spark - 0 fbfb5a9 - 7 c0c - 4 f9f - befd - 3 c8f56bc4688 / __spark_conf__5472705335503774914 . zip -> hdfs : // ip - 172 - 31 - 45 - 211. ec2 . internal : 8020 / user / hadoop /. sparkStaging / application_1645574675843_0007 / __spark_conf__ . zip If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop\u2019s ShutdownHook . If this happens, increase hadoop.service.shutdown.timeout property from 30s to to a larger value. Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR.","title":"Precautions to take for highly concurrent workloads"},{"location":"applications/spark/best_practices/#bp-5122-how-the-number-of-partitions-are-determined-when-reading-a-raw-file","text":"When reading a raw file, that can be a text file, csv, etc. the count behind the number of partitions created from Spark depends from many variables as the methods used to read the file, the default parallelism and so on. Following an overview of how these factors are related between each other so to better understand how files are processed. Here a brief summary of relationship between core nodes - executors - tasks: each File is composed by blocks that will be parsed according to the InputFormat corresponding to the specific data format, and generally combines several blocks into one input slice, called InputSplit InputSplit and Task are one-to-one correspondence relationship each of these specific Tasks will be assigned to one executor of the nodes on the cluster each node can have one or more Executors, depending on the node resources and executor settings each Executor consists of cores and memory whose default is based on the node type. Each executor can only execute one task at time. So based on that, the number of threads/tasks will be based on the number of partitions while reading. Please note that the S3 connector takes some configuration option (e.g. s3a: fs.s3a.block.size) to simulate blocks in Hadoop services, but the concept of blocks in S3 does not really exists. Unlike HDFS that is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior, EMRFS is an object store, not a file system. For more information, see Hadoop documentation for Object Stores vs. Filesystems . Now, there are several factors that dictate how a dataset or file is mapped to a partition. First is the method used to read the file (e.g. text file), that changes if you're working with rdds or dataframes: sc . textFile (...) returns a RDD [ String ] textFile ( String path , int minPartitions ) Read a text file from HDFS , a local file system ( available on all nodes ), or any Hadoop - supported file system URI , and return it as an RDD of Strings . spark . read . text (...) returns a DataSet [ Row ] or a DataFrame text ( String path ) Loads text files and returns a DataFrame whose schema starts with a string column named \"value\" , and followed by partitioned columns if there are any .","title":"BP 5.1.22  -  How the number of partitions are determined when reading a raw file"},{"location":"applications/spark/best_practices/#spark-core-api-rdds","text":"When using sc.textFile Spark uses the block size set for the filesysytem protocol it's reading from, to calculate the number of partitions in input: SparkContext.scala /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file */ def textFile ( path : String , minPartitions : Int = defaultMinPartitions ) : RDD [ String ] = withScope { assertNotStopped () hadoopFile ( path , classOf [ TextInputFormat ] , classOf [ LongWritable ] , classOf [ Text ] , minPartitions ). map ( pair => pair . _2 . toString ). setName ( path ) } FileInputFormat.java if ( isSplitable ( fs , path )) { long blockSize = file . getBlockSize () ; long splitSize = computeSplitSize ( goalSize , minSize , blockSize ) ; When using the S3A protocol the block size is set through the fs.s3a.block.size parameter (default 32M), and when using S3 protocol through fs.s3n.block.size (default 64M). Important to notice here is that with S3 protocol the parameter used is fs.s3n.block.size and not fs.s3.block.size as you would expect. In EMR indeed, when using EMRFS, which means using s3 with s3:// prefix, fs.s3.block.size will not have any affect on the EMRFS configration. Following some testing results using these parameters: CONF Input: 1 file, total size 336 MB TEST 1 (default) S3A protocol - fs.s3a.block.size = 32M (default) - Spark no. partitions: 336/32 = 11 S3 protocol - fs.s3n.block.size = 64M (default) - Spark no. partitions: 336/64 = 6 TEST 2 (modified) S3A protocol - fs.s3a.block.size = 64M (modified) - Spark no. partitions: 336/64 = 6 S3protocol - fs.s3n.block.size = 128M (modified) - Spark no. partitions: 336/128 = 3","title":"Spark Core API (RDDs)"},{"location":"applications/spark/best_practices/#spark-sql-dataframes","text":"When using spark.read.text no. of spark tasks/partitions depends on default parallelism: DataSourceScanExec.scala val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism val totalBytes = selectedPartitions.flatMap(_.files.map (_.getLen + openCostInBytes)).sum val bytesPerCore = totalBytes / defaultParallelism val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) The default Parallelism is determined via: CoarseGrainedSchedulerBackend.scala override def defaultParallelism(): Int = { conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2)) } If defaultParallelism is too large, bytesPerCore will be small, and maxSplitBytes can be small, which can result in more no. of spark tasks/partitions. So if there're more cores, spark.default.parallelism can be large, defaultMaxSplitBytes can be small, and no. of spark tasks/partitions can be large. In order to tweak the input no. of partitions the following parameters need to be set: Classification Property Description spark-default spark.default.parallelism default: max(total number of vCores, 2) spark-default spark.sql.files.maxPartitionBytes default: 128MB If these parameters are modified, maximizeResourceAllocation need to be disabled, as it would override spark.default.parallelism parameter . Following some testing results using these parameters: CONF - Total number of vCores = 16 -> spark . default . parallelism = 16 - spark . sql . files . maxPartitionBytes = 128 MB TEST 1 - Input : 1 CSV file , total size 352 , 3 MB - Spark no . partitions : 16 - Partition size = 352 , 3 / 16 = \u223c 22 , 09 MB TEST 2 - Input : 10 CSV files , total size 3523 MB - Spark no . partitions : 30 - Partition size = 3523 / 30 = \u223c 117 , 43 MB Disclaimer When writing a file the number of partitions in output will depends from the number of partitions in input that will be maintained if no shuffle operations are applied on the data processed, changed otherwise based on spark.default.parallelism for RDDs and spark.sql.shuffle.partitions for dataframes.","title":"Spark SQL (DATAFRAMEs)"},{"location":"applications/spark/best_practices_BACKUP_3845/","text":"<<<<<<< HEAD 5.1 - Spark \u00b6 BP 5.1.1 - Determine right infrastructure for your Spark workloads \u00b6 ======= 5 - Spark \u00b6 BP 5.1 - Determine right infrastructure for your Spark workloads \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. <<<<<<< HEAD BP 5.1.2 - Choose the right deploy mode \u00b6 ======= BP 5.2 - Choose the right deploy mode \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. <<<<<<< HEAD BP 5.1.3 - Use right file formats and compression type \u00b6 ======= BP 5.3 - Use right file formats and compression type \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) <<<<<<< HEAD BP 5.1.4 - Partitioning \u00b6 ======= BP 5.4 - Partitioning \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. <<<<<<< HEAD BP 5.1.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 ======= BP 5.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. <<<<<<< HEAD For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\". BP 5.1.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 ======= BP 5.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. <<<<<<< HEAD BP 5.1.7 - Use appropriate garbage collector \u00b6 ======= BP 5.7 - Use appropriate garbage collector \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. <<<<<<< HEAD BP 5.1.8 - Use appropriate APIs wherever possible \u00b6 ======= BP 5.8 - Use appropriate APIs wherever possible \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. <<<<<<< HEAD BP 5.1.9 - Leverage spot nodes with managed autoscaling \u00b6 ======= BP 5.9 - Leverage spot nodes with managed autoscaling \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. <<<<<<< HEAD BP 5.1.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 ======= BP 5.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] <<<<<<< HEAD BP 5.1.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 ======= BP 5.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. <<<<<<< HEAD BP 5.1.12 - Spark speculation with EMRFS \u00b6 ======= BP 5.12 - Spark speculation with EMRFS \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. <<<<<<< HEAD BP 5.1.13 - Data quality and integrity checks with deequ \u00b6 ======= BP 5.13 - Data quality and integrity checks with deequ \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog <<<<<<< HEAD BP 5.1.14 - Use DataFrames wherever possible \u00b6 ======= BP 5.14 - Use DataFrames wherever possible \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. <<<<<<< HEAD BP 5.1.15 - Data Skew \u00b6 ======= BP 5.15 - Data Skew \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, <<<<<<< HEAD BP 5.1.16 - Use right type of join \u00b6 ======= BP 5.16 - Use right type of join \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 <<<<<<< HEAD BP 5.1.17 - Configure observability \u00b6 ======= BP 5.17 - Configure observability \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Choose an observability platform based on your requirements. <<<<<<< HEAD BP 5.1.18 - Debugging and monitoring Spark applications \u00b6 BP 5.1.19 - Common Errors \u00b6 ======= BP 5.18 - Debugging and monitoring Spark applications \u00b6 BP 5.19 - Common Errors \u00b6 be7df0756ee7b84580040efd25305e72c02f8054 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"best practices BACKUP 3845"},{"location":"applications/spark/best_practices_BACKUP_3845/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-511-determine-right-infrastructure-for-your-spark-workloads","text":"=======","title":"BP 5.1.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#5-spark","text":"","title":"5 - Spark"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-51-determine-right-infrastructure-for-your-spark-workloads","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. <<<<<<< HEAD","title":"BP 5.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-512-choose-the-right-deploy-mode","text":"=======","title":"BP 5.1.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-52-choose-the-right-deploy-mode","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. <<<<<<< HEAD","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-513-use-right-file-formats-and-compression-type","text":"=======","title":"BP 5.1.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-53-use-right-file-formats-and-compression-type","text":"be7df0756ee7b84580040efd25305e72c02f8054 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) <<<<<<< HEAD","title":"BP 5.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-514-partitioning","text":"=======","title":"BP 5.1.4  -  Partitioning"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-54-partitioning","text":"be7df0756ee7b84580040efd25305e72c02f8054 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. <<<<<<< HEAD","title":"BP 5.4  -  Partitioning"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"=======","title":"BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-55-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"be7df0756ee7b84580040efd25305e72c02f8054 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. <<<<<<< HEAD For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\".","title":"BP 5.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"=======","title":"BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-56-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. <<<<<<< HEAD","title":"BP 5.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-517-use-appropriate-garbage-collector","text":"=======","title":"BP 5.1.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-57-use-appropriate-garbage-collector","text":"be7df0756ee7b84580040efd25305e72c02f8054 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. <<<<<<< HEAD","title":"BP 5.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-518-use-appropriate-apis-wherever-possible","text":"=======","title":"BP 5.1.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-58-use-appropriate-apis-wherever-possible","text":"be7df0756ee7b84580040efd25305e72c02f8054 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_BACKUP_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_BACKUP_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. <<<<<<< HEAD","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-519-leverage-spot-nodes-with-managed-autoscaling","text":"=======","title":"BP 5.1.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-59-leverage-spot-nodes-with-managed-autoscaling","text":"be7df0756ee7b84580040efd25305e72c02f8054 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. <<<<<<< HEAD","title":"BP 5.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5110-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"=======","title":"BP 5.1.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-510-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"be7df0756ee7b84580040efd25305e72c02f8054 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] <<<<<<< HEAD","title":"BP 5.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"=======","title":"BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-511-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"be7df0756ee7b84580040efd25305e72c02f8054 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. <<<<<<< HEAD","title":"BP 5.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5112-spark-speculation-with-emrfs","text":"=======","title":"BP 5.1.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-512-spark-speculation-with-emrfs","text":"be7df0756ee7b84580040efd25305e72c02f8054 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. <<<<<<< HEAD","title":"BP 5.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5113-data-quality-and-integrity-checks-with-deequ","text":"=======","title":"BP 5.1.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-513-data-quality-and-integrity-checks-with-deequ","text":"be7df0756ee7b84580040efd25305e72c02f8054 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog <<<<<<< HEAD","title":"BP 5.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5114-use-dataframes-wherever-possible","text":"=======","title":"BP 5.1.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-514-use-dataframes-wherever-possible","text":"be7df0756ee7b84580040efd25305e72c02f8054 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. <<<<<<< HEAD","title":"BP 5.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5115-data-skew","text":"=======","title":"BP 5.1.15  -   Data Skew"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-515-data-skew","text":"be7df0756ee7b84580040efd25305e72c02f8054 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, <<<<<<< HEAD","title":"BP 5.15  -   Data Skew"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5116-use-right-type-of-join","text":"=======","title":"BP 5.1.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-516-use-right-type-of-join","text":"be7df0756ee7b84580040efd25305e72c02f8054 There are several types of joins in Spark","title":"BP 5.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BACKUP_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#broadcast-nested-loop-join","text":"<<<<<<< HEAD","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5117-configure-observability","text":"=======","title":"BP 5.1.17 -   Configure observability"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-517-configure-observability","text":"be7df0756ee7b84580040efd25305e72c02f8054 Choose an observability platform based on your requirements. <<<<<<< HEAD","title":"BP 5.17 -   Configure observability"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5118-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.1.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-5119-common-errors","text":"=======","title":"BP 5.1.19  -   Common Errors"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-518-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BACKUP_3845/#bp-519-common-errors","text":"be7df0756ee7b84580040efd25305e72c02f8054 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.19  -   Common Errors"},{"location":"applications/spark/best_practices_BASE_3845/","text":"6 - Spark \u00b6 BP 6.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 6.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 6.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 6.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 6.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for BP 6.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 6.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 6.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 6.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 6.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 6.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 6.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 6.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 6.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 6.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 6.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 6.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 6.18 - Debugging and monitoring Spark applications \u00b6 BP 6.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 6 - Spark **"},{"location":"applications/spark/best_practices_BASE_3845/#6-spark","text":"","title":"6 - Spark"},{"location":"applications/spark/best_practices_BASE_3845/#bp-61-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 6.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_BASE_3845/#bp-62-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 6.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_BASE_3845/#bp-63-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 6.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_BASE_3845/#bp-64-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 6.4  -  Partitioning"},{"location":"applications/spark/best_practices_BASE_3845/#bp-65-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for","title":"BP 6.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_BASE_3845/#bp-66-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 6.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_BASE_3845/#bp-67-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 6.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_BASE_3845/#bp-68-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 6.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_BASE_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_BASE_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_BASE_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_BASE_3845/#bp-69-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 6.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_BASE_3845/#bp-610-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 6.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_BASE_3845/#bp-611-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 6.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_BASE_3845/#bp-612-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 6.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_BASE_3845/#bp-613-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 6.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_BASE_3845/#bp-614-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 6.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_BASE_3845/#bp-615-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 6.15  -   Data Skew"},{"location":"applications/spark/best_practices_BASE_3845/#bp-616-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 6.16  -   Use right type of join"},{"location":"applications/spark/best_practices_BASE_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_BASE_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_BASE_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_BASE_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_BASE_3845/#bp-617-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 6.17 -   Configure observability"},{"location":"applications/spark/best_practices_BASE_3845/#bp-618-debugging-and-monitoring-spark-applications","text":"","title":"BP 6.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_BASE_3845/#bp-619-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 6.19  -   Common Errors"},{"location":"applications/spark/best_practices_LOCAL_3845/","text":"5.1 - Spark \u00b6 BP 5.1.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 5.1.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.1.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.1.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 5.1.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\". BP 5.1.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.1.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 5.1.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 5.1.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 5.1.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 5.1.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 5.1.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 5.1.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 5.1.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 5.1.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 5.1.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 5.1.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 5.1.18 - Debugging and monitoring Spark applications \u00b6 BP 5.1.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 5.1 - Spark **"},{"location":"applications/spark/best_practices_LOCAL_3845/#51-spark","text":"","title":"5.1 - Spark"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-511-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 5.1.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-512-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.1.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-513-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.1.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-514-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 5.1.4  -  Partitioning"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-515-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, we recommend you to tune Spark driver/executor configurations and see if you can achieve a better performance. Following are the general recommendations. For a starting point, generally, its advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations. Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following. spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor=0.1875) If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each. Please note that some jobs benefit from bigger executor JVMs (more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation . Setting this property to true will lead to one fat JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal all types of workloads. It is not recommended to set this property to true if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase. After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization. While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s in same fleet). EMR will configure driver/executor memory based on minimum of master, core and task OS memory. Generally, in this case, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in that case, you will need to take YARN memory and vCores of all the different instance families into consideration. To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory are different. Instance YARN memory in MB c5.4xlarge 24576 c5.12xlarge 90112 m5.4xlarge 57344 m5.12xlarge 188416 r5.4xlarge 122880 r5.12xlarge 385024 Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -> 24576 / 4 = 6144. spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB In default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of JVMs. In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property \"spark.yarn.heterogeneousExecutors.enabled\" and is set to \"true\" by default. Further, you will be able to control the maximum resources allocated to each executor with properties \"spark.executor.maxMemory\" and \"spark.executor.maxCores\". Minimum resources are calculated with \"spark.executor.cores\" and \"spark.executor.memory\". For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting this property to false and see if you get better performance. Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then driver resources are taken from the master node or remote server and will not affect the resources available for executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for following conditions: 1) Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver. 2) Your result size retrieved during actions such as printing output to console is very large. For this, you will also need to tune \"spark.driver.maxResultSize\". You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel. Now, coming to \"spark.sql.shuffle.partitions\" for Dataframes and Datasets and \"spark.default.parallelism\" for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a Spark partition. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI. From above, you can see average size in exchange is 2.2 KB which means we can try to reduce \"spark.sql.shuffle.partitions\".","title":"BP 5.1.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-516-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.1.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-517-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 5.1.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-518-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.1.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_LOCAL_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_LOCAL_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_LOCAL_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-519-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 5.1.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5110-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 5.1.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5111-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 5.1.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5112-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 5.1.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5113-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 5.1.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5114-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 5.1.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5115-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 5.1.15  -   Data Skew"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5116-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 5.1.16  -   Use right type of join"},{"location":"applications/spark/best_practices_LOCAL_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5117-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 5.1.17 -   Configure observability"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5118-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.1.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_LOCAL_3845/#bp-5119-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.1.19  -   Common Errors"},{"location":"applications/spark/best_practices_REMOTE_3845/","text":"5 - Spark \u00b6 BP 5.1 - Determine right infrastructure for your Spark workloads \u00b6 Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost. BP 5.2 - Choose the right deploy mode \u00b6 Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements. Client deploy mode \u00b6 This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console. Cluster deploy mode \u00b6 In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs. BP 5.3 - Use right file formats and compression type \u00b6 It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" ) BP 5.4 - Partitioning \u00b6 Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark. BP 5.5 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources \u00b6 EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for BP 5.6 - Use Kryo serializer by registering custom classes especially for Dataset schemas \u00b6 Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k. BP 5.7 - Use appropriate garbage collector \u00b6 By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance. BP 5.8 - Use appropriate APIs wherever possible \u00b6 When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples. repartition vs coalesce \u00b6 Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill. groupByKey vs reduceByKey \u00b6 Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type. orderBy vs sortBy or sortWithinPartitions \u00b6 orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed. BP 5.9 - Leverage spot nodes with managed autoscaling \u00b6 Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. BP 5.10 - For workloads with fixed/predictable pattern, disable dynamic allocation \u00b6 Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }] BP 5.11 - Leverage HDFS as temporary storage for I/O intensive workloads \u00b6 Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section. BP 5.12 - Spark speculation with EMRFS \u00b6 In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination. BP 5.13 - Data quality and integrity checks with deequ \u00b6 Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog BP 5.14 - Use DataFrames wherever possible \u00b6 WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list. BP 5.15 - Data Skew \u00b6 Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting, BP 5.16 - Use right type of join \u00b6 There are several types of joins in Spark Broadcast Join \u00b6 * Broadcast joins are the most optimal options Shuffle Hash Join \u00b6 Sort Merge Join \u00b6 Broadcast Nested Loop Join \u00b6 BP 5.17 - Configure observability \u00b6 Choose an observability platform based on your requirements. BP 5.18 - Debugging and monitoring Spark applications \u00b6 BP 5.19 - Common Errors \u00b6 Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"** 5 - Spark **"},{"location":"applications/spark/best_practices_REMOTE_3845/#5-spark","text":"","title":"5 - Spark"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-51-determine-right-infrastructure-for-your-spark-workloads","text":"Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, start your benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job\u2019s needs. Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and union on large tables, use a lot of internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles (when dynamic allocation is disabled) are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively. CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads . Spark jobs involving complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia. General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of of 3 example instances instances at a similar price. Instance Type Instance EC2 price EMR price Cores Memory in GiB CPU:memory ratio Compute m5.4xlarge $3.06 0.27 72 144 2 Memory m6g.4xlarge $3.02 0.27 48 384 8 General m5.16xlarge $3.07 0.27 64 256 4 Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput and low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD storage like r5ds, c5ds, m5ds etc.. For jobs that perform massive shuffles (when dynamic allocation is enabled), Spark external shuffle service will write the shuffle data blocks to the local disks of each node running executors. GPU instances such as p3 family for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can also use Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines. Starting EMR 5.31+ and 6.1+, there is Graviton support (r6g, m6g, c6g.) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmarks. They are a great choice to replace your legacy instances to observe improved performance at a lower cost.","title":"BP 5.1  -  Determine right infrastructure for your Spark workloads"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-52-choose-the-right-deploy-mode","text":"Spark offers two kinds of deploy modes called client and cluster deploy modes. Deploy mode determines where your Spark driver runs. Spark driver is the cockpit for your application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of all tasks executed by the executors and the state of the executors via heartbeats. Driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.","title":"BP 5.2  -  Choose the right deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#client-deploy-mode","text":"This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your job from EMR master node (using EMR step or spark-submit) or using a client external to EMR. Spark driver is a single point of failure. A failed driver JVM will not be relaunched in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed. Use this deploy mode if :- * You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those drivers running on a single node can lead to resource contention. * You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the resource procurement of your applications. * If you are running too many executors (1000+). Since Spark driver manages and monitors tasks and executors, too many executors will slow down the driver since they have to send heartbeats, poll task status etc. Since EMR allows you to specify a different instance type for master instance, you can choose a very powerful instance like z1d and allocate the memory and CPU resources to Spark drivers especially if you are running a very high number of executors. * If you want to print output to the console.","title":"Client deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#cluster-deploy-mode","text":"In cluster deploy mode, your Spark driver will be colocated within the Application Master (AM) container from YARN regardless of where you submit your Spark application from. Use cluster deploy mode if :- * You are submitting multiple applications at the same time or have higher job or EMR step concurrency. Since drivers will be launched within the AM, for multiple applications, the driver resources will be spread across the cluster considering AM will be launched on one of the worker nodes. * If there are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor from too many executors. * You are storing results in S3/HDFS and there is no need to print output to the console. * You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes. * If you want to relaunch a failed driver JVM. By default, YARN re-attempts AM loss twice based on property \"spark.yarn.maxAppAttempts\". You can increase this value further if needed. * If you want to ensure that termination of your client will not terminate your application. * If you want to return success from your client after job submission based on the property \"spark.yarn.submit.waitAppCompletion\". Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.","title":"Cluster deploy mode"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-53-use-right-file-formats-and-compression-type","text":"It is highly recommended that you use right file formats for optimal performance. Do not use legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet, ORC etc. Especially for Spark, Parquet would be the best choice. When using Parquet file format, Spark will use EMRFSOutputCommitter which is an optimized file committer for writing Parquet data files to S3 from Spark. Also, using Parquet file format is great for schema evolution, filter push downs and integration with applications with transactional support like Apache Hudi, Iceberg etc. Also, it\u2019s recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when there is a large GZIP compressed file, it can only be processed by a single task/executor leading to OOM errors. Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use defaults. You can also apply columnar encryption using KMS. sc . hadoopConfiguration . set ( \"parquet.encryption.kms.client.class\" , \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\" ) // Explicit master keys ( base64 encoded ) - required only for mock InMemoryKMS sc . hadoopConfiguration . set ( \"parquet.encryption.key.list\" , \"keyA:AAECAwQFBgcICQoLDA0ODw== , keyB:AAECAAECAAECAAECAAECAA==\" ) // Activate Parquet encryption , driven by Hadoop properties sc . hadoopConfiguration . set ( \"parquet.crypto.factory.class\" , \"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\" ) // Write encrypted dataframe files . // Column \"square\" will be protected with master key \"keyA\" . // Parquet file footers will be protected with master key \"keyB\" squaresDF . write . option ( \"parquet.encryption.column.keys\" , \"keyA:square\" ) . option ( \"parquet.encryption.footer.key\" , \"keyB\" ) . parquet ( \"/path/to/table.parquet.encrypted\" ) // Read encrypted dataframe files val df2 = spark . read . parquet ( \"/path/to/table.parquet.encrypted\" )","title":"BP 5.3  -  Use right file formats and compression type"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-54-partitioning","text":"Partitioning your data is very important if you are going to run your code or queries with filter conditions. Partitioning helps you arrange your data files into S3 prefixes based on the partition key. It helps minimize read/write access footprint i.e., you will be able to only read files from partition folder specified in your where clause - thus skipping a full read. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput if you are performing full table scans. You can choose your partition field based on :- Query pattern. i.e., if you find workloads use 1-2 columns frequently as filter fields more so than other columns, it is recommended to consider using them as partitioning field. Ingestion pattern. i.e., if you load data into your table once everyday, if you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format) Cardinality of the partitioning column. For partitioning, cardinality should not be too high. File sizes per partition. It is highly recommended that your individual file sizes within each partition is ~128 MB and not too small since that would be optimal for Spark executor JVM to process. Also, number of shuffle partitions will determine the number of output files per partition. df.repartition(400).write.partitionBy(\"datecol\").parquet(\"s3://bucket/output/\") The above code will create maximum of 400 files per datecol partition. You can use repartitioning to control the file size in destination. i.e., for merging smaller files. But for splitting large files, you can use the property \"spark.sql.files.maxPartitionBytes\". Partitioning ensures that pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Query plan can be studied from Spark UI to ensure that pruning takes place while reading and writing to partitioned tables from Spark.","title":"BP 5.4  -  Partitioning"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-55-tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","text":"EMR defaults are aimed at smaller JVM sizes. For example, following are the default configuration for","title":"BP 5.5 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-56-use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","text":"Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application. val spark = SparkSession . builder . appName ( \"my spark application name\" ) . config ( getConfig ) . config ( \"spark.serializer\" , \"org.apache.spark.serializer.KryoSerializer\" ) // use this if you need to increment Kryo buffer size. Default 64k . config ( \"spark.kryoserializer.buffer\" , \"1024k\" ) // use this if you need to increment Kryo buffer max size. Default 64m . config ( \"spark.kryoserializer.buffer.max\" , \"1024m\" ) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ . config ( \"spark.kryo.registrationRequired\" , \"true\" ) . getOrCreate If you do not specify classesToRegister, then there will be a Kryo conversion overhead. So, it is highly recommended to register classes in your application. Especially, if you are using data sets, consider registering your data set schema classes along with classes used in Spark internally based on the data types and structures used in your program. val conf = new SparkConf () conf . registerKryoClasses ( Array ( classOf [ org.myPackage.FlightDataset ] , classOf [ org.myPackage.BookingDataset ] , classOf [ scala.collection.mutable.WrappedArray.ofRef[_ ] ] , classOf [ org.apache.spark.sql.types.StructType ] , classOf [ Array[org.apache.spark.sql.types.StructType ] ] , classOf [ org.apache.spark.sql.types.StructField ] , classOf [ Array[org.apache.spark.sql.types.StructField ] ] , Class . forName ( \"org.apache.spark.sql.types.StringType$\" ), Class . forName ( \"org.apache.spark.sql.types.LongType$\" ), Class . forName ( \"org.apache.spark.sql.types.BooleanType$\" ), Class . forName ( \"org.apache.spark.sql.types.DoubleType$\" ), classOf [ org.apache.spark.sql.types.Metadata ] , classOf [ org.apache.spark.sql.types.ArrayType ] , Class . forName ( \"org.apache.spark.sql.execution.joins.UnsafeHashedRelation\" ), classOf [ org.apache.spark.sql.catalyst.InternalRow ] , classOf [ Array[org.apache.spark.sql.catalyst.InternalRow ] ] , classOf [ org.apache.spark.sql.catalyst.expressions.UnsafeRow ] , Class . forName ( \"org.apache.spark.sql.execution.joins.LongHashedRelation\" ), Class . forName ( \"org.apache.spark.sql.execution.joins.LongToUnsafeRowMap\" ), classOf [ org.apache.spark.util.collection.BitSet ] , classOf [ org.apache.spark.sql.types.DataType ] , classOf [ Array[org.apache.spark.sql.types.DataType ] ] , Class . forName ( \"org.apache.spark.sql.types.NullType$\" ), Class . forName ( \"org.apache.spark.sql.types.IntegerType$\" ), Class . forName ( \"org.apache.spark.sql.types.TimestampType$\" ), Class . forName ( \"org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult\" ), Class . forName ( \"org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage\" ), Class . forName ( \"scala.collection.immutable.Set$EmptySet$\" ), Class . forName ( \"scala.reflect.ClassTag$$anon$1\" ), Class . forName ( \"java.lang.Class\" ) ) ) } You can also fine tune the following Kryo configs :- spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster has a mix of AMD and intel types for example. spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase but this property upto 1024m value should be below 2048m spark.kryoserializer.buffer - Initial size of Kryo's serialization buffer. Default is 64k. Recommended to increase up to 1024k.","title":"BP 5.6 -  Use Kryo serializer by registering custom classes especially for Dataset schemas"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-57-use-appropriate-garbage-collector","text":"By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility. Following is the spark configuration :- [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\", \"spark.driver.extraJavaOptions\": \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\" }, \"configurations\": [] }] You can also tune the GC parameters for GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below :- -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200 You can also monitor GC performance using Spark UI. the GC time should be ideally <= 1% of total task runtime. If not, tune the GC settings or executor size. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is a poor GC performance.","title":"BP 5.7  -   Use appropriate garbage collector"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-58-use-appropriate-apis-wherever-possible","text":"When using spark APIs, try to go with the most optimal choice if your use case permits. Following are a few examples.","title":"BP 5.8  -   Use appropriate APIs wherever possible"},{"location":"applications/spark/best_practices_REMOTE_3845/#repartition-vs-coalesce","text":"Both repartition and coalesce are used for changing the number of shuffle partitions. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. The reason is, repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as receivers of shuffle data. df.coalesce(1) //instead of df.repartition(1) But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.","title":"repartition vs coalesce"},{"location":"applications/spark/best_practices_REMOTE_3845/#groupbykey-vs-reducebykey","text":"Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.","title":"groupByKey vs reduceByKey"},{"location":"applications/spark/best_practices_REMOTE_3845/#orderby-vs-sortby-or-sortwithinpartitions","text":"orderBy does global sorting. i.e., all data is sorted in a single JVM. Whereas, sortBy or sortWithinPartitions does local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global sorting is not necessary - especially during writes. Try to avoid orderBy clause. Values can be aggregated across partitions in your queries if needed.","title":"orderBy vs sortBy or sortWithinPartitions"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-59-leverage-spot-nodes-with-managed-autoscaling","text":"Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2 there have been optimizations made to managed scaling to make it more resilient for your Spark workloads. Try to leverage task instance fleets with many instance types per fleet with Spot request since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to EMRFS since we will have limited core node capacity. Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand which is highly recommended. Rest of the nodes are Spot task nodes. Following experimentation illustrates the performance gains using Managed Autoscaling. For Spark workloads, we observed ~50% gains compared to custom autoscaling clusters.","title":"BP 5.9 -   Leverage spot nodes with managed autoscaling"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-510-for-workloads-with-fixedpredictable-pattern-disable-dynamic-allocation","text":"Dynamic allocation is enabled in EMR by default. It is a great feature when your cluster is set for autoscaling and [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\": \"12\", \"spark.executor.memory\": \"4G\", \"spark.executor.cores\": \"2\" }, \"configurations\": [] }]","title":"BP 5.10  -   For workloads with fixed/predictable pattern, disable dynamic allocation"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-511-leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","text":"Many EMR users directly read and write data in S3. This is generally suited for most type of use cases. However, for I/O intensive workflows, this approach could be slower - especially for heavy writes. For very I/O intensive workloads or for workloads where the temporary transform data is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location. For example, for a fraud detection use case where you could be performing transforms on TBs of data but your final output report is only a few KBs, in such scenarios, leveraging HDFS will give you better performance and also helps you avoid S3 throttling. Following is a typical application of HDFS for transient storage. A Spark context could be shared between multiple workflows, wherein, each workflow comprising of multiple transformations. After all transformations are complete, each workflow would write the output to HDFS location. Once all transforms are complete, you can save the final output to S3 either using S3DistCp or SDK determined by the number of files and output size. Even if you are using S3 directly to store your data, if your workloads are intensive, use disk optimized instances or SSD storage (i.e., r5d\u2019s and r6gd\u2019s instead of r5s and r6g\u2019s). Because, the dynamic allocation will use Spark external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 3.14 in Reliability section.","title":"BP 5.11  -   Leverage HDFS as temporary storage for I/O intensive workloads"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-512-spark-speculation-with-emrfs","text":"In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to data loss or duplicate data. By default, \u201cspark.speculation\u201d is turned off. Only enable spark.speculation if you are doing one of the following. Writing Parquet files to S3 using EMRFSOutputCommitter Using HDFS as temporary storage (in an understanding that final output will be written to S3 using S3DistCp) Using HDFS as storage You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. Reason is that, due to some hardware issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met). Please do not enable spark.speculation if you are not using EMRFSOutputCommitter to write Parquet files or if you are not using HDFS to write the output since it may lead to incorrect or missing or duplicate data in your destination.","title":"BP 5.12  -   Spark speculation with EMRFS"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-513-data-quality-and-integrity-checks-with-deequ","text":"Spark and Hadoop frameworks do not guarantee inherent data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. In order to check your data integrity, consider using Deequ for your Spark workloads. Following are the blogs that can help you get started with Deequ for Spark workloads. Test data quality at scale with Deequ | AWS Big Data Blog Testing data quality at scale with PyDeequ | AWS Big Data Blog","title":"BP 5.13 -   Data quality and integrity checks with deequ"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-514-use-dataframes-wherever-possible","text":"WKT we must use Dataframes and Datasets instead of RDDs since both have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example - Datasets perform many serializations and deserializations that Dataframes do not perform. Dataframes perform better push downs. For example, if there is a filter operation, that is applied early on in the query plan so that the data transfer in-memory is reduced. Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in datasets but with only one exchange in DFs. Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in class. case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked in a single class. This can be considered as an industry standard. While using Spark dataframes, you can achieve something similar by maintaining the table columns in a list.","title":"BP 5.14 -   Use DataFrames wherever possible"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-515-data-skew","text":"Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues. When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size but that will impact other tasks and is not the best approach. Some common approaches include salting,","title":"BP 5.15  -   Data Skew"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-516-use-right-type-of-join","text":"There are several types of joins in Spark","title":"BP 5.16  -   Use right type of join"},{"location":"applications/spark/best_practices_REMOTE_3845/#broadcast-join","text":"* Broadcast joins are the most optimal options","title":"Broadcast Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#shuffle-hash-join","text":"","title":"Shuffle Hash Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#sort-merge-join","text":"","title":"Sort Merge Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#broadcast-nested-loop-join","text":"","title":"Broadcast Nested Loop Join"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-517-configure-observability","text":"Choose an observability platform based on your requirements.","title":"BP 5.17 -   Configure observability"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-518-debugging-and-monitoring-spark-applications","text":"","title":"BP 5.18  - Debugging and monitoring Spark applications"},{"location":"applications/spark/best_practices_REMOTE_3845/#bp-519-common-errors","text":"Avoid 503 slow downs For mitigating S3 throttling errors, consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it based on your workload needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB. [{ \"classification\" : \"emrfs-site\" , \"properties\" : { \"fs.s3.maxRetries\" : \"20\" , \"fs.s3n.multipart.uploads.split.size\" : \"268435456\" }, \"configurations\" : [] }] Consider using Iceberg tables\u2019 ObjectStoreLocationProvider to store data under [0*7FFFFF] prefixes and thus, help Amazon S3 learn write pattern to scale accordingly. CREATE TABLE my_catalog . my_ns . my_table ( id bigint , data string , category string ) USING iceberg OPTIONS ( 'write.object-storage.enabled' = true , 'write.data.path' = 's3://my-table-data-bucket' ) PARTITIONED BY ( category ); Your S3 files will be arranged under MURMUR3 hash prefixes like below. 2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet If using Iceberg is not an option and if above approaches don\u2019t resolve the issue, you can create an AWS support case to partition your S3 prefixes. But the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/ / Increase event queue size and heartbeat interval for large number of executors If the size of \u201cdropped events Increase HADOOP, YARN and HDFS heap sizes for intensive workflows Use spark.yarn.archive to avoid compressing dependencies especially for high concurrency workloads","title":"BP 5.19  -   Common Errors"},{"location":"applications/spark/introduction/","text":"Introduction \u00b6 This section offers best practices and tuning guidance for running Apache Spark workloads on Amazon EMR. The guidances cover the following main themes :- Cost optimization Performance optimization Error mitigation","title":"Introduction"},{"location":"applications/spark/introduction/#introduction","text":"This section offers best practices and tuning guidance for running Apache Spark workloads on Amazon EMR. The guidances cover the following main themes :- Cost optimization Performance optimization Error mitigation","title":"Introduction"},{"location":"architecture/adhoc/architecture/","text":"Ad Hoc - Architecture \u00b6 The following diagram illustrates a common architecture to use PrestoSQL/Trino on Amazon EMR with the Glue Data Catalog as a big data query engine to query data in Amazon S3 using standard SQL.","title":"Architecture"},{"location":"architecture/adhoc/architecture/#ad-hoc-architecture","text":"The following diagram illustrates a common architecture to use PrestoSQL/Trino on Amazon EMR with the Glue Data Catalog as a big data query engine to query data in Amazon S3 using standard SQL.","title":"Ad Hoc - Architecture"},{"location":"architecture/adhoc/introduction/","text":"Ad Hoc - Introduction \u00b6 Amazon EMR on EC2 provides a number of engines to support your ad hoc query use cases. Trino, (formely PrestoSQL) has become a popular choice for its low latency, ANSI SQL standard, ease of use and integrations with Amazon EMR feature set. Choosing between Amazon Athena and Trino on Amazon EMR \u00b6 Amazon Athena is a serverless interactive query engine that executes SQL queries on data that rests in Amazon S3. Many customers use Athena for a wide variety of use cases, including interactive querying of data to exploring data, to powering dashboards on top of operational metrics saved on S3, to powering visualization tools, such as Amazon QuickSight or Tableau. We recommend you consider Amazon Athena for these types of workloads. Athena is easy to integrate with, has several features, such as cost management and security controls, and requires little capacity planning. All of these characteristics lead to lower operational burden and costs. However, there are some use cases where Trino on Amazon EMR may be better suited than Amazon Athena. For example, consider the following priorities: Cost reduction: If cost reduction is your primary goal, we recommend that you estimate cost based on both approaches. You may find that the load and query patterns are lower in cost with Trino on Amazon EMR. Keep in mind that there is an operational cost associated with managing a Trino EMR environment. You\u2019ll need to weight the cost benefits of Trino on EMR vs its operational overhead. Performance or Specific Tuning requirements: If your use case includes a high sensitivity to performance or you want the ability to fine-tune a Presto cluster to meet the performance requirements then Trino on EMR may be a better fit. Critical features: If there are features that Amazon Athena does not currently provide, such as the use of custom serializer/deserializers for custom data types, or connectors to data stores other than those currently supported, then Trino on EMR may be a better fit. The rest of the section will focus on Trino on Amazon EMR. For more details on Amazon Athena, see here: https://aws.amazon.com/athena/","title":"Introduction"},{"location":"architecture/adhoc/introduction/#ad-hoc-introduction","text":"Amazon EMR on EC2 provides a number of engines to support your ad hoc query use cases. Trino, (formely PrestoSQL) has become a popular choice for its low latency, ANSI SQL standard, ease of use and integrations with Amazon EMR feature set.","title":"Ad Hoc - Introduction"},{"location":"architecture/adhoc/introduction/#choosing-between-amazon-athena-and-trino-on-amazon-emr","text":"Amazon Athena is a serverless interactive query engine that executes SQL queries on data that rests in Amazon S3. Many customers use Athena for a wide variety of use cases, including interactive querying of data to exploring data, to powering dashboards on top of operational metrics saved on S3, to powering visualization tools, such as Amazon QuickSight or Tableau. We recommend you consider Amazon Athena for these types of workloads. Athena is easy to integrate with, has several features, such as cost management and security controls, and requires little capacity planning. All of these characteristics lead to lower operational burden and costs. However, there are some use cases where Trino on Amazon EMR may be better suited than Amazon Athena. For example, consider the following priorities: Cost reduction: If cost reduction is your primary goal, we recommend that you estimate cost based on both approaches. You may find that the load and query patterns are lower in cost with Trino on Amazon EMR. Keep in mind that there is an operational cost associated with managing a Trino EMR environment. You\u2019ll need to weight the cost benefits of Trino on EMR vs its operational overhead. Performance or Specific Tuning requirements: If your use case includes a high sensitivity to performance or you want the ability to fine-tune a Presto cluster to meet the performance requirements then Trino on EMR may be a better fit. Critical features: If there are features that Amazon Athena does not currently provide, such as the use of custom serializer/deserializers for custom data types, or connectors to data stores other than those currently supported, then Trino on EMR may be a better fit. The rest of the section will focus on Trino on Amazon EMR. For more details on Amazon Athena, see here: https://aws.amazon.com/athena/","title":"Choosing between Amazon Athena and Trino on Amazon EMR"},{"location":"architecture/batch/introduction/","text":"coming soon...","title":"Introduction"},{"location":"architecture/datalake_storage/introduction/","text":"coming soon...","title":"Introduction"},{"location":"architecture/notebooks/introduction/","text":"coming soon...","title":"Introduction"},{"location":"cost_optimization/best_practices/","text":"1 - Cost Optimizations \u00b6 Best Practices (BP) for running cost optimized workloads on EMR. BP 1.1 Use Amazon S3 as your persistent data store \u00b6 As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost. BP 1.2 Compress, compact and convert your Amazon S3 Objects \u00b6 Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size. BP 1.3 Partition and Bucket your data in Amazon S3 \u00b6 Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones. BP 1.4 Use the right hardware family for the job type \u00b6 Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure. BP 1.5 Use instances with instance store for jobs that require high disk IOPS \u00b6 Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost. BP 1.6 Use Graviton2 instances \u00b6 Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances BP 1.7 Select the appropriate pricing model for your use case and node type \u00b6 The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html BP 1.8 Use spot instances \u00b6 Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. For more information, see the spot usage best practices section Additionally, AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances BP 1.9 Mix on-Demand and spot instances \u00b6 Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand. BP 1.10 Use EMR managed scaling \u00b6 With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost. BP 1.11 Right size application containers \u00b6 By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html BP 1.12 Monitor cluster utilization \u00b6 Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ BP 1.13 Monitor and decommission idle EMR cluster \u00b6 Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/ BP 1.14 Use the latest Amazon EMR version \u00b6 Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html BP 1.15 Using transient and long-running clusters \u00b6 Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"Best Practices"},{"location":"cost_optimization/best_practices/#1-cost-optimizations","text":"Best Practices (BP) for running cost optimized workloads on EMR.","title":"1 - Cost Optimizations"},{"location":"cost_optimization/best_practices/#bp-11-use-amazon-s3-as-your-persistent-data-store","text":"As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you\u2019ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~ 4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication. Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings. EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3. Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost.","title":"BP 1.1 Use Amazon S3 as your persistent data store"},{"location":"cost_optimization/best_practices/#bp-12-compress-compact-and-convert-your-amazon-s3-objects","text":"Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data. Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same. Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count( * ) query between Parquet and JSON (text) file formats. The query that executed over parquet ran 74x faster despite being larger in size.","title":"BP 1.2 Compress, compact and convert your Amazon S3 Objects"},{"location":"cost_optimization/best_practices/#bp-13-partition-and-bucket-your-data-in-amazon-s3","text":"Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost. For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not. The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data. Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example: CREATE TABLE IF NOT EXISTS database1 . table1 ( col1 INT , col2 STRING , col3 TIMESTAMP ) CLUSTERED BY ( col1 ) INTO 5 BUCKETS STORED AS PARQUET LOCATION \u2018 s3 :/// buckets_test / hive-clustered / \u2019 ; In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause. Bucketing is similar to partitioning \u2013 in both cases, data is segregated and stored \u2013 but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones.","title":"BP 1.3 Partition and Bucket your data in Amazon S3"},{"location":"cost_optimization/best_practices/#bp-14-use-the-right-hardware-family-for-the-job-type","text":"Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly. The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it\u2019s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure.","title":"BP 1.4 Use the right hardware family for the job type"},{"location":"cost_optimization/best_practices/#bp-15-use-instances-with-instance-store-for-jobs-that-require-high-disk-iops","text":"Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that spend heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost.","title":"BP 1.5 Use instances with instance store for jobs that require high disk IOPS"},{"location":"cost_optimization/best_practices/#bp-16-use-graviton2-instances","text":"Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances. For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is Instance Type EC2 + EMR Cost m5.4xlarge: $0.960 m6g.4xlarge: $0.770 This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory For more information, see: https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances","title":"BP 1.6 Use Graviton2 instances"},{"location":"cost_optimization/best_practices/#bp-17-select-the-appropriate-pricing-model-for-your-use-case-and-node-type","text":"The following table is general guideline for purchasing options depending on your application scenario. Application scenario Master node purchasing option Core nodes purchasing option Task nodes purchasing option Long-running clusters and data warehouses On-Demand On-Demand or instance-fleet mix Spot or instance-fleet mix Cost-driven workloads Spot Spot Spot Data-critical workloads On-Demand On-Demand Spot or instance-fleet mix Application testing Spot Spot Spot For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html","title":"BP 1.7 Select the appropriate pricing model for your use case and node type"},{"location":"cost_optimization/best_practices/#bp-18-use-spot-instances","text":"Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets. For more information, see the spot usage best practices section Additionally, AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/ Amazon EMR Cluster configuration guidelines and best practices https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances","title":"BP 1.8 Use spot instances"},{"location":"cost_optimization/best_practices/#bp-19-mix-on-demand-and-spot-instances","text":"Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when: 1) Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated. In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes. 2) You need to meet SLAs but are also considered about cost In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you\u2019ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job. For example: 10 node cluster run ning for 14 hours Cos t = 1.0 * 10 * 14 = $ 140 Add 10 more nodes on Spot at 0.5 $ / node 20 node cluster run ning for 7 hours Cos t = 1.0 * 10 * 7 = $ 70 = 0.5 * 10 * 7 = $ 35 To tal $ 105 50 % less run - time ( 14 \u2192 7 ) 25 % less cos t ( 140 \u2192 105 ) One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand.","title":"BP 1.9 Mix on-Demand and spot instances"},{"location":"cost_optimization/best_practices/#bp-110-use-emr-managed-scaling","text":"With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application. Here\u2019s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need. Here\u2019s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost.","title":"BP 1.10 Use EMR managed scaling"},{"location":"cost_optimization/best_practices/#bp-111-right-size-application-containers","text":"By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It\u2019s important to use all the available resources of the cluster - both CPU and Memory. For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you\u2019d have 16 vCPU and 64GB of memory. EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4 Spark will only be able to allocate 2 executors on each node resulting in 57,344-44,000 (22,000 * 2) = 13,344 of unallocated resources and 76.7% memory utilization However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example, spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4 Spark will be able to allocate 4 executors on each node resulting in only 57,344-52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization For more information on Spark and YARN right sizing see: https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html","title":"BP 1.11 Right size application containers"},{"location":"cost_optimization/best_practices/#bp-112-monitor-cluster-utilization","text":"Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus. Regardless of which tool you use, you\u2019ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you\u2019re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle. For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example, if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU. If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically. For more information on which cloudwatch metrics are available, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html For more information on the Grafana and Prometheus solution, see: https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/","title":"BP 1.12 Monitor cluster utilization"},{"location":"cost_optimization/best_practices/#bp-113-monitor-and-decommission-idle-emr-cluster","text":"Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR\u2019s \u201cautomatic termination policy\u201d starting 5.30.0 and 6.1.0 or, by monitoring the \u201cisIdle\u201d metric in cloudwatch and terminating yourself. With EMR\u2019s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-auto-termination-policy.html With EMR\u2019s \u201cisIdle\u201d cloudwatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with the \u201cIsIdle\u201d Metrics For a sample solution of this approach, see https://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/","title":"BP 1.13 Monitor and decommission idle EMR cluster"},{"location":"cost_optimization/best_practices/#bp-114-use-the-latest-amazon-emr-version","text":"Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features. For more information, see EMR Release Guide https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html","title":"BP 1.14 Use the latest Amazon EMR version"},{"location":"cost_optimization/best_practices/#bp-115-using-transient-and-long-running-clusters","text":"Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type. In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don\u2019t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc. In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs. From a cost optimization standpoint, If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you\u2019re able to lower your requested compute while still meeting your SLA. If using long running clusters, ensure you\u2019re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters.","title":"BP 1.15 Using transient and long-running clusters"},{"location":"cost_optimization/introduction/","text":"EMR Cost Optimization best practices focus on the continual process of refinement and improvement of a system over its entire lifecycle. From the initial design of your very first proof of concept to the ongoing operation of production workloads, adopting the practices in this document can enable you to build and operate cost-aware systems that achieve business outcomes and minimize costs, thus allowing your business to maximize its return on investment. A cost-optimized workload is one that Meets functional and non functional requirements Fully utilizes all cluster resources and Achieves an outcome at the lowest possible price point To better understand this, let\u2019s look at an example. Let\u2019s assume we have an ETL job that needs to be completed within 8 hours. In order to meet the requirements of completing the job within 8 hours, a certain amount of compute resources will be required. This is represented in the graph by the \u201cJob Demand\u201d. Sometimes this is static, where the amount of resources needed is consistent throughout the duration of the job. And sometimes it\u2019s more dynamic, where throughout the job, you have various peaks and valleys depending on the number of tasks that are running at each stage. In order for the job to finish within the 8 hours, it needs enough cluster capacity to meet the jobs compute demand - represented by the blue dotted line. If our cluster capacity is below our jobs compute demand Our job will be resource constrained and It\u2019ll cause the job to run longer than our 8 hour sla Now, just being able to meet your SLA is not enough to be cost optimized. This leads us to our 2nd step of a cost optimized workload - Fully utilizing all cluster resources Take these next two graphs as an example, in the first case, we have a cluster that has compute capacity well beyond the jobs needs, represented by space in between the jobs demand and cluster capacity In this 2nd graph, we have a better match between the clusters capacity and jobs compute demands The space in between in between two is unused resources. These are resources that are being charged for but the job does not actually need. Fully utilizing all resources means reducing this space as much as possible. Going back to our job with less predictable workload patterns, a static cluster size may not be the best way to maximize cluster resources, but instead, using something like EMR autoscaling that adjusts cluster capacity based off of your workload demand would be a better fit. In this graph, our cluster scales up and down depending on demand. Our cluster capacity becomes a function of the jobs demand of resources. The last part of being \u201cCost Optimized\u201d is achieving your jobs outcomes at the lowest price point possible. EMR has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your needs. For example, On-Demand, Spot and Commitment discounts - Savings Plans/ Reserved Instances/Capacity All of these pricing options leverage the exact same infrastructure but depending on which option you choose, the cost of your job will vary significantly. The numbers are just examples, with spot you can get up to 90% off on demand prices and with saving plans or RI, up to 72%. In the next sections, we\u2019ll discuss best practices on choosing the right pricing model for your workload. For the purpose of this example, regardless of of which option you choose, the cluster compute capacity stays the same.","title":"Introduction"},{"location":"features/capacity/best_practices/","text":"best_practices.md","title":"Best practices"},{"location":"features/managed_scaling/best_practices/","text":"4.1 - Managed Scaling \u00b6 BP 4.1.1 Keep core nodes constant and scale with only task nodes \u00b6 Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. During scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need rebalance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes' disk volume during HDFS rebalance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy, making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the HDFS replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that at a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot. BP 4.1.2 Monitor Managed Scaling with Cloudwatch Metrics \u00b6 You can monitor your managed scaling cluster with CloudWatch metrics. This is useful if you want to better understand how your cluster is resizing to the change in job load/usage. Lets looks at an example: At 18:25, \u201cYARNMemoryAvailablePercentage\u201d starts at 100%. This means that no jobs are running. At 18:27 a job starts and we see \u201cYARNMemoryAvailablePercentage\u201d begin to drop, reaching 0% at 18:29. This triggers managed scaling to start a resize request - represented by the increase in the metric \u201cTotalNodesRequested\u201d. After 5-6 mins, at 18:35 the nodes finish provisioning and are considered \u201cRUNNING\u201d. We see an increase in the metric, \u201cTotalNodesRunning\u201d. Around the same time, we see \u201cYARNMemoryAvailablePercentage\u201d begin increasing back to 100%. For a full list of metrics and description of each, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/managed-scaling-metrics.html BP 4.1.3 Consider adjusting YARN decommissioning timeouts depending on your workload \u00b6 There are two decommissioning timeouts that are important in managed scaling: yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED. spark.blacklist.decommissioning.timeout: This is the maximal time that Spark does not schedule new tasks on executors running on that node. Tasks already running are allowed to complete. When managed scaling triggers a scale down, YARN will put nodes it wants to decomission in a \u201cDECOMMISSIONING\u201d state. Spark will detect this and add these nodes to a \u201cblack list\u201d. In this state, Spark will not assign any new tasks to the node and once all tasks are completed, YARN will finish decommissioning the node. If the task runs longer than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs , the node is force-terminated and the task will be reassigned to another node. In certain scale down scenarios where you have long running tasks, many nodes can end up in this state where they are \u201cDECOMMISSIONING\u201d and \u201cblacklisted\u201d because of spark.blacklist.decommissioning.timeout. You may observe that new jobs run slower because it cannot assign tasks to all nodes in the cluster. To mitigate this, you can lower spark.blacklist.decommissioning.timeout to make the node available for other pending containers to continue task processing. This can improve job run times. However, please take the below into consideration: If a task is assigned to this node, and YARN transitions from DECOMMISSIONING into DECOMMISSIONED, the task will fail and will need to be reassigned to another node. Spark blacklist also protects from bad nodes in the cluster, e.g., faulty hardware leading to high task failure rate. Lowering the blacklist timeout can increase task failure rate since tasks will continue to be assigned to these nodes. Nodes can be transitioned from DECOMMISSIONING to RUNNING due to a scale up request. In this scenario, tasks will not fail and with a lower blacklist timeout, pending tasks can continuously be assigned to the node. With yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, consider increasing this from the default of 1hr to the length of your longest running task. This is to ensure that YARN does not force-terminate the node while the task is running, causing it to re-run on another node. The cost associated with rerunning the long running task is generally higher than keeping the node running to ensure it's completed. For more information, see: https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-3.html https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#spark-decommissioning BP 4.1.5 EMR Managed Scaling compared to Custom Automatic Scaling \u00b6 The following link highlights the key differences between EMR managed scaling vs. custom automatic scaling: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-scale-on-demand.html In general, we recommend using EMR managed scaling since the metric evaluation is every 5-10 seconds. This means your EMR cluster will adjust quicker to the change in the required cluster resources. In addition, EMR managed scaling also supports instance fleets and the the scaling policy is simpler to configure because EMR managed scaling only requires min and max amounts for purchasing options (On-Demand/Spot) and node type (core/task). Custom automatic scaling should be considered if you want autoscaling outside of YARN applications or if you want full control over your scaling policies (e.g., evaluation period, cool down, number of nodes) BP 4.1.6 Configure Spark History Server (SHS) custom executor log URL to point to Job History Server (JHS) Directly \u00b6 When you use SHS to access application container logs, YARN ResourceManager relies on the NodeManager that the jobs' Application Master (AM) ran on, to redirect to the JHS. The JHS is what hosts the container logs. A job's executor logs cannot be accessed if the AM ran on a node that\u2019s been decommissioned due to managed scaling or spot. A solution to this is pointing SHS to the JHS directly, instead of letting node manager redirect. Spark 3.0 introduced spark.history.custom.executor.log.url , which allows you to specify a custom Spark executor log url. You can configure spark.history.custom.executor.log.url as below to point to JHS directly: {{ HTTP_SCHEME }} <JHS_HOST>:<JHS_PORT>/jobhistory/logs/ {{ NM_HOST }} : {{ NM_PORT }} / {{ CONTAINER_ID }} / {{ CONTAINER_ID }} / {{ USER }} / {{ FILE_NAME }} ?start=-4096 Replace JHS_HOST and JHS_PORT with actual values. JHS_HOST is the EMR master node.","title":"4.1 - Managed Scaling"},{"location":"features/managed_scaling/best_practices/#41-managed-scaling","text":"","title":"4.1 - Managed Scaling"},{"location":"features/managed_scaling/best_practices/#bp-411-keep-core-nodes-constant-and-scale-with-only-task-nodes","text":"Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. During scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need rebalance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes' disk volume during HDFS rebalance. If the nodes disk utilization exceeds 90%, it\u2019ll mark the node as unhealthy, making it unusable by YARN. In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the HDFS replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that at a minimum, you keep 2 core nodes and set dfs.replication=2. Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot.","title":"BP 4.1.1 Keep core nodes constant and scale with only task nodes"},{"location":"features/managed_scaling/best_practices/#bp-412-monitor-managed-scaling-with-cloudwatch-metrics","text":"You can monitor your managed scaling cluster with CloudWatch metrics. This is useful if you want to better understand how your cluster is resizing to the change in job load/usage. Lets looks at an example: At 18:25, \u201cYARNMemoryAvailablePercentage\u201d starts at 100%. This means that no jobs are running. At 18:27 a job starts and we see \u201cYARNMemoryAvailablePercentage\u201d begin to drop, reaching 0% at 18:29. This triggers managed scaling to start a resize request - represented by the increase in the metric \u201cTotalNodesRequested\u201d. After 5-6 mins, at 18:35 the nodes finish provisioning and are considered \u201cRUNNING\u201d. We see an increase in the metric, \u201cTotalNodesRunning\u201d. Around the same time, we see \u201cYARNMemoryAvailablePercentage\u201d begin increasing back to 100%. For a full list of metrics and description of each, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/managed-scaling-metrics.html","title":"BP 4.1.2 Monitor Managed Scaling with Cloudwatch Metrics"},{"location":"features/managed_scaling/best_practices/#bp-413-consider-adjusting-yarn-decommissioning-timeouts-depending-on-your-workload","text":"There are two decommissioning timeouts that are important in managed scaling: yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED. spark.blacklist.decommissioning.timeout: This is the maximal time that Spark does not schedule new tasks on executors running on that node. Tasks already running are allowed to complete. When managed scaling triggers a scale down, YARN will put nodes it wants to decomission in a \u201cDECOMMISSIONING\u201d state. Spark will detect this and add these nodes to a \u201cblack list\u201d. In this state, Spark will not assign any new tasks to the node and once all tasks are completed, YARN will finish decommissioning the node. If the task runs longer than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs , the node is force-terminated and the task will be reassigned to another node. In certain scale down scenarios where you have long running tasks, many nodes can end up in this state where they are \u201cDECOMMISSIONING\u201d and \u201cblacklisted\u201d because of spark.blacklist.decommissioning.timeout. You may observe that new jobs run slower because it cannot assign tasks to all nodes in the cluster. To mitigate this, you can lower spark.blacklist.decommissioning.timeout to make the node available for other pending containers to continue task processing. This can improve job run times. However, please take the below into consideration: If a task is assigned to this node, and YARN transitions from DECOMMISSIONING into DECOMMISSIONED, the task will fail and will need to be reassigned to another node. Spark blacklist also protects from bad nodes in the cluster, e.g., faulty hardware leading to high task failure rate. Lowering the blacklist timeout can increase task failure rate since tasks will continue to be assigned to these nodes. Nodes can be transitioned from DECOMMISSIONING to RUNNING due to a scale up request. In this scenario, tasks will not fail and with a lower blacklist timeout, pending tasks can continuously be assigned to the node. With yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, consider increasing this from the default of 1hr to the length of your longest running task. This is to ensure that YARN does not force-terminate the node while the task is running, causing it to re-run on another node. The cost associated with rerunning the long running task is generally higher than keeping the node running to ensure it's completed. For more information, see: https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-3.html https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#spark-decommissioning","title":"BP 4.1.3 Consider adjusting YARN decommissioning timeouts depending on your workload"},{"location":"features/managed_scaling/best_practices/#bp-415-emr-managed-scaling-compared-to-custom-automatic-scaling","text":"The following link highlights the key differences between EMR managed scaling vs. custom automatic scaling: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-scale-on-demand.html In general, we recommend using EMR managed scaling since the metric evaluation is every 5-10 seconds. This means your EMR cluster will adjust quicker to the change in the required cluster resources. In addition, EMR managed scaling also supports instance fleets and the the scaling policy is simpler to configure because EMR managed scaling only requires min and max amounts for purchasing options (On-Demand/Spot) and node type (core/task). Custom automatic scaling should be considered if you want autoscaling outside of YARN applications or if you want full control over your scaling policies (e.g., evaluation period, cool down, number of nodes)","title":"BP 4.1.5 EMR Managed Scaling compared to Custom Automatic Scaling"},{"location":"features/managed_scaling/best_practices/#bp-416-configure-spark-history-server-shs-custom-executor-log-url-to-point-to-job-history-server-jhs-directly","text":"When you use SHS to access application container logs, YARN ResourceManager relies on the NodeManager that the jobs' Application Master (AM) ran on, to redirect to the JHS. The JHS is what hosts the container logs. A job's executor logs cannot be accessed if the AM ran on a node that\u2019s been decommissioned due to managed scaling or spot. A solution to this is pointing SHS to the JHS directly, instead of letting node manager redirect. Spark 3.0 introduced spark.history.custom.executor.log.url , which allows you to specify a custom Spark executor log url. You can configure spark.history.custom.executor.log.url as below to point to JHS directly: {{ HTTP_SCHEME }} <JHS_HOST>:<JHS_PORT>/jobhistory/logs/ {{ NM_HOST }} : {{ NM_PORT }} / {{ CONTAINER_ID }} / {{ CONTAINER_ID }} / {{ USER }} / {{ FILE_NAME }} ?start=-4096 Replace JHS_HOST and JHS_PORT with actual values. JHS_HOST is the EMR master node.","title":"BP 4.1.6 Configure Spark History Server (SHS) custom executor log URL to point to Job History Server (JHS) Directly"},{"location":"features/spot_usage/best_practices/","text":"4.2 - Spot Usage \u00b6 BP 4.2.1 When to use spot vs. on demand \u00b6 Spot is a great way to help reduce costs. However, there are certain scenarios where you should consider on demand because there's always a chance that an interruption can happen. The considerations are: Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Avoid spot if your workload has 0 fault tolerance or when recomputing tasks are expensive Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost. BP 4.2.1 Use Instancefleets when using Spot Instances \u00b6 Instancefleets provides clusters with Instance flexibility. Instead of relying on a single instance to reach your target capacity, you can specify up to 30 instances. This is a best practice when using Spot because EMR will automatically provision instances from the most-available Spot capacity pools when allocation strategy is enabled. Because your Spot Instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your Spot Instances are reclaimed. A good rule of thumb is to be flexible across at least 10 instance types for each workload. In addition, make sure that all Availability Zones are configured for use in your VPC and selected for your workload. An EMR cluster will only be provisioned in a single AZ but will look across all for the initial provisioning. BP 4.2.2 Ensure Application Masters only run on an On Demand Node \u00b6 When a job is submitted to EMR, the Application Master (AM) can run on any of the nodes*. The AM is is the main container requesting, launching and monitoring application specific resources. Each job launches a single AM and if the AM is assigned to a spot node, and that spot node is interrupted, your job will fail. Therefore, it's important to ensure the AM is as resilient as possible. Assuming you are running a mixed cluster of On demand and Spot, by placing AM's on On demand nodes, you'll ensure AM's do not fail due to a spot interruption. The following uses \"yarn.nodemanager.node-labels.provider.script.path\" to run a script that sets node label to the market type - On Demand or Spot. yarn-site is also updated so that application masters are only assigned to the \"on_demand\" label. Finally, the cluster is updated to include the new node label. This is a good option when you run a mix of On demand and Spot. You can enable this with the following steps: 1. Save getNodeLabels_bootstrap.sh and getNodeLabels.py in S3 and run getNodeLabels_bootstrap.sh as an EMR bootstrap action getNodeLabels_bootstrap.sh 1 2 3 #!/bin/bash aws s3 cp s3://<bucket>/getNodeLabels.py /home/hadoop chmod +x /home/hadoop/getNodeLabels.py This script will copy getNodeLabels.py onto each node which is used by YARN to set NODE_PARTITION getNodeLabels.py 1 2 3 4 5 6 7 8 #!/usr/bin/python3 import json k = '/mnt/var/lib/info/extraInstanceData.json' with open ( k ) as f : response = json . load ( f ) #print ((response['instanceRole'],response['marketType'])) if ( response [ 'instanceRole' ] in [ 'core' , 'task' ] and response [ 'marketType' ] == 'on_demand' ): print ( f \"NODE_PARTITION: { response [ 'marketType' ] . upper () } \" ) This script is run every time a node is provisioned and sets NODE_PARTITION to on_demand. 2. Set yarn-site classification to schedule AMs on ON_DEMAND nodes. [ { \"classification\":\"yarn-site\", \"Properties\":{ \"yarn.nodemanager.node-labels.provider\":\"script\", \"yarn.nodemanager.node-labels.provider.script.path\":\"/home/hadoop/getNodeLabels.py\", \"yarn.node-labels.enabled\":\"true\", \"yarn.node-labels.am.default-node-label-expression\":\"ON_DEMAND\", \"yarn.nodemanager.node-labels.provider.configured-node-partition\":\"ON_DEMAND,SPOT\" } }, { \"classification\":\"capacity-scheduler\", \"Properties\":{ \"yarn.scheduler.capacity.root.accessible-node-labels.ON_DEMAND.capacity\":\"100\", \"yarn.scheduler.capacity.root.default.accessible-node-labels.ON_DEMAND.capacity\":\"100\" } } ] 3. Add EMR Step 1 2 #!/bin/bash sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"SPOT(exclusive=false),ON_DEMAND(exclusive=false)\" Step should be the first step on the EMR cluster. This step adds the new node labels. Once your cluster is provisioned, AM's will only run on On Demand nodes. Other non AM containers will run on all nodes. * EMR 5.19 and later uses the node label feature to assign AMs on core nodes only. Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The application master processes can run on both core and task nodes by default. BP 4.2.3 Allow application masters (AM) to run on all nodes \u00b6 With EMR 5.x, AM only run on core nodes. Because Spot Instances are often used to run task nodes, it prevents applications from failing in case an AM is assigned to a spot node. As a result of this, in scenarios where applications are occupying the full core node capacity, AM's will be in a PENDING state since they can only run on core nodes. The application will have to wait for capacity to be available on the core nodes even if there's capacity on the task nodes. Allowing AM's to run on all nodes is a good option if you are not using Spot, or run a small number of core nodes and do not want your cluster to be limited by Core capacity. You can disable this behavior with the bootstrap action below: 1 2 3 4 5 #!/bin/bash echo \"backup original init.pp\" sudo cp cp /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp /tmp/ echo \"replacing node label check\" sudo sed -i '/add-to-cluster-node-labels.*/,+5d' /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The AM processes can run on both core and task nodes by default. You can enable the YARN node labels feature by configuring following properties: yarn.node-labels.enabled: true yarn.node-labels.am.default-node-label-expression: 'CORE' When you allow AM's to run on all nodes and are using managed scaling, consider increasing yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs so AM's are not automatically terminated after the 1hr timeout in the event of a scale down. See BP 4.1.3 for more details. BP 4.2.4 Reserve core nodes for only application masters (am) \u00b6 This is not necessarily related to Spot, but An alternative to BP 4.2.2 is to reserve core nodes for only application masters/spark drivers. This means tasks spawned from executors or AMs will only run on the task nodes. The approach keeps the \u201cCORE\u201d label for core nodes and specifies it as exclusive=true. This means that containers will only be allocated to CORE nodes when it matches the node partition during job submission. By default, EMR will set AM=Core and as long as users are not specifying node label = core, all containers will run on task. Add EMR step during EMR provisioning 1 2 3 4 #!/bin/bash #Change core label from exclusive=false to exclusive=true. sudo -u yarn yarn rmadmin -removeFromClusterNodeLabels \"CORE\" sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"CORE(exclusive=true)\" Applications can still be waiting for resources if the # of jobs you\u2019re submitting exceeds the available space on your core nodes. However, this is less likely to occur now that tasks cant be assigned to core. The other option to consider is allowing AM to run on all nodes but OD. I would not recommend having AM run on task. BP 4.2.5 Reduce spot interruptions by setting purchase Option to \"Use on-demand as max price\" \u00b6 By setting the spot purchase option to \"use on-demand as max price\", your spot nodes will only be interrupted when EC2 takes back spot capacity and not because of someone outbidding your spot price. BP 4.2.6 Reduce the impact of spot interruptions \u00b6 There's a few strategies to consider when using that spot will help you take advantage of spot pricing while still getting capacity: Mix on demad nodes Spot Use on demand for core nodes and spot for task Reduce provisioning timeout and switch to on demand - When using Instancefleets, EMR allows you to set a timeout duration for getting spot capacity. Once the duration is hit, you can choose to terminate the cluster or fall back to on demand. The default value is 60min but consider lowering this quickly fall back to on demand when spot is not available Checkpoint often - This allows you to retry from a certain part of your pipeline if you ever lose too many spot nodes BP 4.2.7 Adjust Spark task size to complete within 2 minutes \u00b6 Consider reducing spark task sizes to minimize the impact of a spot interruption. Smaller task sizes complete faster. These tasks will be less impacted by spot because the amount of time to recompute the failed task is less. There are a number of factors that impact the # of spark tasks and their run time such as spark.sql.files.maxPartitionBytes, spark.default.parallelism, # of objects in s3, or are the objects can be split. See Spark best practices section on how to adjust task run times. When using Spot, the optimal task run time is less than 2 minutes. This is because EC2 Spot instances receive a two-minute warning when these instances are about to be reclaimed by Amazon EC2 which means most tasks will be able to finish before the spot node is reclaimed. In addition, EMR does not assign new tasks to nodes that have received the 2 minute warning.","title":"4.2 - Spot Usage"},{"location":"features/spot_usage/best_practices/#42-spot-usage","text":"","title":"4.2 - Spot Usage"},{"location":"features/spot_usage/best_practices/#bp-421-when-to-use-spot-vs-on-demand","text":"Spot is a great way to help reduce costs. However, there are certain scenarios where you should consider on demand because there's always a chance that an interruption can happen. The considerations are: Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLA Use Spot for testing and development workloads or when testing testing new applications. Avoid spot if your workload requires predictable completion time or has service level agreement (SLA) requirements Avoid spot if your workload has 0 fault tolerance or when recomputing tasks are expensive Use instance fleet with allocation strategy while using Spot so that you can diversify across many different instances. Spot capacity pool is unpredictable so diversifying with as many instances that meets your requirements can help increase the likelihood of securing spot instances which in turn, reduces cost.","title":"BP 4.2.1 When to use spot vs. on demand"},{"location":"features/spot_usage/best_practices/#bp-421-use-instancefleets-when-using-spot-instances","text":"Instancefleets provides clusters with Instance flexibility. Instead of relying on a single instance to reach your target capacity, you can specify up to 30 instances. This is a best practice when using Spot because EMR will automatically provision instances from the most-available Spot capacity pools when allocation strategy is enabled. Because your Spot Instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your Spot Instances are reclaimed. A good rule of thumb is to be flexible across at least 10 instance types for each workload. In addition, make sure that all Availability Zones are configured for use in your VPC and selected for your workload. An EMR cluster will only be provisioned in a single AZ but will look across all for the initial provisioning.","title":"BP 4.2.1 Use Instancefleets when using Spot Instances"},{"location":"features/spot_usage/best_practices/#bp-422-ensure-application-masters-only-run-on-an-on-demand-node","text":"When a job is submitted to EMR, the Application Master (AM) can run on any of the nodes*. The AM is is the main container requesting, launching and monitoring application specific resources. Each job launches a single AM and if the AM is assigned to a spot node, and that spot node is interrupted, your job will fail. Therefore, it's important to ensure the AM is as resilient as possible. Assuming you are running a mixed cluster of On demand and Spot, by placing AM's on On demand nodes, you'll ensure AM's do not fail due to a spot interruption. The following uses \"yarn.nodemanager.node-labels.provider.script.path\" to run a script that sets node label to the market type - On Demand or Spot. yarn-site is also updated so that application masters are only assigned to the \"on_demand\" label. Finally, the cluster is updated to include the new node label. This is a good option when you run a mix of On demand and Spot. You can enable this with the following steps: 1. Save getNodeLabels_bootstrap.sh and getNodeLabels.py in S3 and run getNodeLabels_bootstrap.sh as an EMR bootstrap action getNodeLabels_bootstrap.sh 1 2 3 #!/bin/bash aws s3 cp s3://<bucket>/getNodeLabels.py /home/hadoop chmod +x /home/hadoop/getNodeLabels.py This script will copy getNodeLabels.py onto each node which is used by YARN to set NODE_PARTITION getNodeLabels.py 1 2 3 4 5 6 7 8 #!/usr/bin/python3 import json k = '/mnt/var/lib/info/extraInstanceData.json' with open ( k ) as f : response = json . load ( f ) #print ((response['instanceRole'],response['marketType'])) if ( response [ 'instanceRole' ] in [ 'core' , 'task' ] and response [ 'marketType' ] == 'on_demand' ): print ( f \"NODE_PARTITION: { response [ 'marketType' ] . upper () } \" ) This script is run every time a node is provisioned and sets NODE_PARTITION to on_demand. 2. Set yarn-site classification to schedule AMs on ON_DEMAND nodes. [ { \"classification\":\"yarn-site\", \"Properties\":{ \"yarn.nodemanager.node-labels.provider\":\"script\", \"yarn.nodemanager.node-labels.provider.script.path\":\"/home/hadoop/getNodeLabels.py\", \"yarn.node-labels.enabled\":\"true\", \"yarn.node-labels.am.default-node-label-expression\":\"ON_DEMAND\", \"yarn.nodemanager.node-labels.provider.configured-node-partition\":\"ON_DEMAND,SPOT\" } }, { \"classification\":\"capacity-scheduler\", \"Properties\":{ \"yarn.scheduler.capacity.root.accessible-node-labels.ON_DEMAND.capacity\":\"100\", \"yarn.scheduler.capacity.root.default.accessible-node-labels.ON_DEMAND.capacity\":\"100\" } } ] 3. Add EMR Step 1 2 #!/bin/bash sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"SPOT(exclusive=false),ON_DEMAND(exclusive=false)\" Step should be the first step on the EMR cluster. This step adds the new node labels. Once your cluster is provisioned, AM's will only run on On Demand nodes. Other non AM containers will run on all nodes. * EMR 5.19 and later uses the node label feature to assign AMs on core nodes only. Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The application master processes can run on both core and task nodes by default.","title":"BP 4.2.2 Ensure Application Masters only run on an On Demand Node"},{"location":"features/spot_usage/best_practices/#bp-423-allow-application-masters-am-to-run-on-all-nodes","text":"With EMR 5.x, AM only run on core nodes. Because Spot Instances are often used to run task nodes, it prevents applications from failing in case an AM is assigned to a spot node. As a result of this, in scenarios where applications are occupying the full core node capacity, AM's will be in a PENDING state since they can only run on core nodes. The application will have to wait for capacity to be available on the core nodes even if there's capacity on the task nodes. Allowing AM's to run on all nodes is a good option if you are not using Spot, or run a small number of core nodes and do not want your cluster to be limited by Core capacity. You can disable this behavior with the bootstrap action below: 1 2 3 4 5 #!/bin/bash echo \"backup original init.pp\" sudo cp cp /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp /tmp/ echo \"replacing node label check\" sudo sed -i '/add-to-cluster-node-labels.*/,+5d' /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The AM processes can run on both core and task nodes by default. You can enable the YARN node labels feature by configuring following properties: yarn.node-labels.enabled: true yarn.node-labels.am.default-node-label-expression: 'CORE' When you allow AM's to run on all nodes and are using managed scaling, consider increasing yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs so AM's are not automatically terminated after the 1hr timeout in the event of a scale down. See BP 4.1.3 for more details.","title":"BP 4.2.3 Allow application masters (AM) to run on all nodes"},{"location":"features/spot_usage/best_practices/#bp-424-reserve-core-nodes-for-only-application-masters-am","text":"This is not necessarily related to Spot, but An alternative to BP 4.2.2 is to reserve core nodes for only application masters/spark drivers. This means tasks spawned from executors or AMs will only run on the task nodes. The approach keeps the \u201cCORE\u201d label for core nodes and specifies it as exclusive=true. This means that containers will only be allocated to CORE nodes when it matches the node partition during job submission. By default, EMR will set AM=Core and as long as users are not specifying node label = core, all containers will run on task. Add EMR step during EMR provisioning 1 2 3 4 #!/bin/bash #Change core label from exclusive=false to exclusive=true. sudo -u yarn yarn rmadmin -removeFromClusterNodeLabels \"CORE\" sudo -u yarn yarn rmadmin -addToClusterNodeLabels \"CORE(exclusive=true)\" Applications can still be waiting for resources if the # of jobs you\u2019re submitting exceeds the available space on your core nodes. However, this is less likely to occur now that tasks cant be assigned to core. The other option to consider is allowing AM to run on all nodes but OD. I would not recommend having AM run on task.","title":"BP 4.2.4 Reserve core nodes for only application masters (am)"},{"location":"features/spot_usage/best_practices/#bp-425-reduce-spot-interruptions-by-setting-purchase-option-to-use-on-demand-as-max-price","text":"By setting the spot purchase option to \"use on-demand as max price\", your spot nodes will only be interrupted when EC2 takes back spot capacity and not because of someone outbidding your spot price.","title":"BP 4.2.5 Reduce spot interruptions by setting purchase Option to \"Use on-demand as max price\""},{"location":"features/spot_usage/best_practices/#bp-426-reduce-the-impact-of-spot-interruptions","text":"There's a few strategies to consider when using that spot will help you take advantage of spot pricing while still getting capacity: Mix on demad nodes Spot Use on demand for core nodes and spot for task Reduce provisioning timeout and switch to on demand - When using Instancefleets, EMR allows you to set a timeout duration for getting spot capacity. Once the duration is hit, you can choose to terminate the cluster or fall back to on demand. The default value is 60min but consider lowering this quickly fall back to on demand when spot is not available Checkpoint often - This allows you to retry from a certain part of your pipeline if you ever lose too many spot nodes","title":"BP 4.2.6 Reduce the impact of spot interruptions"},{"location":"features/spot_usage/best_practices/#bp-427-adjust-spark-task-size-to-complete-within-2-minutes","text":"Consider reducing spark task sizes to minimize the impact of a spot interruption. Smaller task sizes complete faster. These tasks will be less impacted by spot because the amount of time to recompute the failed task is less. There are a number of factors that impact the # of spark tasks and their run time such as spark.sql.files.maxPartitionBytes, spark.default.parallelism, # of objects in s3, or are the objects can be split. See Spark best practices section on how to adjust task run times. When using Spot, the optimal task run time is less than 2 minutes. This is because EC2 Spot instances receive a two-minute warning when these instances are about to be reclaimed by Amazon EC2 which means most tasks will be able to finish before the spot node is reclaimed. In addition, EMR does not assign new tasks to nodes that have received the 2 minute warning.","title":"BP 4.2.7 Adjust Spark task size to complete within 2 minutes"},{"location":"reliability/best_practices/","text":"2 - Reliability \u00b6 Best Practices (BP) for running reliable workloads on EMR. BP 2.1 Treat all clusters as transient resources \u00b6 Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html BP 2.2 Decouple storage and compute \u00b6 Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3. BP 2.3 Use the latest AMI and EMR version available \u00b6 In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html BP 2.4 Spread clusters across availability zones/subnets and time of provisioning \u00b6 Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html BP 2.5 Use on demand for core nodes and spot for task \u00b6 Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable. BP 2.6 Use instance fleet with allocation strategy \u00b6 The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy BP 2.7 With instance fleet, diversify with instances in the same family and across generations first \u00b6 When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes. BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet \u00b6 When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different. BP 2.9 If optimizing for availability, avoid exotic instance types \u00b6 Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability. BP 2.10 Handling S3 503 slow downs \u00b6 When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic BP 2.11 Audit and update EMR and EC2 limits to avoid throttling \u00b6 Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html BP 2.12 Set dfs.replication > 1 if using Spot for core nodes or for long running clusters \u00b6 dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes \u00b6 When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"Best Practices"},{"location":"reliability/best_practices/#2-reliability","text":"Best Practices (BP) for running reliable workloads on EMR.","title":"2 - Reliability"},{"location":"reliability/best_practices/#bp-21-treat-all-clusters-as-transient-resources","text":"Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it\u2019s recommended to recreate the cluster during some periodical interval. Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services. There are several benefits to this approach. It makes upgrading, patching, rotating AMI\u2019s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case. If you need to store state on cluster, ensure the state is backed up and synced. For more information on orchestrating transient EMR cluster, see: https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/ https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html Specifically for EMR application logging, consider using EMR\u2019s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated. For more information on off cluster monitoring options, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/ For more information on external catalog, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html","title":"BP 2.1 Treat all clusters as transient resources"},{"location":"reliability/best_practices/#bp-22-decouple-storage-and-compute","text":"Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS. By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket. From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data. HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3.","title":"BP 2.2 Decouple storage and compute"},{"location":"reliability/best_practices/#bp-23-use-the-latest-ami-and-emr-version-available","text":"In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI. The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html","title":"BP 2.3 Use the latest AMI and EMR version available"},{"location":"reliability/best_practices/#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning","text":"Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ. EMR helps you achieve this with instance fleets. Instead of specifying a single Amazon EC2 availability zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of availability zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, If Amazon EMR detects an AWS large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections. With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by picking through round robin or at random. If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html","title":"BP 2.4 Spread clusters across availability zones/subnets and time of provisioning"},{"location":"reliability/best_practices/#bp-25-use-on-demand-for-core-nodes-and-spot-for-task","text":"Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on spot and the spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and usable.","title":"BP 2.5 Use on demand for core nodes and spot for task"},{"location":"reliability/best_practices/#bp-26-use-instance-fleet-with-allocation-strategy","text":"The instance fleet configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster. You can have one instance fleet for each node group - master, core and task. Within the instance fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet. In an instance fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy","title":"BP 2.6 Use instance fleet with allocation strategy"},{"location":"reliability/best_practices/#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first","text":"When deciding which instances to include in your instance fleet, it is recommend to first diversify across the same family. For example, if you are using m5.4xlarge, you should first add m5.8xlarge and then m5.12xlarge. Instances within the same family are identical and your job should perform consistent across the different instances. Ensure your application container (spark executors, tez container) is not larger than the smallest instance in your fleet. Next, you should diversify across generations, for example, including m6.4xlarge and m4.8xlarge. Diversifying your instance fleet across families should be considered last e.g r5 and m5 due to difference in core to memory ratios resulting in potential underutilization depending on your application container sizes.","title":"BP 2.7 With instance fleet, diversify with instances in the same family and across generations first"},{"location":"reliability/best_practices/#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet","text":"When using instance fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet. For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different.","title":"BP 2.8 With instance fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet"},{"location":"reliability/best_practices/#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types","text":"Exotic instances are designed for specific use cases such as \u201czn\u201d, \u201cdn\u201c, and \u201cad\" as well as large instance types like 24xlarge. Exotic instance types have smaller EC2 capacity pools which increase the likelihood of Insufficient Capacity Errors and spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher guarantees of instance availability.","title":"BP 2.9 If optimizing for availability, avoid exotic instance types"},{"location":"reliability/best_practices/#bp-210-handling-s3-503-slow-downs","text":"When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 slow downs. 1) Use EMRFS retry strategies EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration. Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html 2) Increase fs.s3n.multipart.uploads.split.size Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) \u2013 you can start with something in the middle and see if there\u2019s any impact to performance (for example 1-2 gb) For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart 3) Combine or stagger out requests to S3 Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways: If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors. If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests. If possible, stagger jobs out across the day or hour. For example, If your jobs don\u2019t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ 4) Optimize your S3 Data layout Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3 For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests. s3://<bucket1>/dt=2021-11-01 s3://<bucket2>/product=1/dt=2021-11-01 s3://<bucket2>/product=2/dt=2021-11-01 It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3. For more information, see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic","title":"BP 2.10 Handling S3 503 slow downs"},{"location":"reliability/best_practices/#bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling","text":"Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits: 1) Limit on Resources - maximum number of clusters that can The maximum number of active clusters that can be run at the same time. The maximum number of active instances per instance group. 2) Limits on APIs Burst limit \u2013 This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit. Rate limit \u2013 This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit. To prevent throttling errors, we recommend: Reduce the frequency of the API calls. For example, if you\u2019re using the DescribeStep API and you don\u2019t need to know the status of the job right away, you can reduce the frequency of the call to 1min+ Stagger the intervals of the API calls so that they don't all run at the same time. Implement exponential back-off ( https://docs.aws.amazon.com/general/latest/gr/api-retries.html ) when making API calls. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html","title":"BP 2.11 Audit and update  EMR and EC2 limits to avoid throttling"},{"location":"reliability/best_practices/#bp-212-set-dfsreplication-1-if-using-spot-for-core-nodes-or-for-long-running-clusters","text":"dfs.replication is the number of copies of each block to store for durability in HDFS. if dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, you may not be able to perform certain EMR actions. e.g submit hive job if core tez library in HDFS is missing dfs.replication defaults are set based off of initial core count: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2. Few other considerations: Do not scale down below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3 Increasing dfs.replication will require additional EBS volume","title":"BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters"},{"location":"reliability/best_practices/#bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes","text":"When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk is marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node. If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated. The two most common reasons disk\u2019s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate. From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric \u201cHDFSUtilization\u201d. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance. The 90% is a default value which can be configured by \u201cyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\" in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this % but instead, right size your EBS volumes. For more information, see: https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/ https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html Calculating required HDFS utilization: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs","title":"BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes"},{"location":"reliability/introduction/","text":"Introduction \u00b6 EMR Reliability best practices discusses how to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues.","title":"Introduction"},{"location":"reliability/introduction/#introduction","text":"EMR Reliability best practices discusses how to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues.","title":"Introduction"},{"location":"security/best_practices/","text":"3 - Security \u00b6 Best Practices (BP) for running secure workloads on EMR. BP 3.1 Encrypt Data at rest and in transit \u00b6 Properly protecting your data at rest and in transit using encryption is a core component of our well-architected pillar of security. Amazon EMR security configurations make it easy for you to encrypt data both at rest and in transit. A security configuration is like a template for encryption and other security configurations that you can apply to any cluster when you launch it. For data at rest, EMR provides encryption options for reading and writing data in S3 via EMRFS. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) as the Default encryption mode when you enable encryption at rest. Optionally, you can specify different encryption methods for individual buckets using Per bucket encryption overrides. EMR also provides the option to encrypt local disk storage. These are EC2 instance store volumes and the attached Amazon Elastic Block Store (EBS) storage that are provisioned with your cluster. You have the options of using linux Unified Key Setup (LUKS) encryption or using AWS KMS as your key provider. For data in transit, EMR security configurations allows you you to either manually create PEM certificates, zip them in a file, and reference from Amazon S3 or implement a certificate custom provider in Java and specify the S3 path to the JAR. In either case, EMR automatically downloads artifacts to each node in the cluster and later uses them to implement the open-source, in-transit encryption features. For more information on how these certificates are used with different big data technologies, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post Secure Amazon EMR with Encryption, see: https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption BP 3.2 Restrict network access to your EMR cluster and keep EMR block public access feature enabled \u00b6 Inbound and outbound network access to your EMR cluster is controlled by security groups. It is recommended to apply the principle of least privilege to your security groups. This is so that your cluster is locked down to only the applications or individuals who need access from the expected source IPs. It\u2019s also recommended to not allow SSH access to the hadoop user. The hadoop user has elevated sudo access and access to this user is typically not requred. EMR provides a number of ways for users to interact with clusters remotely. For job submission, users can use EMR Steps API or an orchestration service like Managed airflow or AWS Step functions. For Ad hoc or notebook use cases, you can use EMR studio or allow users to connect to the specific application portsports e.g Hiveserver2 JDBC, Livy or Notebook UI\u2019s The block public access feature prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception - port 22 is an exception by default. This feature is enabled by default for each AWS Region in your AWS account and is not recommended to be turned off. Use Persistent Application UI's to remove the need to open firewall to get access to debugging UI BP 3.3 Provision clusters in a private subnet \u00b6 It is recommended to provision your EMR clusters in Private Subnets. Private subnets allow you to limit access to deployed components, and to control security and routing of the system. With a private subnet, you can enable communication with your own network over a VPN tunnel or AWS direct connect. This would allow you to access your EMR clusters from your network, without exposure to the internet. For access to other AWS services from your EMR Cluster e.g S3, VPC endpoints can be used. For more information on configuring EMR clusters in private subnets or VPC endpoints, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html BP 3.4 Configure EC2 instance metadata service (IMDS) v2 \u00b6 In AWS, Instance Metadata Service (IMDS) provides \u201cdata about your instance that you can use to configure or manage the running instance. Every instance has access to its own MDS using any HTTP client request, such as, curl command from the instance to http://169.254.169.254/latest/meta-data. IMDSv1 is fully secure and AWS will continue to support it. But IMDSv2 adds new \u201cbelt and suspenders\u201d protections for four types of vulnerabilities that could be used to try to access the IMDS. For more see: https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/ From EMR 5.32 and 6.2 onward, Amazon EMR components use IMDSv2 for all IMDS calls. For IMDS calls in your application code, you can use both IMDSv1 and IMDSv2. It is recommended to turn off IMDSv1 and only allow IMDSv2 for added security. This can be configured in EMR Security Configurations. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html#emr-security-configuration-imdsv2 BP 3.5 Create a separate IAM role for each cluster or use case \u00b6 EMR uses an IAM service roles to perform actions on your behalf to provision and manage clusters. It is recommended to create a separate IAM role for each use case and workload. This allows you to segregate access control between clusters. If you have multiple clusters, each cluster can only the services and data defined within the IAM policy. BP 3.6 Use scoped down IAM policies for authorization such as AmazonEMRFullAccessPolicy_v2 \u00b6 EMR provides managed IAM policies to grant specific access privileges to users. Managed policies offer the benefit of updating automatically if permission requirements change. If you use inline policies, service changes may occur that cause permission errors to appear. It is recommended to use new managed policies (v2 policies) which have been scoped-down to align with AWS best practices. The v2 managed policies restrict access using tags. They allow only specified Amazon EMR actions and require cluster resources that are tagged with an EMR-specific key. For more details and usage, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-policy-fullaccess-v2.html BP 3.7 Audit user activity with AWS CloudTrail \u00b6 AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service, is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events. The calls captured include calls from the Amazon EMR console and code calls to the Amazon EMR API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR. You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn\u2019t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR. Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements. BP 3.8 Upgrade your EMR Releases frequently or use a Custom AMI to get the latest OS and application software patches \u00b6 Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. If you must use an earlier release version of Amazon EMR for compatibility, we recommend that you use the latest release in a series. For example, if you must use the 5.12 series, use 5.12.2 instead of 5.12.0 or 5.12.1. If a new release becomes available in a series, consider migrating your applications to the new release.","title":"Best Practices"},{"location":"security/best_practices/#3-security","text":"Best Practices (BP) for running secure workloads on EMR.","title":"3 - Security"},{"location":"security/best_practices/#bp-31-encrypt-data-at-rest-and-in-transit","text":"Properly protecting your data at rest and in transit using encryption is a core component of our well-architected pillar of security. Amazon EMR security configurations make it easy for you to encrypt data both at rest and in transit. A security configuration is like a template for encryption and other security configurations that you can apply to any cluster when you launch it. For data at rest, EMR provides encryption options for reading and writing data in S3 via EMRFS. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) as the Default encryption mode when you enable encryption at rest. Optionally, you can specify different encryption methods for individual buckets using Per bucket encryption overrides. EMR also provides the option to encrypt local disk storage. These are EC2 instance store volumes and the attached Amazon Elastic Block Store (EBS) storage that are provisioned with your cluster. You have the options of using linux Unified Key Setup (LUKS) encryption or using AWS KMS as your key provider. For data in transit, EMR security configurations allows you you to either manually create PEM certificates, zip them in a file, and reference from Amazon S3 or implement a certificate custom provider in Java and specify the S3 path to the JAR. In either case, EMR automatically downloads artifacts to each node in the cluster and later uses them to implement the open-source, in-transit encryption features. For more information on how these certificates are used with different big data technologies, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post Secure Amazon EMR with Encryption, see: https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption","title":"BP 3.1 Encrypt Data at rest and in transit"},{"location":"security/best_practices/#bp-32-restrict-network-access-to-your-emr-cluster-and-keep-emr-block-public-access-feature-enabled","text":"Inbound and outbound network access to your EMR cluster is controlled by security groups. It is recommended to apply the principle of least privilege to your security groups. This is so that your cluster is locked down to only the applications or individuals who need access from the expected source IPs. It\u2019s also recommended to not allow SSH access to the hadoop user. The hadoop user has elevated sudo access and access to this user is typically not requred. EMR provides a number of ways for users to interact with clusters remotely. For job submission, users can use EMR Steps API or an orchestration service like Managed airflow or AWS Step functions. For Ad hoc or notebook use cases, you can use EMR studio or allow users to connect to the specific application portsports e.g Hiveserver2 JDBC, Livy or Notebook UI\u2019s The block public access feature prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception - port 22 is an exception by default. This feature is enabled by default for each AWS Region in your AWS account and is not recommended to be turned off. Use Persistent Application UI's to remove the need to open firewall to get access to debugging UI","title":"BP 3.2 Restrict network access to your EMR cluster and keep EMR block public access feature enabled"},{"location":"security/best_practices/#bp-33-provision-clusters-in-a-private-subnet","text":"It is recommended to provision your EMR clusters in Private Subnets. Private subnets allow you to limit access to deployed components, and to control security and routing of the system. With a private subnet, you can enable communication with your own network over a VPN tunnel or AWS direct connect. This would allow you to access your EMR clusters from your network, without exposure to the internet. For access to other AWS services from your EMR Cluster e.g S3, VPC endpoints can be used. For more information on configuring EMR clusters in private subnets or VPC endpoints, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html","title":"BP 3.3 Provision clusters in a private subnet"},{"location":"security/best_practices/#bp-34-configure-ec2-instance-metadata-service-imds-v2","text":"In AWS, Instance Metadata Service (IMDS) provides \u201cdata about your instance that you can use to configure or manage the running instance. Every instance has access to its own MDS using any HTTP client request, such as, curl command from the instance to http://169.254.169.254/latest/meta-data. IMDSv1 is fully secure and AWS will continue to support it. But IMDSv2 adds new \u201cbelt and suspenders\u201d protections for four types of vulnerabilities that could be used to try to access the IMDS. For more see: https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/ From EMR 5.32 and 6.2 onward, Amazon EMR components use IMDSv2 for all IMDS calls. For IMDS calls in your application code, you can use both IMDSv1 and IMDSv2. It is recommended to turn off IMDSv1 and only allow IMDSv2 for added security. This can be configured in EMR Security Configurations. For more information, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html#emr-security-configuration-imdsv2","title":"BP 3.4 Configure EC2 instance metadata service (IMDS) v2"},{"location":"security/best_practices/#bp-35-create-a-separate-iam-role-for-each-cluster-or-use-case","text":"EMR uses an IAM service roles to perform actions on your behalf to provision and manage clusters. It is recommended to create a separate IAM role for each use case and workload. This allows you to segregate access control between clusters. If you have multiple clusters, each cluster can only the services and data defined within the IAM policy.","title":"BP 3.5 Create a separate IAM role for each cluster or use case"},{"location":"security/best_practices/#bp-36-use-scoped-down-iam-policies-for-authorization-such-as-amazonemrfullaccesspolicy_v2","text":"EMR provides managed IAM policies to grant specific access privileges to users. Managed policies offer the benefit of updating automatically if permission requirements change. If you use inline policies, service changes may occur that cause permission errors to appear. It is recommended to use new managed policies (v2 policies) which have been scoped-down to align with AWS best practices. The v2 managed policies restrict access using tags. They allow only specified Amazon EMR actions and require cluster resources that are tagged with an EMR-specific key. For more details and usage, see: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-policy-fullaccess-v2.html","title":"BP 3.6 Use scoped down IAM policies for authorization such as AmazonEMRFullAccessPolicy_v2"},{"location":"security/best_practices/#bp-37-audit-user-activity-with-aws-cloudtrail","text":"AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service, is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events. The calls captured include calls from the Amazon EMR console and code calls to the Amazon EMR API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR. You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn\u2019t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR. Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements.","title":"BP 3.7 Audit user activity with AWS CloudTrail"},{"location":"security/best_practices/#bp-38-upgrade-your-emr-releases-frequently-or-use-a-custom-ami-to-get-the-latest-os-and-application-software-patches","text":"Each Amazon EMR release version is \"locked\" to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate. If you must use an earlier release version of Amazon EMR for compatibility, we recommend that you use the latest release in a series. For example, if you must use the 5.12 series, use 5.12.2 instead of 5.12.0 or 5.12.1. If a new release becomes available in a series, consider migrating your applications to the new release.","title":"BP 3.8 Upgrade your EMR Releases frequently or use a Custom AMI to get the latest OS and application software patches"},{"location":"security/introduction/","text":"Introduction \u00b6 EMR Security best practices discusses how to take advantage of AWS and EMR features to protect data, systems, and assets in a way that can improve your security posture. It's recommended to first read our Well Architected paper on security to understand the risks we mitigate and how we think about security. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html","title":"Introduction"},{"location":"security/introduction/#introduction","text":"EMR Security best practices discusses how to take advantage of AWS and EMR features to protect data, systems, and assets in a way that can improve your security posture. It's recommended to first read our Well Architected paper on security to understand the risks we mitigate and how we think about security. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html","title":"Introduction"}]}