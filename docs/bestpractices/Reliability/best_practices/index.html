<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bestpractices/Reliability/best_practices" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">2 - Reliability | AWS Open Data Analytics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" name="twitter:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" property="og:url" content="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="2 - Reliability | AWS Open Data Analytics"><meta data-rh="true" name="description" content="Best Practices (BP) for running reliable workloads on EMR."><meta data-rh="true" property="og:description" content="Best Practices (BP) for running reliable workloads on EMR."><link data-rh="true" rel="icon" href="/aws-emr-best-practices/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices" hreflang="en"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices" hreflang="x-default"><link rel="stylesheet" href="/aws-emr-best-practices/assets/css/styles.7a6c5961.css">
<script src="/aws-emr-best-practices/assets/js/runtime~main.199ae6fb.js" defer="defer"></script>
<script src="/aws-emr-best-practices/assets/js/main.3c3486c3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/aws-emr-best-practices/"><div class="navbar__logo"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AWS Open Data Analytics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/aws-emr-best-practices/docs/bestpractices/">Best Practices</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/benchmarks/introduction">Benchmarks</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/utilities/">Utilities</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/migration/introduction">Migration</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/aws-emr-best-practices/docs/bestpractices/">EMR Best Practices Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/Introduction">Cost Optimizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction">Reliability</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices">Best Practices</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Security/introduction">Security</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction">Applications</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction">Architecture</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/aws-emr-best-practices/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Reliability</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Best Practices</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>2 - Reliability</h1>
<p>Best Practices (BP) for running reliable workloads on EMR.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-21-treat-all-clusters-as-transient-resources">BP 2.1 Treat all clusters as transient resources<a href="#bp-21-treat-all-clusters-as-transient-resources" class="hash-link" aria-label="Direct link to BP 2.1 Treat all clusters as transient resources" title="Direct link to BP 2.1 Treat all clusters as transient resources">​</a></h2>
<p>Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it’s recommended to recreate the cluster during some periodical interval.</p>
<p>Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator&#x27;s (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services.</p>
<p>There are several benefits to this approach. It makes upgrading, patching, rotating AMI’s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case.</p>
<p>If you need to store state on cluster, ensure the state is backed up and synced.</p>
<p><img loading="lazy" alt="BP - 1" src="/aws-emr-best-practices/assets/images/bp-1-23ceac6eef4c787c65f04039252f3d8a.png" width="837" height="521" class="img_ev3q"></p>
<p>For more information on orchestrating transient EMR cluster, see:</p>
<p>(<a href="https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/</a>)</p>
<p>(<a href="https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/</a>)</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html</a>)</p>
<p>Specifically for EMR application logging, consider using EMR’s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated.</p>
<p>For more information on off cluster monitoring options, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html</a>)</p>
<p>(<a href="https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/</a>)</p>
<p>For more information on external catalog, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-22-decouple-storage-and-compute">BP 2.2 Decouple storage and compute<a href="#bp-22-decouple-storage-and-compute" class="hash-link" aria-label="Direct link to BP 2.2 Decouple storage and compute" title="Direct link to BP 2.2 Decouple storage and compute">​</a></h2>
<p>Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, <code>s3://DOC-EXAMPLE-BUCKET1/path</code> references an Amazon S3 bucket using EMRFS.</p>
<p>By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket.</p>
<p>From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data.</p>
<p>HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3.</p>
<p><img loading="lazy" alt="BP - 2" src="/aws-emr-best-practices/assets/images/bp-2-dc4f3c872824b8ff33aaabf6b2d01bfb.png" width="943" height="494" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-23-use-the-latest-ami-and-emr-version-available">BP 2.3 Use the latest AMI and EMR version available<a href="#bp-23-use-the-latest-ami-and-emr-version-available" class="hash-link" aria-label="Direct link to BP 2.3 Use the latest AMI and EMR version available" title="Direct link to BP 2.3 Use the latest AMI and EMR version available">​</a></h2>
<p>In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI.</p>
<p>The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is &quot;locked&quot; to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate.</p>
<p>When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released.</p>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning">BP 2.4 Spread clusters across Availability Zones/subnets and time of provisioning<a href="#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning" class="hash-link" aria-label="Direct link to BP 2.4 Spread clusters across Availability Zones/subnets and time of provisioning" title="Direct link to BP 2.4 Spread clusters across Availability Zones/subnets and time of provisioning">​</a></h2>
<p>Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ.</p>
<p>EMR helps you achieve this with Instance Fleets. Instead of specifying a single Amazon EC2 Availability Zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of Availability Zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, if Amazon EMR detects a large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections.</p>
<p>With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by selecting AZ&#x27;s through a round robin or random strategy.</p>
<p><img loading="lazy" alt="BP - 5" src="/aws-emr-best-practices/assets/images/bp-5-926a8d1f80e60371968b6902fb8fd084.png" width="451" height="233" class="img_ev3q"></p>
<p>If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors.</p>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-25-use-on-demand-for-core-nodes-and-spot-for-task">BP 2.5 Use on demand for core nodes and spot for task<a href="#bp-25-use-on-demand-for-core-nodes-and-spot-for-task" class="hash-link" aria-label="Direct link to BP 2.5 Use on demand for core nodes and spot for task" title="Direct link to BP 2.5 Use on demand for core nodes and spot for task">​</a></h2>
<p>Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on Spot Instances and the Spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and unusable.</p>
<p><img loading="lazy" alt="BP - 3" src="/aws-emr-best-practices/assets/images/bp-3-05929e192b165df8d9ea67da182f9f6a.png" width="436" height="317" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-26-use-instance-fleet-with-an-allocation-strategy">BP 2.6 Use Instance Fleet with an allocation strategy<a href="#bp-26-use-instance-fleet-with-an-allocation-strategy" class="hash-link" aria-label="Direct link to BP 2.6 Use Instance Fleet with an allocation strategy" title="Direct link to BP 2.6 Use Instance Fleet with an allocation strategy">​</a></h2>
<p>The Instance Fleets configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster.</p>
<p>You can have one Instance Fleet for each node group - master, core and task. Within the Instance Fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet.</p>
<p><img loading="lazy" alt="BP - 6" src="/aws-emr-best-practices/assets/images/bp-6-4936d72d1984690927eceb4015e77cd3.png" width="490" height="171" class="img_ev3q"></p>
<p>In an Instance Fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each Fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing.</p>
<p>It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors.</p>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first">BP 2.7 With Instance Fleet, diversify with instances in the same family and across generations first<a href="#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first" class="hash-link" aria-label="Direct link to BP 2.7 With Instance Fleet, diversify with instances in the same family and across generations first" title="Direct link to BP 2.7 With Instance Fleet, diversify with instances in the same family and across generations first">​</a></h2>
<p>Best practices for instance and availablity zone flexibility can be found, here:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-flexibility.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-flexibility.html</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet">BP 2.8 With Instance Fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet<a href="#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet" class="hash-link" aria-label="Direct link to BP 2.8 With Instance Fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet" title="Direct link to BP 2.8 With Instance Fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet">​</a></h2>
<p>When using Instance Fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet.</p>
<p>For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-29-if-optimizing-for-availability-avoid-exotic-instance-types">BP 2.9 If optimizing for availability, avoid exotic instance types<a href="#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types" class="hash-link" aria-label="Direct link to BP 2.9 If optimizing for availability, avoid exotic instance types" title="Direct link to BP 2.9 If optimizing for availability, avoid exotic instance types">​</a></h2>
<p>Exotic instances are designed for specific use cases, and includes instance types whose name ends with “zn”, “dn“, and “ad&quot;, as well as large instance types like 24xlarge. Exotic instance type capacity pools are generally smaller, which increases the likelihood of Insufficient Capacity Errors and Spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher instance availability.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-210-handling-s3-503-slow-downs">BP 2.10 Handling S3 503 slow downs<a href="#bp-210-handling-s3-503-slow-downs" class="hash-link" aria-label="Direct link to BP 2.10 Handling S3 503 slow downs" title="Direct link to BP 2.10 Handling S3 503 slow downs">​</a></h2>
<p>When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 responses:</p>
<ol>
<li>Use EMRFS retry strategies</li>
</ol>
<p>EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your <code>emrfs-site</code> configuration.</p>
<ul>
<li>
<p>Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting <code>fs.s3.maxRetries</code> in <code>emrfs-site.xml</code>)</p>
</li>
<li>
<p>Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later.</p>
</li>
</ul>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html</a>)</p>
<ol start="2">
<li>Increase fs.s3n.multipart.uploads.split.size</li>
</ol>
<ul>
<li>Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) – you can start with something in the middle and see if there’s any impact to performance (for example 1-2 gb)</li>
</ul>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart</a>)</p>
<ol start="3">
<li>Combine or stagger out requests to S3</li>
</ol>
<p>Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways:</p>
<ul>
<li>If the error happens during write, reduce the parallelism of the jobs. For example, use Spark <code>.coalesce()</code> or <code>.repartition()</code> operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors.</li>
<li>If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests.</li>
<li>If possible, stagger jobs out across the day or hour. For example, If your jobs don’t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3.</li>
</ul>
<p>For more information, see:</p>
<p>(<a href="https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/</a>)</p>
<ol start="4">
<li>Optimize your S3 Data layout</li>
</ol>
<p>Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3</p>
<p>For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">s3://&lt;bucket1&gt;/dt=2021-11-01</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">s3://&lt;bucket2&gt;/product=1/dt=2021-11-01</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">s3://&lt;bucket2&gt;/product=2/dt=2021-11-01</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3.</p>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling">BP 2.11 Audit and update EMR and EC2 limits to avoid throttling<a href="#bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling" class="hash-link" aria-label="Direct link to BP 2.11 Audit and update EMR and EC2 limits to avoid throttling" title="Direct link to BP 2.11 Audit and update EMR and EC2 limits to avoid throttling">​</a></h2>
<p>Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits:</p>
<ol>
<li>Limit on Resources - maximum number of clusters that can</li>
</ol>
<ul>
<li>The maximum number of active clusters that can be run at the same time.</li>
<li>The maximum number of active instances per instance group.</li>
</ul>
<ol start="2">
<li>Limits on APIs</li>
</ol>
<ul>
<li>Burst limit – This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit.</li>
<li>Rate limit – This is the replenishment rate of the API&#x27;s burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds <code>(0.5 calls/second X 2 seconds = 1 call)</code> to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit.</li>
</ul>
<p>To prevent throttling errors, we recommend:</p>
<ul>
<li>Reduce the frequency of the API calls. For example, if you’re using the DescribeStep API and you don’t need to know the status of the job right away, you can reduce the frequency of the call to 1min+</li>
<li>Stagger the intervals of the API calls so that they don&#x27;t all run at the same time.</li>
<li>Implement exponential back-off (<a href="https://docs.aws.amazon.com/general/latest/gr/api-retries.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/general/latest/gr/api-retries.html</a>) when making API calls.</li>
</ul>
<p>For more information, see:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html</a>)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-212-set-dfsreplication--1-if-using-spot-for-core-nodes-or-for-long-running-clusters">BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters<a href="#bp-212-set-dfsreplication--1-if-using-spot-for-core-nodes-or-for-long-running-clusters" class="hash-link" aria-label="Direct link to BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters" title="Direct link to BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters">​</a></h2>
<p><code>dfs.replication</code> is the number of copies of each block to store for durability in HDFS. If <code>dfs.replication</code> is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, operating software may be affected and you may not be able to perform certain EMR actions - for example to submit a Hive job if the Tez library in HDFS is missing.</p>
<p><code>dfs.replication</code> defaults are set based off of initial core count:</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html</a>)</p>
<p>To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set <code>dfs.replication</code> parameter to 2.</p>
<p>Few other considerations:</p>
<ul>
<li>Do not scale your node count below <code>dfs.replication</code>. For example if <code>dfs.replication=3</code>, keep your core node minimum to 3</li>
<li>Increasing <code>dfs.replication</code> will require additional EBS storage</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes">BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes<a href="#bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes" class="hash-link" aria-label="Direct link to BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes" title="Direct link to BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes">​</a></h2>
<p>When disk usage on a core or task node disk (for example, <code>/mnt</code> or <code>/mnt1</code>) exceeds 90%, the disk will be marked as unhealthy. If fewer than 25% of a node&#x27;s disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node.</p>
<p>If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated.</p>
<p>The two most common reasons disk’s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate.</p>
<p>From a monitoring and alerting perspective, there are a few options.  You can monitor and alert on HDFS utilization using the Cloudwatch metric <code>HDFSUtilization</code>. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance.</p>
<p>The 90% is a default value which can be configured by <code>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</code> in <code>yarn-site.xml</code>. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this %, but instead right size your EBS volumes.</p>
<p>For more information, see:</p>
<p>(<a href="https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/</a>)</p>
<p>(<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html</a>)</p>
<p>Calculating required HDFS utilization: (<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs</a>)</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bestpractices/2 - Reliability/best_practices.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/aws-emr-best-practices/docs/bestpractices/Security/introduction"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#bp-21-treat-all-clusters-as-transient-resources" class="table-of-contents__link toc-highlight">BP 2.1 Treat all clusters as transient resources</a></li><li><a href="#bp-22-decouple-storage-and-compute" class="table-of-contents__link toc-highlight">BP 2.2 Decouple storage and compute</a></li><li><a href="#bp-23-use-the-latest-ami-and-emr-version-available" class="table-of-contents__link toc-highlight">BP 2.3 Use the latest AMI and EMR version available</a></li><li><a href="#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning" class="table-of-contents__link toc-highlight">BP 2.4 Spread clusters across Availability Zones/subnets and time of provisioning</a></li><li><a href="#bp-25-use-on-demand-for-core-nodes-and-spot-for-task" class="table-of-contents__link toc-highlight">BP 2.5 Use on demand for core nodes and spot for task</a></li><li><a href="#bp-26-use-instance-fleet-with-an-allocation-strategy" class="table-of-contents__link toc-highlight">BP 2.6 Use Instance Fleet with an allocation strategy</a></li><li><a href="#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first" class="table-of-contents__link toc-highlight">BP 2.7 With Instance Fleet, diversify with instances in the same family and across generations first</a></li><li><a href="#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet" class="table-of-contents__link toc-highlight">BP 2.8 With Instance Fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet</a></li><li><a href="#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types" class="table-of-contents__link toc-highlight">BP 2.9 If optimizing for availability, avoid exotic instance types</a></li><li><a href="#bp-210-handling-s3-503-slow-downs" class="table-of-contents__link toc-highlight">BP 2.10 Handling S3 503 slow downs</a></li><li><a href="#bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling" class="table-of-contents__link toc-highlight">BP 2.11 Audit and update EMR and EC2 limits to avoid throttling</a></li><li><a href="#bp-212-set-dfsreplication--1-if-using-spot-for-core-nodes-or-for-long-running-clusters" class="table-of-contents__link toc-highlight">BP 2.12 Set dfs.replication &gt; 1 if using Spot for core nodes or for long running clusters</a></li><li><a href="#bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes" class="table-of-contents__link toc-highlight">BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws/aws-emr-best-practices/tree/main" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
</body>
</html>