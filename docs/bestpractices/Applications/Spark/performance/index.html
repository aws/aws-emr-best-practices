<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bestpractices/Applications/Spark/performance" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">Performance | AWS Open Data Analytics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" name="twitter:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" property="og:url" content="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/performance"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Performance | AWS Open Data Analytics"><meta data-rh="true" name="description" content="Use right file formats and compression type"><meta data-rh="true" property="og:description" content="Use right file formats and compression type"><link data-rh="true" rel="canonical" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/performance"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/performance" hreflang="en"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/performance" hreflang="x-default"><link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-MF59LKNSDN","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script><link rel="stylesheet" href="/aws-emr-best-practices/assets/css/styles.28ef5182.css">
<script src="/aws-emr-best-practices/assets/js/runtime~main.e82b2229.js" defer="defer"></script>
<script src="/aws-emr-best-practices/assets/js/main.0d4d448d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/aws-emr-best-practices/"><b class="navbar__title text--truncate">AWS Open Data Analytics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/aws-emr-best-practices/docs/bestpractices/introduction">Best Practices</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/benchmarks/introduction">Benchmarks</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/migration/introduction">Migration</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/utilities/introduction">Utilities</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/aws-emr-best-practices/docs/bestpractices/introduction">EMR Best Practices</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/aws-emr-best-practices/docs/bestpractices/Applications/HBase/introduction">Applications</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/HBase/introduction">HBase</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hadoop/introduction">Hadoop</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hive/introduction">Hive</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Spark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices">Best Practices</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/data_quality">Data Quality</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/data_skew">Data Skew</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/joins">Join Types</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/observability">Observability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/performance">Performance</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/thrift">Thrift Server</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting">Troubleshooting</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/Introduction">Cost Optimizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Features/EMRFS/aimd">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Observability/intro">Observability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction">Reliability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Security/introduction">Security</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Troubleshooting/Troubleshooting EMR">Troubleshooting</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/aws-emr-best-practices/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Applications</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Spark</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Performance</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Performance</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-right-file-formats-and-compression-type">Use right file formats and compression type<a href="#use-right-file-formats-and-compression-type" class="hash-link" aria-label="Direct link to Use right file formats and compression type" title="Direct link to Use right file formats and compression type">​</a></h2>
<p>Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support.</p>
<p>When writing Parquet files to S3, EMR Spark will use <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3-optimized-committer.html" target="_blank" rel="noopener noreferrer">EMRFSOutputCommitter</a> which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc.</p>
<p>Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors.</p>
<p><img decoding="async" loading="lazy" alt="BP - 6" src="/aws-emr-best-practices/assets/images/spark-bp-6-b9a7158171cdee924ec4f8676f11c0e5.png" width="590" height="266" class="img_ev3q"></p>
<p>Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults.</p>
<p><img decoding="async" loading="lazy" alt="BP - 7" src="/aws-emr-best-practices/assets/images/spark-bp-7-89937807f0d272059b3daa865ad322da.png" width="507" height="282" class="img_ev3q"></p>
<p>You can also apply columnar encryption on Parquet files using KMS:</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.encryption.kms.client.class&quot; ,&quot;org.apache.parquet.crypto.keytools.mocks.InMemoryKMS&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.encryption.key.list&quot; ,&quot;keyA:AAECAwQFBgcICQoLDA0ODw ,  keyB:AAECAAECAAECAAECAAECAA&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Activate Parquet encryption, driven by Hadoop properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.crypto.factory.class&quot; ,&quot;org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Write encrypted dataframe files.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Column &quot;square&quot; will be protected with master key &quot;keyA&quot;.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Parquet file footers will be protected with master key &quot;keyB&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">squaresDF.write.option(&quot;parquet.encryption.column.keys&quot; , &quot;keyA:square&quot;).option(&quot;parquet.encryption.footer.key&quot; , &quot;keyB&quot;).parquet(&quot;/path/to/table.parquet.encrypted&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Read encrypted dataframe files</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df2 = spark.read.parquet(&quot;/path/to/table.parquet.encrypted&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-partitioning">Data Partitioning<a href="#data-partitioning" class="hash-link" aria-label="Direct link to Data Partitioning" title="Direct link to Data Partitioning">​</a></h2>
<p>Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans.</p>
<p>You can choose one or more partition fields from your dataset or table columns based on:</p>
<ul>
<li>Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field.</li>
<li>Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions).</li>
<li>Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields.</li>
<li>File sizes per partition. It is recommended that your individual file sizes within each partition are &gt;=128 MB.</li>
</ul>
<p>The number of shuffle partitions will determine the number of output files per table partition.</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df.repartition(400).write.partitionBy(&quot;datecol&quot;).parquet(&quot;s3://bucket/output/&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property <em><code>spark.sql.shuffle.partitions</code></em>. You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property <em><code>spark.sql.files.maxPartitionBytes</code></em>.</p>
<p>Partitioning ensures that <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic" target="_blank" rel="noopener noreferrer">dynamic partition pruning</a> takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark.</p>
<p>For example, following query will push partition filters for better performance. <code>l_shipdate</code> and <code>l_shipmode</code> are partition fields of the table &quot;testdb.lineitem_shipmodesuppkey_part&quot;.</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.sql(&quot;select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate=&#x27;1993-12-03&#x27; and l_shipmode=&#x27;SHIP&#x27;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.queryExecution.toString</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Printing the query execution plan where we can see pushed filters for the two partition fields in where clause:</p>
<div class="language-c codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token operator" style="color:#393A34">==</span><span class="token plain"> Physical Plan </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AdaptiveSparkPlan isFinalPlan</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> Final Plan </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">HashAggregate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">keys</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> functions</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> output</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">#</span><span class="token number" style="color:#36acaa">320</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> ShuffleQueryStage </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> Exchange SinglePartition</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ENSURE_REQUIREMENTS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">#</span><span class="token number" style="color:#36acaa">198</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">HashAggregate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">keys</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> functions</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">partial_count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> output</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">count#</span><span class="token number" style="color:#36acaa">318L</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> Project</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">               </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> ColumnarToRow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> FileScan orc testdb</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lineitem_shipmodesuppkey_part</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">l_shipdate#</span><span class="token number" style="color:#36acaa">313</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">l_shipmode#</span><span class="token number" style="color:#36acaa">314</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> Batched</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> DataFilters</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Format</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> ORC</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Location</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> InMemoryFileIndex</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">s3</span><span class="token operator" style="color:#393A34">:</span><span class="token comment" style="color:#999988;font-style:italic">//vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> Initial Plan </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token function" style="color:#d73a49">HashAggregate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">keys</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> functions</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> output</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">#</span><span class="token number" style="color:#36acaa">320</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> Exchange SinglePartition</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ENSURE_REQUIREMENTS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">#</span><span class="token number" style="color:#36acaa">179</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">HashAggregate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">keys</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> functions</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token function" style="color:#d73a49">partial_count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> output</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">count#</span><span class="token number" style="color:#36acaa">318L</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> Project</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token operator" style="color:#393A34">+</span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> FileScan orc testdb</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lineitem_shipmodesuppkey_part</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">l_shipdate#</span><span class="token number" style="color:#36acaa">313</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">l_shipmode#</span><span class="token number" style="color:#36acaa">314</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> Batched</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> DataFilters</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Format</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> ORC</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Location</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> InMemoryFileIndex</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">s3</span><span class="token operator" style="color:#393A34">:</span><span class="token comment" style="color:#999988;font-style:italic">//vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="maximize-cluster-utilization">Maximize cluster utilization<a href="#maximize-cluster-utilization" class="hash-link" aria-label="Direct link to Maximize cluster utilization" title="Direct link to Maximize cluster utilization">​</a></h2>
<p>Amazon EMR configures Spark defaults during the cluster launch based on your cluster&#x27;s infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning.</p>
<p>For a starting point, generally, it is advisable to set <em><code>spark.executor.cores</code></em> to 4 or 5 and tune <em><code>spark.executor.memory</code></em> around this value. Also, when you calculate the <em><code>spark.executor.memory</code></em>, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of <em><code>spark.executor.memory</code></em>). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations.</p>
<p>Based on <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html" target="_blank" rel="noopener noreferrer">Task Configurations</a> r4.8xlarge node has YARN memory of 241664 MB (based on the value of <em><code>yarn.nodemanager.resource.memory-mb</code></em>). The instance has 32 vCores. If we set <em><code>spark.executor.cores</code></em> as 4, we can run 8 executors at a time. So, the configurations will be following.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memory </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memory </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">yarn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memoryOverheadFactor</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">241664</span><span class="token plain"> MB </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">30208</span><span class="token plain"> MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memory </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">24544</span><span class="token plain"> MB </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">substituting default spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">yarn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memoryOverheadFactor </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.1875</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each.</p>
<p>Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation" target="_blank" rel="noopener noreferrer">maximizeResourceAllocation</a>. Setting this property to &quot;true&quot; will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed.</p>
<p>After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener noreferrer">Spark JMX metrics</a> provides JMX level visibility which is the best way to determine resource utilization.</p>
<p>While using instance fleets, it is generally advisable to request worker nodes with similar vCore<!-- -->:memory<!-- --> ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration.</p>
<p>To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different.</p>
<table><thead><tr><th>Instance</th><th>YARN memory in MB</th></tr></thead><tbody><tr><td>c5.4xlarge</td><td>24576</td></tr><tr><td>c5.12xlarge</td><td>90112</td></tr><tr><td>m5.4xlarge</td><td>57344</td></tr><tr><td>m5.12xlarge</td><td>188416</td></tr><tr><td>r5.4xlarge</td><td>122880</td></tr><tr><td>r5.12xlarge</td><td>385024</td></tr></tbody></table>
<p>Now, let us calculate executor memory after setting <em><code>spark.executor.cores</code></em> = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by <em><code>spark.executor.cores</code></em> to get the total container size -&gt; 24576 / 4 = 6144.</p>
<p><em><code>spark.executor.memory</code></em> = 6144 - (6144 * 0.1875) = 4992 MB</p>
<p>Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources.</p>
<p>In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property <em><code>spark.yarn.heterogeneousExecutors.enabled</code></em> and is set to &quot;true&quot; by default. Further, you will be able to control the maximum resources allocated to each executor with properties <em><code>spark.executor.maxMemory</code></em> and <em><code>spark.executor.maxCores</code></em>. Minimum resources are calculated with <em><code>spark.executor.cores</code></em> and <em><code>spark.executor.memory</code></em>. For uniform instance groups or for flexible fleets with instance types having similar vCore<!-- -->:memory<!-- --> ratio, you can try setting <em><code>spark.yarn.heterogeneousExecutors.enabled</code></em> to &quot;false&quot; and see if you get better performance.</p>
<p>Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn&#x27;t matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions:</p>
<ol>
<li>Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver.</li>
<li>Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune <em><code>spark.driver.maxResultSize</code></em>.</li>
</ol>
<p>You can use smaller driver memory (or use the default <em><code>spark.driver.memory</code></em>) if you are running multiple jobs in parallel.</p>
<p>Now, coming to <em><code>spark.sql.shuffle.partitions</code></em> for Dataframes and Datasets and <em><code>spark.default.parallelism</code></em> for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI.</p>
<p><img decoding="async" loading="lazy" alt="BP - 16" src="/aws-emr-best-practices/assets/images/spark-bp-16-9c85d0b01740ac27e75ba9195a5a1af7.png" width="331" height="961" class="img_ev3q"></p>
<p>From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce <em><code>spark.sql.shuffle.partitions</code></em> to increase partition size during the exchange.</p>
<p>Apart from this, if you want to use tools to receive tuning suggestions, consider using <a href="https://aws.amazon.com/blogs/big-data/tune-hadoop-and-spark-performance-with-dr-elephant-and-sparklens-on-amazon-emr/" target="_blank" rel="noopener noreferrer">Sparklens and Dr. Elephant</a> with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas">Use Kryo serializer by registering custom classes especially for Dataset schemas<a href="#use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="hash-link" aria-label="Direct link to Use Kryo serializer by registering custom classes especially for Dataset schemas" title="Direct link to Use Kryo serializer by registering custom classes especially for Dataset schemas">​</a></h2>
<p>Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application.</p>
<div class="language-c codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val spark </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">builder</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">appName</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;my spark application name&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">getConfig</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;spark.serializer&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic">// use this if you need to increment Kryo buffer size. Default 64k</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;spark.kryoserializer.buffer&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;1024k&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic">// use this if you need to increment Kryo buffer max size. Default 64m</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;spark.kryoserializer.buffer.max&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;1024m&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic">/*</span><br></span><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">               * Use this if you need to register all Kryo required classes.</span><br></span><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">               * If it is false, you do not need register any class for Kryo, </span><br></span><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">               * but it will increase your data size when the data is serializing.</span><br></span><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">              */</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;spark.kryo.registrationRequired&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;true&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">getOrCreate</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below:</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val conf = new SparkConf()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conf.registerKryoClasses(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.myPackage.FlightDataset],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.myPackage.BookingDataset],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[scala.collection.mutable.WrappedArray.ofRef[_]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.types.StructType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[Array[org.apache.spark.sql.types.StructType]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.types.StructField],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[Array[org.apache.spark.sql.types.StructField]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.StringType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.LongType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.BooleanType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.DoubleType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.types.Metadata],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.types.ArrayType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.execution.joins.UnsafeHashedRelation&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.catalyst.InternalRow],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[Array[org.apache.spark.sql.catalyst.InternalRow]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.catalyst.expressions.UnsafeRow],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.execution.joins.LongHashedRelation&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.execution.joins.LongToUnsafeRowMap&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.util.collection.BitSet],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[org.apache.spark.sql.types.DataType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classOf[Array[org.apache.spark.sql.types.DataType]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.NullType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.IntegerType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.types.TimestampType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;scala.collection.immutable.Set$EmptySet$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;scala.reflect.ClassTag$$anon$1&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Class.forName(&quot;java.lang.Class&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can also optionally fine tune the following Kryo configs :</p>
<ul>
<li><em><code>spark.kryo.unsafe</code></em> - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet).</li>
<li><em><code>spark.kryoserializer.buffer.max</code></em> - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m.</li>
<li><em><code>spark.kryoserializer.buffer</code></em> - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tune-java-garbage-collector">Tune Java Garbage Collector<a href="#tune-java-garbage-collector" class="hash-link" aria-label="Direct link to Tune Java Garbage Collector" title="Direct link to Tune Java Garbage Collector">​</a></h2>
<p>By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility.</p>
<p>Following is the spark configuration:</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.executor.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#x27;kill -9 %p&#x27;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.driver.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#x27;kill -9 %p&#x27;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters <a href="https://www.oracle.com/technical-resources/articles/java/g1gc.html" target="_blank" rel="noopener noreferrer">here</a> for G1GC and <a href="https://docs.oracle.com/en/java/javase/11/gctuning/parallel-collector1.html" target="_blank" rel="noopener noreferrer">here</a> for ParallelGC. Some useful ones are below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">-XX:ConcGCThreads=n</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:ParallelGCThreads=n</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:InitiatingHeapOccupancyPercent=45</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:MaxGCPauseMillis=200</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can monitor GC performance using Spark UI. The GC time should be ideally <code>&lt;=</code> 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance.</p>
<p><img decoding="async" loading="lazy" alt="BP - 8" src="/aws-emr-best-practices/assets/images/spark-bp-8-68da14d9be672fcadb43a9d7bbdf5cc1.png" width="956" height="602" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-optimal-apis-wherever-possible">Use optimal APIs wherever possible<a href="#use-optimal-apis-wherever-possible" class="hash-link" aria-label="Direct link to Use optimal APIs wherever possible" title="Direct link to Use optimal APIs wherever possible">​</a></h2>
<p>When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="repartition-vs-coalesce">repartition vs coalesce<a href="#repartition-vs-coalesce" class="hash-link" aria-label="Direct link to repartition vs coalesce" title="Direct link to repartition vs coalesce">​</a></h3>
<p>Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df.coalesce(1) // instead of df.repartition(1)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="groupbykey-vs-reducebykey">groupByKey vs reduceByKey<a href="#groupbykey-vs-reducebykey" class="hash-link" aria-label="Direct link to groupByKey vs reduceByKey" title="Direct link to groupByKey vs reduceByKey">​</a></h3>
<p>Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="orderby-vs-sortby-or-sortwithinpartitions">orderBy vs sortBy or sortWithinPartitions<a href="#orderby-vs-sortby-or-sortwithinpartitions" class="hash-link" aria-label="Direct link to orderBy vs sortBy or sortWithinPartitions" title="Direct link to orderBy vs sortBy or sortWithinPartitions">​</a></h3>
<p>orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation">For workloads with predictable pattern, consider disabling dynamic allocation<a href="#for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="hash-link" aria-label="Direct link to For workloads with predictable pattern, consider disabling dynamic allocation" title="Direct link to For workloads with predictable pattern, consider disabling dynamic allocation">​</a></h2>
<p>Dynamic allocation is enabled in EMR by default. It is a great feature for following cases:</p>
<ol>
<li>Workloads processing variable amount of data</li>
<li>When your cluster uses autoscaling</li>
<li>Dynamic processing requirements or unpredictable workload patterns</li>
<li>Streaming and ad-hoc workloads</li>
<li>When your cluster runs multiple concurrent applications</li>
<li>Your cluster is long-running</li>
</ol>
<p>The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where:</p>
<ol>
<li>Workloads have a very predicatable pattern</li>
<li>Amount of data processed is predictable and consistent throughout the application</li>
<li>Cluster runs Spark application in batch mode</li>
<li>Clusters are transient and are of fixed size (no autoscaling)</li>
<li>Application processing is relatively uniform. Workload is not spikey in nature.</li>
</ol>
<p>For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs.</p>
<p>For such use cases, you can consider disabling dynamic allocation along with setting the precise  number and size of executors and cores like below.</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.dynamicAllocation.enabled&quot;: &quot;false&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.executor.instances&quot;: &quot;12&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.executor.memory&quot;: &quot;8G&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.executor.cores&quot;: &quot;4&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well.</p>
<p>However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="leverage-hdfs-as-temporary-storage-for-io-intensive-workloads">Leverage HDFS as temporary storage for I/O intensive workloads<a href="#leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="hash-link" aria-label="Direct link to Leverage HDFS as temporary storage for I/O intensive workloads" title="Direct link to Leverage HDFS as temporary storage for I/O intensive workloads">​</a></h2>
<p>Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes.</p>
<p><img decoding="async" loading="lazy" alt="BP - 12" src="/aws-emr-best-practices/assets/images/spark-bp-12-de4d3cdee6e9295bbcb2c93aa97fe9e3.png" width="508" height="99" class="img_ev3q"></p>
<p>For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors.</p>
<p><img decoding="async" loading="lazy" alt="BP - 13" src="/aws-emr-best-practices/assets/images/spark-bp-13-ce40b366b2751caef72fe9874cfc475b.png" width="852" height="203" class="img_ev3q"></p>
<p>Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size.</p>
<p><img decoding="async" loading="lazy" alt="BP - 14" src="/aws-emr-best-practices/assets/images/spark-bp-14-2d94786b60420eb612071c0bfb6ff2ed.png" width="901" height="202" class="img_ev3q"></p>
<p>However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations.</p>
<p>Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d’s and r6gd’s instead of r5s and r6g’s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spark-speculation-with-emrfs">Spark speculation with EMRFS<a href="#spark-speculation-with-emrfs" class="hash-link" aria-label="Direct link to Spark speculation with EMRFS" title="Direct link to Spark speculation with EMRFS">​</a></h2>
<p>In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, <em><code>spark.speculation</code></em> is turned off. Only enable <em><code>spark.speculation</code></em> if you are doing one of the following.</p>
<ul>
<li>Writing Parquet files to S3 using EMRFSOutputCommitter</li>
<li>Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp</li>
<li>Using HDFS as storage (not recommended)</li>
</ul>
<p>Do not enable <em><code>spark.speculation</code></em> if none of the above criteria is met since it will lead to incorrect or missing or duplicate data.</p>
<p>You can consider enabling <em><code>spark.speculation</code></em> especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, <em><code>spark.speculation</code></em> will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met).</p>
<p>You can set <em><code>spark.speculation</code></em> to true in spark-defaults or pass it as a command line option (--conf <em><code>spark.speculation</code></em>=&quot;true&quot;).</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.speculation&quot;: &quot;true&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please do not enable <em><code>spark.speculation</code></em> if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-dataframes-wherever-possible">Use DataFrames wherever possible<a href="#use-dataframes-wherever-possible" class="hash-link" aria-label="Direct link to Use DataFrames wherever possible" title="Direct link to Use DataFrames wherever possible">​</a></h2>
<p>We must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example -</p>
<ul>
<li>Datasets perform many serializations and deserializations that Dataframes tries to skip.</li>
<li>Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced.</li>
<li>Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes.</li>
</ul>
<p>Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class.</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">case class DeviceIoTData (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  battery_level: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  c02_level: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cca2: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cca3: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cn: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  device_id: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  device_name: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  humidity: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  latitude: Double,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  longitude: Double,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  scale: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  temp: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  timestamp: Long</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="consider-spark-blacklisting-for-large-clusters">Consider Spark Blacklisting for large clusters<a href="#consider-spark-blacklisting-for-large-clusters" class="hash-link" aria-label="Direct link to Consider Spark Blacklisting for large clusters" title="Direct link to Consider Spark Blacklisting for large clusters">​</a></h2>
<p>Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14.</p>
<p>This blacklisting is enabled by default in Amazon EMR with the <em><code>spark.blacklist.decommissioning.enabled</code></em> property set to true. You can control the time for which the node is blacklisted using <em><code>spark.blacklist.decommissioning.timeout property</code></em>, which is set to 1 hour by default, equal to the default value for <em><code>yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs</code></em>. It is recommended to set <em><code>spark.blacklist.decommissioning.timeout</code></em> to a value equal to or greater than <em><code>yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs</code></em> to make sure that Amazon EMR blacklists the node for the entire decommissioning period.</p>
<p>Following are some <em>experimental</em> blacklisting properties.</p>
<ul>
<li><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code> determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2.</li>
<li><code>spark.blacklist.task.maxTaskAttemptsPerNode</code> determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2.</li>
<li><code>spark.blacklist.stage.maxFailedTasksPerExecutor</code> is same as <em><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code></em> but the executor is blacklisted for the entire stage.</li>
<li><code>spark.blacklist.stage.maxFailedExecutorsPerNode</code> determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2.</li>
<li><code>spark.blacklist.application.maxFailedTasksPerExecutor</code> is same as <em><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code></em> but the executor is blacklisted for the entire application.</li>
<li><code>spark.blacklist.application.maxFailedExecutorsPerNode</code> is same as <em><code>spark.blacklist.stage.maxFailedExecutorsPerNode</code></em> but the worker node is blacklisted for the entire application.</li>
<li><code>spark.blacklist.killBlacklistedExecutors</code> when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring.</li>
<li><code>spark.blacklist.application.fetchFailure.enabled</code> when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring.</li>
</ul>
<p>The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application.</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.blacklist.killBlacklistedExecutors&quot;: &quot;true&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;spark.blacklist.application.fetchFailure.enabled&quot;: &quot;true&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs.</p>
<p><img decoding="async" loading="lazy" alt="BP - 30" src="/aws-emr-best-practices/assets/images/spark-bp-30-40f380cf8cfd58a0c805cfc1e1bdcb6d.png" width="826" height="580" class="img_ev3q"></p>
<p>When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by <em><code>spark.stage.maxConsecutiveAttempts</code></em>. This is determined by the setting <em><code>spark.stage.attempt.ignoreOnDecommissionFetchFailure</code></em> being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-the-number-of-partitions-are-determined-when-reading-a-raw-file">How the number of partitions are determined when reading a raw file<a href="#how-the-number-of-partitions-are-determined-when-reading-a-raw-file" class="hash-link" aria-label="Direct link to How the number of partitions are determined when reading a raw file" title="Direct link to How the number of partitions are determined when reading a raw file">​</a></h2>
<p>When reading a raw file, that can be a text file, csv, etc. the count behind the number of partitions created from Spark depends from many variables as the methods used to read the file, the default parallelism and so on. Following an overview of how these factors are related between each other so to better understand how files are processed.</p>
<p>Here a brief summary of relationship between core nodes - executors - tasks:</p>
<ul>
<li>each File is composed by blocks that will be parsed according to the InputFormat corresponding to the specific data format, and generally combines several blocks into one input slice, called InputSplit</li>
<li>InputSplit and Task are one-to-one correspondence relationship</li>
<li>each of these specific Tasks will be assigned to one executor of the nodes on the cluster</li>
<li>each node can have one or more Executors, depending on the node resources and executor settings</li>
<li>each Executor consists of cores and memory whose default is based on the node type. Each executor can only execute one task at time.</li>
</ul>
<p>So based on that, the number of threads/tasks will be based on the number of partitions while reading.</p>
<p>Please note that the S3 connector takes some configuration option (e.g. s3a: fs.s3a.block.size) to simulate blocks in Hadoop services, but the concept of blocks in S3 does not really exists. Unlike HDFS that is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior, EMRFS is an object store, not a file system. For more information, see Hadoop documentation for <a href="https://hadoop.apache.org/docs/stable2/hadoop-project-dist/hadoop-common/filesystem/introduction.html#Object_Stores_vs._Filesystems" target="_blank" rel="noopener noreferrer">Object Stores vs. Filesystems</a>.</p>
<p>Now, there are several factors that dictate how a dataset or file is mapped to a partition. First is the method used to read the file (e.g. text file), that changes if you&#x27;re working with rdds or dataframes:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sc.textFile(...) returns a RDD[String]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   textFile(String path, int minPartitions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   file system URI, and return it as an RDD of Strings.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.read.text(...) returns a DataSet[Row] or a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   text(String path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Loads text files and returns a DataFrame whose schema starts with a string column named &quot;value&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   and followed by partitioned columns if there are any.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-core-api-rdds">Spark Core API (RDDs)<a href="#spark-core-api-rdds" class="hash-link" aria-label="Direct link to Spark Core API (RDDs)" title="Direct link to Spark Core API (RDDs)">​</a></h3>
<p>When using <em><code>sc.textFile</code></em> Spark uses the block size set for the filesysytem protocol it&#x27;s reading from, to calculate the number of partitions in input:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/core/src/main/scala/org/apache/spark/SparkContext.scala#L819-L832" target="_blank" rel="noopener noreferrer">SparkContext.scala</a></p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  /* Read a text file from HDFS, a local file system (available on all nodes), or any</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * Hadoop-supported file system URI, and return it as an RDD of Strings.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @param path path to the text file on a supported file system</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @param minPartitions suggested minimum number of partitions for the resulting RDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @return RDD of lines of the text file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  def textFile(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      path: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    assertNotStopped()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><a href="https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L373" target="_blank" rel="noopener noreferrer">FileInputFormat.java</a></p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">        if (isSplitable(fs, path)) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          long blockSize = file.getBlockSize();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>When using the <em>S3A</em> protocol the block size is set through the <em><code>fs.s3a.block.size parameter</code></em> (default 32M), and when using <em>S3</em> protocol through <em><code>fs.s3n.block.size</code></em> (default 64M). Important to notice here is that with <em>S3</em> protocol the parameter used is <em><code>fs.s3n.block.size</code></em> and not <em><code>fs.s3.block.size</code></em> as you would expect. In EMR indeed, when using EMRFS, which means using <em>s3</em> with <em>s3://</em> prefix, <em><code>fs.s3.block.size</code></em> will not have any affect on the EMRFS configration.</p>
<p>Following some testing results using these parameters:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">CONF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Input: 1 file, total size 336 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 1 (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3A protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3a.block.size = 32M (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/32 = 11</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3 protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3n.block.size = 64M (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/64 = 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 2 (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3A protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3a.block.size = 64M (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/64 = 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3n.block.size = 128M (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/128 = 3</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-sql-dataframes">Spark SQL (DATAFRAMEs)<a href="#spark-sql-dataframes" class="hash-link" aria-label="Direct link to Spark SQL (DATAFRAMEs)" title="Direct link to Spark SQL (DATAFRAMEs)">​</a></h3>
<p>When using <em><code>spark.read.text</code></em> no. of spark tasks/partitions depends on default parallelism:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L423-L430" target="_blank" rel="noopener noreferrer">DataSourceScanExec.scala</a></p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    val defaultMaxSplitBytes =</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val totalBytes = selectedPartitions.flatMap(_.files.map (_.getLen + openCostInBytes)).sum</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val bytesPerCore = totalBytes / defaultParallelism</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The default Parallelism is determined via:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L457-L459" target="_blank" rel="noopener noreferrer">CoarseGrainedSchedulerBackend.scala</a></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  override def defaultParallelism(): Int = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If <em>defaultParallelism</em> is too large, <em>bytesPerCore</em> will be small, and <em>maxSplitBytes</em> can be small, which can result in more no. of spark tasks/partitions. So if there&#x27;re more cores, <em><code>spark.default.parallelism</code></em> can be large, <em>defaultMaxSplitBytes</em> can be small, and no. of spark tasks/partitions can be large.</p>
<p>In order to tweak the input no. of partitions the following parameters need to be set:</p>
<table><thead><tr><th>Classification</th><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>spark-default</td><td><em><code>spark.default.parallelism</code></em></td><td>default: max(total number of vCores, 2)</td></tr><tr><td>spark-default</td><td><em><code>spark.sql.files.maxPartitionBytes</code></em></td><td>default: 128MB</td></tr></tbody></table>
<p>If these parameters are modified, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation" target="_blank" rel="noopener noreferrer"><em>maximizeResourceAllocation</em></a> need to be disabled, as it would override <em><code>spark.default.parallelism parameter</code></em>.</p>
<p>Following some testing results using these parameters:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">CONF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Total number of vCores = 16 -&gt; spark.default.parallelism = 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- spark.sql.files.maxPartitionBytes = 128MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Input: 1 CSV file, total size 352,3 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Partition size = 352,3/16 = ∼22,09 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Input: 10 CSV files, total size 3523 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 30</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Partition size = 3523/30 = ∼117,43 MB</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<p>Disclaimer</p>
<p>When writing a file the number of partitions in output will depends from the number of partitions in input that will be maintained if no shuffle operations are applied on the data processed, changed otherwise based on <em><code>spark.default.parallelism</code></em> for RDDs and <em><code>spark.sql.shuffle.partitions</code></em> for dataframes.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bestpractices/Applications/Spark/performance.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/observability"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Observability</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/thrift"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Thrift Server</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#use-right-file-formats-and-compression-type" class="table-of-contents__link toc-highlight">Use right file formats and compression type</a></li><li><a href="#data-partitioning" class="table-of-contents__link toc-highlight">Data Partitioning</a></li><li><a href="#maximize-cluster-utilization" class="table-of-contents__link toc-highlight">Maximize cluster utilization</a></li><li><a href="#use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="table-of-contents__link toc-highlight">Use Kryo serializer by registering custom classes especially for Dataset schemas</a></li><li><a href="#tune-java-garbage-collector" class="table-of-contents__link toc-highlight">Tune Java Garbage Collector</a></li><li><a href="#use-optimal-apis-wherever-possible" class="table-of-contents__link toc-highlight">Use optimal APIs wherever possible</a><ul><li><a href="#repartition-vs-coalesce" class="table-of-contents__link toc-highlight">repartition vs coalesce</a></li><li><a href="#groupbykey-vs-reducebykey" class="table-of-contents__link toc-highlight">groupByKey vs reduceByKey</a></li><li><a href="#orderby-vs-sortby-or-sortwithinpartitions" class="table-of-contents__link toc-highlight">orderBy vs sortBy or sortWithinPartitions</a></li></ul></li><li><a href="#for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="table-of-contents__link toc-highlight">For workloads with predictable pattern, consider disabling dynamic allocation</a></li><li><a href="#leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="table-of-contents__link toc-highlight">Leverage HDFS as temporary storage for I/O intensive workloads</a></li><li><a href="#spark-speculation-with-emrfs" class="table-of-contents__link toc-highlight">Spark speculation with EMRFS</a></li><li><a href="#use-dataframes-wherever-possible" class="table-of-contents__link toc-highlight">Use DataFrames wherever possible</a></li><li><a href="#consider-spark-blacklisting-for-large-clusters" class="table-of-contents__link toc-highlight">Consider Spark Blacklisting for large clusters</a></li><li><a href="#how-the-number-of-partitions-are-determined-when-reading-a-raw-file" class="table-of-contents__link toc-highlight">How the number of partitions are determined when reading a raw file</a><ul><li><a href="#spark-core-api-rdds" class="table-of-contents__link toc-highlight">Spark Core API (RDDs)</a></li><li><a href="#spark-sql-dataframes" class="table-of-contents__link toc-highlight">Spark SQL (DATAFRAMEs)</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contributing</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws/aws-emr-best-practices/tree/main" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
</body>
</html>