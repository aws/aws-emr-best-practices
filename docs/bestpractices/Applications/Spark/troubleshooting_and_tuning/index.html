<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bestpractices/Applications/Spark/troubleshooting_and_tuning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">5.2 - Spark troubleshooting and performance tuning | AWS Open Data Analytics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" name="twitter:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" property="og:url" content="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="5.2 - Spark troubleshooting and performance tuning | AWS Open Data Analytics"><meta data-rh="true" name="description" content="5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK"><meta data-rh="true" property="og:description" content="5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK"><link data-rh="true" rel="icon" href="/aws-emr-best-practices/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning" hreflang="en"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning" hreflang="x-default"><link rel="stylesheet" href="/aws-emr-best-practices/assets/css/styles.7a6c5961.css">
<script src="/aws-emr-best-practices/assets/js/runtime~main.63e43d74.js" defer="defer"></script>
<script src="/aws-emr-best-practices/assets/js/main.e11bb194.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/aws-emr-best-practices/"><div class="navbar__logo"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AWS Open Data Analytics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/aws-emr-best-practices/docs/bestpractices/">Best Practices</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/benchmarks/introduction">Benchmarks</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/utilities/">Utilities</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/migration/introduction">Migration</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/aws-emr-best-practices/docs/bestpractices/">EMR Best Practices Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/Introduction">Cost Optimizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction">Reliability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Security/introduction">Security</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction">Applications</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction">Hbase</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hive/introduction">Hive</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Spark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices">5.1 - Spark General</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning">5.2 - Spark troubleshooting and performance tuning</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction">Architecture</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/aws-emr-best-practices/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Applications</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Spark</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">5.2 - Spark troubleshooting and performance tuning</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>5.2 - Spark troubleshooting and performance tuning</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="521-----spark-structured-streaming-applications-have-high-connection-create-rate-to-amazon-msk">5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK<a href="#521-----spark-structured-streaming-applications-have-high-connection-create-rate-to-amazon-msk" class="hash-link" aria-label="Direct link to 5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK" title="Direct link to 5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK">​</a></h2>
<p><strong>Symptom</strong>: Amazon MSK cluster has high CPU usage and MSK metrics indicate high connection create rate to the cluster.</p>
<p><strong>Analysis</strong>: By default, Spark Structured Streaming Kafka connector has a 1:1 mapping relation of MSK TopicPartitions to Spark tasks. Between micro batches, Spark tries to (best effort) assign the same MSK TopicPartitions to the same executors which in turn reuses the Kafka consumers and connections.</p>
<p>Spark Structured Streaming Kafka connector has an option <code>minPartitions</code> which can divide large TopicPartitions to smaller pieces. When <code>minPartitions</code> is set to a value larger than the number of TopicPartitions,  Spark creates tasks based on <code>minPartitions</code> to increase parallelism (the number of Spark tasks will be approximately <code>minPartition</code>).</p>
<ul>
<li>As 1:1 mapping doesn&#x27;t exist anymore, Spark executors are randomly assigned to process any TopicPartition OffsetRanges. An executor processed TopicPartition X can be assigned to process TopicPartition Y in next micro batch.  A new Kafka consumer/connection needs to be created if Y is on another MSK broker.</li>
<li>One Spark executor can be assigned to process multiple Spark tasks with the same MSK TopicPartition on different OffsetRanges.  And in Spark 2.x, Kafka consumer cache is disabled when multiple tasks in the same executor read the same TopicPartitions .</li>
</ul>
<p>Setting <code>minPartitions</code> comes at a cost of initializing Kakfa consumers at each micro batch. This may impact performance especially when using SSL.</p>
<p>A test was run with following test environment:</p>
<p><strong>Kafka version 2.8.1</strong></p>
<ul>
<li>3 kafka.m5.xlarge instances</li>
<li>test kafka topic has 10 partitions</li>
<li>only SASL/SCRAM authentication enabled</li>
</ul>
<p><strong>EMR 5.36 (Spark 2.4.8)  cluster</strong></p>
<ul>
<li>30 core nodes - EC2 m5.4xlarge</li>
</ul>
<p>Spark Structured Streaming test application has 5 cores 5G memory for each executor.</p>
<p>Below figure shows the test result of different <code>minPartitions</code> values with MSK’s ConnectionCreationRate and CPUUser usages. As shown in the test result, higher ConnectionCreationRate is related to higher CPU usage.</p>
<p>Test1: 50 minPartitions 16:40-17:30<br>
<!-- -->Test2: 100 minPartitions 18:00-18:35<br>
<!-- -->Test3: 200 minPartitions 19:00-19:35<br>
<!-- -->Test4: no minPartitions 20:06 - 20:30</p>
<p><img loading="lazy" alt="ConnectionCreationRate" src="/aws-emr-best-practices/assets/images/spark-tt-1-2764119881a34635e803761b36ee0ba6.png" width="1545" height="603" class="img_ev3q"></p>
<p><strong>Recommendation</strong>:</p>
<ol>
<li>
<p>Upgrade to the latest EMR version (spark 3.x) to use Sparks <a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#consumer-caching" target="_blank" rel="noopener noreferrer">consumer pool cache</a> feature. This feature allows Spark to cache more than one Kafka consumer with same MSK TopicPartition at each executor, and reuse the consumers in later micro batches. This will allow you to set <code>minPartitions</code> while reduce the ConnectionCreationRate.</p>
</li>
<li>
<p>On EMR 5.x (Spark 2.x), only set min partitions when needed - for example, if you have data skew or if your stream is falling behind. Min partitions will allow you to increase parallelism and process records faster but at the expense of high connection rates and CPU.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data"><strong>5.2.2 - spark.driver.maxResultSize error on an EMR heterogeneous cluster but the driver is not collecting data</strong><a href="#522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data" class="hash-link" aria-label="Direct link to 522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data" title="Direct link to 522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data">​</a></h2>
<p><strong>Symptom</strong>: Spark jobs fail from time to time and below error is seen in the log:</p>
<p><code>22/08/22 14:14:24 ERROR FileFormatWriter: Aborting job f6913a46-d2d8-46f0-a545-2d2ba938b113. org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 179502 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2171)</code></p>
<p>By setting <code>spark.driver.maxResultSize</code> to 0(unlimited), the error is gone.  But the Spark job is not collecting data to driver, how can the result returning to driver exceed 1024MB?</p>
<p><strong>Analysis</strong>:  Each finished task sends a serialized <code>WriteTaskResult</code> object to driver. The object size is usually several kilobytes, e.g.</p>
<p><code>22/09/06 22:24:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 5192 bytes result sent to driver</code></p>
<p>From the log, we can see there are 179502 (or more) tasks. For such a number of tasks, the total result size can exceed 1024MB for serialized <code>WriteTaskResult</code> objects only.</p>
<p>The job is reading parquet files from S3 and the input folder has 3.5K parquet files with average size ~116MB per file. As default <code>spark.sql.files.maxPartitionBytes</code> is 128M, so approximately one file to one Spark task.  The job&#x27;s processing logic further splits one task to ~11 tasks. Total tasks should be 3.5K * 11 = 38.5K. But why there are 179502 (or more) tasks?</p>
<p>To find root cause, we need to understand how Spark SQL decides the max size of a partition for non-bucketed splittable files.</p>
<p>Spark2 is following below formula (Spark3 has a slightly different formula which is described later)</p>
<p><code>maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</code></p>
<p><code>maxSplitBytes</code> is the max bytes per partition.</p>
<p><code>defaultMaxSplitBytes</code> is from <code>spark.sql.files.maxPartitionBytes</code> whose default value is 128M.</p>
<p><code>openCostInBytes</code>  is from <code>spark.sql.files.openCostInBytes</code> whose default value is 4M.</p>
<p><code>bytesPerCore</code> = (Sum of all data file size + num of file * <code>openCostInBytes</code> ) / <code>defaultParallelism</code></p>
<p><code>defaultParallelism</code>’s default value is total number of virtual cores running the job.  It can also be set to a value by defining <code>spark.default.parallelism</code>.</p>
<p>When there are a large number of virtual cores allocated to the Spark job by Yarn, <code>maxSplitBytes</code> can be smaller than <code>defaultMaxSplitBytes</code>, i.e. more tasks will be created.  For this case, from Spark UI, we know total number of vcores is 15837, i.e. <code>defaultParallelism</code> is 15837
<img loading="lazy" alt="SparkUITotalCores" src="/aws-emr-best-practices/assets/images/spark-tt-2-8c7a2c6522fbef2dd6c58aa6fdadce9e.png" width="1553" height="205" class="img_ev3q"></p>
<p><code>bytesPerCore</code> = (116M * 3500 + 4M * 3500)/15837 = 26.5M</p>
<p><code>maxSplitBytes</code> = min(128M, max(4M, 26.5M)) = 26.5M.  So one 116M parquet file is split into 4~5 Spark tasks.  3.5K * 11 * 5 = 192.5K -- that&#x27;s why there were 179502 (or more) tasks.</p>
<p>Note that the vcore count is based on Yarn containers, not physical cores.  As Yarn’s default container allocation is based on available memory, this means there can be vcore over subscriptions in a heterogeneous EMR cluster.</p>
<p>For example, if we have a heterogeneous EMR cluster as below:</p>
<p>Core Node: c5.12xlarge(48cores/96GB) — Memory allocated to Yarn: 90112MB<br>
<!-- -->Task Node: r5.8xlarge(32cores/256GB) — Memory allocated to Yarn: 253952MB</p>
<p>The default EMR executor size is based on core node instance type. In this example, for c5.12xlarge, default executor size is 3 cores 4743M memory. Default <code>spark.yarn.executor.memoryOverheadFactor</code> is 0.1875.  A Yarn container has 3 cores, 4743MB*(1+0.1875)= 5632MB memory.</p>
<p>On c5.12xlarge, Yarn can allocate 16 conatainers with 48 vcores in total:<br>
<!-- -->90112MB/5632MB = 16 containers * 3 core = 48 vcores</p>
<p>While on r5.8xlarge, Yarn can allocate 45 containers with 135 vcores in total:<br>
<!-- -->253952MB/5632MB = 45 containers * 3 core = 135 vcores - 32cores = 103 vcore oversubscription</p>
<p><strong>Recommendation</strong>: When Spark reads splittable files, <code>maxSplitBytes</code> can be smaller than <code>spark.sql.files.maxPartitionBytes</code> if there are a big number of vcores allocated to the job.  Use the formula described here to set <code>spark.default.parallelism</code> value properly and have a reasonable <code>maxSplitBytes</code>.</p>
<p><strong>Spark 3</strong></p>
<p>Spark 3 provides more options to control<code>maxSplitBytes</code> as below</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  def maxSplitBytes(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      sparkSession: SparkSession,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      selectedPartitions: Seq[PartitionDirectory]): Long = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val minPartitionNum = sparkSession.sessionState.conf.filesMinPartitionNum</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .getOrElse(sparkSession.leafNodeDefaultParallelism)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val bytesPerCore = totalBytes / minPartitionNum</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><code>filesMinPartitionNum</code> is from <code>spark.sql.files.minPartitionNum</code>. It is the suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is <code>spark.default.parallelism</code>.</p>
<p><code>leafNodeDefaultParallelism</code> is from <code>spark.sql.leafNodeDefaultParallelism</code>. It is the default parallelism of Spark SQL leaf nodes.</p>
<p>Setting either of the above two parameters has the same effect as <code>spark.default.parallelism</code> on <code>maxSplitBytes</code>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="523-----issues-related-to-spark-logs"><strong>5.2.3  -  Issues related to Spark logs</strong><a href="#523-----issues-related-to-spark-logs" class="hash-link" aria-label="Direct link to 523-----issues-related-to-spark-logs" title="Direct link to 523-----issues-related-to-spark-logs">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5231-spark-events-logs-not-getting-pushed-to-s3">5.2.3.1 Spark events logs not getting pushed to S3<a href="#5231-spark-events-logs-not-getting-pushed-to-s3" class="hash-link" aria-label="Direct link to 5.2.3.1 Spark events logs not getting pushed to S3" title="Direct link to 5.2.3.1 Spark events logs not getting pushed to S3">​</a></h3>
<p><strong>Symptom</strong>: This might be caused if the spark logs properties is configured with HDFS location, not with S3 location.</p>
<p><strong>Troubleshooting Steps:</strong>:</p>
<ol>
<li>Check the appusher logs of the cluster for the time of the issue where the app pusher log location on your 21EMR logs bucket is <code>&lt;s3_log_bucket&gt;/&lt;prefix&gt;/j- xxxxxxx&gt;/daemons/apppusher.log.gz</code></li>
<li>Look for error java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Wrong FS: 23 s3://demo-bucket/testing-prod/spark/logs,expected: hdfs://ip-XX-XX-XXX-XXX.eu-west- 241.compute.internal:8020
By default, Spark Structured Streaming Kafka connector has a 1:1 mapping relation of MSK TopicPartitions to Spark tasks. Between micro batches, Spark tries to (best effort) assign the same MSK TopicPartitions to the same executors which in turn reuses the Kafka consumers and connections.</li>
<li>Check the spark-defaults.conf file found in the location /etc/spark/conf.dist on the master node for the 26configurationsmentionedbelow: 27spark.eventLog.dir : The spark jobs themselves must be configured to log events, and to log them to the same 28shared,writabledirectory 29spark.history.fs.logDirectory: For the filesystem history provider, the URL to the directory containing 30application event logs to load.</li>
<li>Restart the spark history server to reflect the configuration changes. <a href="https://repost.aws/knowledge-center/restart-service-emr" target="_blank" rel="noopener noreferrer">Various EMR version have different 32command to restart the services</a>. Sudo systemctl restart spark-history-server</li>
</ol>
<p><strong>Additional Reference:</strong></p>
<p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html#app-history-spark-UI-limitations" target="_blank" rel="noopener noreferrer">Consideration and limitations of spark history UI</a></p>
<p><a href="https://spark.apache.org/docs/latest/monitoring.html#monitoring-and-instrumentation" target="_blank" rel="noopener noreferrer">Spark monitoring and instrumentation</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5232----spark-history-server-not-opening-up">5.2.3.2  - Spark History Server not opening up<a href="#5232----spark-history-server-not-opening-up" class="hash-link" aria-label="Direct link to 5.2.3.2  - Spark History Server not opening up" title="Direct link to 5.2.3.2  - Spark History Server not opening up">​</a></h3>
<p><strong>Symptom</strong>: This issue occurs if the Spark history server memory is low. You may also have to <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html" target="_blank" rel="noopener noreferrer">enable web UI access</a> to the <a href="https://www.youtube.com/watch?v=fpGAnXgBZe0" target="_blank" rel="noopener noreferrer">spark history server</a></p>
<p><strong>Troubleshooting Steps:</strong>:</p>
<ol>
<li>
<p>SSH in to the master node, go to the location /etc/spark/conf/spark-env.sh and set the parameter SPARK_DAEMON_MEMORY to an higher value, for example 4g (or more) You can manually edit the spark-env file at &quot;/etc/spark/conf/spark-env.sh&quot; on the master node</p>
</li>
<li>
<p>Restart the spark-history-server</p>
</li>
<li>
<p>You can confirm the memory increase with the given command below, in this parameter &quot;-Xmx4g&quot;: Or you can reconfigure from EMR console select the primary instance group and click on reconfigure and add the json and while saving click on apply to all instance groups.</p>
</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Classification&quot;: &quot;spark-env&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;SPARK_DAEMON_MEMORY&quot;: &quot;4g&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><em>Note: In multimaster setup does not support persistent application user interface.</em></p>
<p><strong>Additional Reference:</strong></p>
<p><a href="https://repost.aws/knowledge-center/emr-view-spark-history-events" target="_blank" rel="noopener noreferrer">Why can’t I view the Apache Spark history events or logs from the Spark web UI in Amazon EMR?</a></p>
<p><a href="https://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener noreferrer">Web Interfaces</a></p>
<p><a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html" target="_blank" rel="noopener noreferrer">Configure Spark</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5233---spark-application-failure-or-spark-streaming-application-failure-with-137-error-code">5.2.3.3  -Spark application failure or Spark streaming application failure with 137 error code<a href="#5233---spark-application-failure-or-spark-streaming-application-failure-with-137-error-code" class="hash-link" aria-label="Direct link to 5.2.3.3  -Spark application failure or Spark streaming application failure with 137 error code" title="Direct link to 5.2.3.3  -Spark application failure or Spark streaming application failure with 137 error code">​</a></h3>
<p><strong>Symptom</strong>: There can be various reasons for a spark application failures, most common root causes are
<a href="https://repost.aws/knowledge-center/stage-failures-spark-emr" target="_blank" rel="noopener noreferrer">stage failure spark EMR</a>, <a href="https://repost.aws/knowledge-center/container-killed-on-request-137-emr" target="_blank" rel="noopener noreferrer">Container killed on request. Exit code is 137</a> and <a href="https://repost.aws/knowledge-center/executorlostfailure-slave-lost-emr" target="_blank" rel="noopener noreferrer">ExecutorLostFailure &quot;Slave lost&quot;.</a></p>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>
<p>If you have submitted the application via step, check the step logs stderr to get the exception details.</p>
</li>
<li>
<p>In the step logs you will also find the application ID related to the step use the step ID logs to get the application ID and check the container logs stderr.gz to find the error In the above logs look for the information as mentioned on this article <a href="https://repost.aws/knowledge-center/emr-troubleshoot-failed-spark-jobs" target="_blank" rel="noopener noreferrer">EMR troubleshooting failed spark jobs</a></p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5234---spark-application-event-with-incomplete-logs">5.2.3.4  -Spark application event with incomplete logs<a href="#5234---spark-application-event-with-incomplete-logs" class="hash-link" aria-label="Direct link to 5.2.3.4  -Spark application event with incomplete logs" title="Direct link to 5.2.3.4  -Spark application event with incomplete logs">​</a></h3>
<p><strong>Symptom</strong>:You<!-- --> might encounter this error if in case of incorrect configuration of log aggregation for long running applications. Application logs from &quot;/var/log/hadoop-yarn/apps&quot; hdfs directory are deleted after every 2 days. Long-running Spark jobs such as Spark streaming, and large jobs, such as Spark SQL queries can generate <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html#app-history-spark-UI-large-event-logs" target="_blank" rel="noopener noreferrer">large event logs</a>. Large events logs, can rapidly deplete  disk space on compute instances and may result in OutOfMemory errors when loading persistent UIs. To avoid these issues, it is recommended that you turn on the Spark event log rolling and compaction feature. This feature is available on Amazon EMR versions emr-6.1.0 and later. <a href="https://guide.aws.dev/articles/AROsZtWyXQRSaEcknGWhpCGA/how-to-enable-yarn-log-aggregation-for-running-spark-job" target="_blank" rel="noopener noreferrer">Check how to enable yarn log aggregation for spark jobs</a></p>
<p><strong>Troubleshooting Steps:</strong></p>
<p>Check the instance state logs <code>&lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/deamon/instance-state/</code>(find the log as per the error time frame) of the your EMR instances and search for df -h and check for the /mnt usage for that particular instance. Below parameters can be configured.</p>
<ul>
<li>
<p><code>spark.eventLog.rolling.enabled</code> – Turns on event log rolling based on size. This is deactivated by default.</p>
</li>
<li>
<p><code>spark.eventLog.rolling.maxFileSize</code> – When rolling is activated, specifies the maximum size of the event log file before it rolls over.</p>
</li>
<li>
<p><code>spark.history.fs.eventLog.rolling.maxFilesToRetain</code> – Specifies the maximum number of non-compacted event log files to retain.</p>
</li>
</ul>
<p><em>Note: Instance-state logs are snapshots captured at the instance level every 15 min.</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5235---access-spark-history-server-for-old-cluster">5.2.3.5  -Access spark history server for old cluster<a href="#5235---access-spark-history-server-for-old-cluster" class="hash-link" aria-label="Direct link to 5.2.3.5  -Access spark history server for old cluster" title="Direct link to 5.2.3.5  -Access spark history server for old cluster">​</a></h3>
<p><strong>Symptom</strong>: To access your old EMR cluster logs, go to application history and relevant log files for active and terminated clusters. The logs are available for 30 days after the application ends. On the Application user interfaces tab or the cluster Summary page for your cluster in the old console for Amazon EMR 5.30.1 or 6.x, choose the YARN timeline server, Tez UI, or Spark history server link. Starting from EMR version 5.25.0, persistent application user interface (UI) links are available for Spark History Server, which doesn&#x27;t require you to set up a web proxy through a SSH connection and can be directly accessed via the AWS EMR Web Console. Navigate to the terminated cluster whose logs you require, clicking on Application user interfaces tab (on old EMR console) or Applications tab and select Persistent application UIs (on new EMR console) and finally clicking on <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html#app-history-spark-UI-event-logs" target="_blank" rel="noopener noreferrer">Spark History Server link</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="524----issues-related-to-spark-session"><strong>5.2.4  - Issues Related to Spark session</strong><a href="#524----issues-related-to-spark-session" class="hash-link" aria-label="Direct link to 524----issues-related-to-spark-session" title="Direct link to 524----issues-related-to-spark-session">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5241---spark-session-not-getting-generated-with-missing-std-out">5.2.4.1  -Spark session not getting generated with Missing std out<a href="#5241---spark-session-not-getting-generated-with-missing-std-out" class="hash-link" aria-label="Direct link to 5.2.4.1  -Spark session not getting generated with Missing std out" title="Direct link to 5.2.4.1  -Spark session not getting generated with Missing std out">​</a></h3>
<p><strong>Symptom</strong>: You might encounter below listed spark session issues</p>
<ul>
<li>
<p>Error when trying to run jupyter note book for spark.</p>
</li>
<li>
<p><code>The code failed because of a fatal error: Invalid status code &#x27;500&#x27; from http://localhost:8998/sessions with error payload: &quot;java.lang.NullPointerException&quot;.java.net.ConnectException: Connection refused</code></p>
</li>
<li>
<p><code>ERROR SparkContext: Error initializing SparkContext.org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for s3ablock-0001-</code></p>
</li>
</ul>
<p><strong>Probable Root cause</strong></p>
<p>Spark code might lack spark session initialization. Make sure you follow the official <a href="https://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener noreferrer">documentation of Spark</a> to build an spark session. <code>Invalid status code &#x27;500&#x27;</code>is related  to a server-side error, check the livy server logs running on the EMR cluster.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5242---spark-session-with-livy-is-failing">5.2.4.2  -Spark session with livy is failing<a href="#5242---spark-session-with-livy-is-failing" class="hash-link" aria-label="Direct link to 5.2.4.2  -Spark session with livy is failing" title="Direct link to 5.2.4.2  -Spark session with livy is failing">​</a></h3>
<p><strong>Symptom</strong>: If you are seeing Error message :<code>The code failed because of a fatal error:Session 11 did not start up in 60 seconds</code>, you can try the below steps:</p>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>Make sure Spark has enough available resources for Jupyter to create a Spark context.</li>
<li>Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.</li>
<li>Restart the kernel.</li>
<li>Check the following configuration are present on your EMR cluster, if not then please add the same</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;Classification&quot;: &quot;livy-conf&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;Properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &quot;livy.server.session.timeout-check&quot;: &quot;true&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &quot;livy.server.session.timeout&quot;: &quot;2h&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &quot;livy.server.yarn.app-lookup-timeout&quot;: &quot;120s&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="5">
<li>Restart the Livy server to apply the configuration</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo systemctl stop livy-server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo systemctl start livy-server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>To further troubleshoot the issue, you can find more information on the livy-livy-server.out found on the following location <code>/var/log/livy</code></p>
<p><strong>Additional resource:</strong></p>
<p>To know more about the livy we you can go over the following <a href="https://www.youtube.com/watch?v=C_3iEf_KNv8" target="_blank" rel="noopener noreferrer">resource</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5243---spark-application-failure-due-to-disk-space-or-cpu-load">5.2.4.3  -Spark application failure due to Disk space or CPU Load<a href="#5243---spark-application-failure-due-to-disk-space-or-cpu-load" class="hash-link" aria-label="Direct link to 5.2.4.3  -Spark application failure due to Disk space or CPU Load" title="Direct link to 5.2.4.3  -Spark application failure due to Disk space or CPU Load">​</a></h3>
<p><strong>Symptom</strong>: You might encounter disk space/CPU related issues due to:</p>
<ul>
<li>
<p>The log pusher service is not running or consuming high CPU, to know more check the <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-debugging.html#emr-plan-debugging-logs" target="_blank" rel="noopener noreferrer">instance-state logs</a> for the CPU, memory state, and garbage collector threads of the node.</p>
</li>
<li>
<p>Local disk of the node is getting full, as application logs might be stored on the nodes local disk where your application is running, so a long running application can fill in the local disk of a node</p>
</li>
<li>
<p>There might be another app which is running on the cluster at the same time could be filling up disk space/consuming CPU cycles.</p>
</li>
<li>
<p>High retention period of the spark event and the yarn container logs</p>
</li>
</ul>
<p><em>Note: Disk space or CPU load may not always cause an application failure, however could lead to performance regression</em></p>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>If the logpusher is malfunctioning or taking up lot of CPU then, it can result disk usage climb up
Check the log pusher server status by using the command
<code>systemctl status logpusher</code></li>
<li>If the log pusher shows up near the top by running the below command in the instance-state logs, then it would benefit a restart</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ps auxwww --sort -%cpu | head -20</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo systemctl status logpushe</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="3">
<li>You can also check latest logs of the logpusher to find errors and exceptions to report to the AWS premium support to help you further. Logpusher logs can be found in the following location <code>/emr/logpusher/log</code></li>
<li>You can also proactively <a href="https://repost.aws/knowledge-center/core-node-emr-cluster-disk-space" target="_blank" rel="noopener noreferrer">manage the disk space</a> by deleting old logs form the below mentioned location</li>
</ol>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Hdfs-site.xml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;value&gt;file:///mnt/hdfs,file:///mnt1/hdfs&lt;/value&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">yarn-site.xml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> &lt;property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;value&gt;/mnt/yarn,/mnt1/yarn&lt;/value&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;final&gt;true&lt;/final&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;description&gt;Where to store container logs.&lt;/description&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;value&gt;/var/log/hadoop-yarn/containers&lt;/value&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/property&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="5">
<li>
<p>You can  use the command <code>sudo du -h /emr | sort -hr</code> to find the files consuming space. Use this command repeatedly and backup the files and delete unwanted files.</p>
</li>
<li>
<p>You can also <a href="https://aws.amazon.com/blogs/big-data/dynamically-scale-up-storage-on-amazon-emr-clusters/" target="_blank" rel="noopener noreferrer">Dynamically scale up storage on Amazon EMR clusters</a></p>
</li>
<li>
<p>You add more EBS capacity if you encounter <a href="https://repost.aws/knowledge-center/no-space-left-on-device-emr-spark" target="_blank" rel="noopener noreferrer">no space left on device error in the spark job in Amazon EMR</a></p>
</li>
<li>
<p><a href="https://repost.aws/knowledge-center/user-cache-disk-space-emr" target="_blank" rel="noopener noreferrer">You can prevent hadoop and spark user cache consuming disk space</a></p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="525----spark-application-failure"><strong>5.2.5  - Spark application failure</strong><a href="#525----spark-application-failure" class="hash-link" aria-label="Direct link to 525----spark-application-failure" title="Direct link to 525----spark-application-failure">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5251-spark-app-class-not-found-exception">5.2.5.1 Spark app class not found exception<a href="#5251-spark-app-class-not-found-exception" class="hash-link" aria-label="Direct link to 5.2.5.1 Spark app class not found exception" title="Direct link to 5.2.5.1 Spark app class not found exception">​</a></h3>
<p>Refer to the the <a href="https://repost.aws/knowledge-center/emr-spark-classnotfoundexception" target="_blank" rel="noopener noreferrer">blog post</a> to troubleshoot spark app class not found exception.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5252-spark-app-failing-to-write-to-s3-or-spark-job-failure-due-access-denied-exception">5.2.5.2 Spark app failing to write to s3 or Spark job failure due access denied exception<a href="#5252-spark-app-failing-to-write-to-s3-or-spark-job-failure-due-access-denied-exception" class="hash-link" aria-label="Direct link to 5.2.5.2 Spark app failing to write to s3 or Spark job failure due access denied exception" title="Direct link to 5.2.5.2 Spark app failing to write to s3 or Spark job failure due access denied exception">​</a></h3>
<p><strong>Symptom</strong>: You can encounter the error <code>EmrOptimizedParquetOutputCommitter: EMR Optimized committer is not supported by this filesystem (org.apache.hadoop.hdfs.DistributedFileSystem)</code>
<code>Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services .s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied</code></p>
<p><strong>Probable Root causes:</strong></p>
<ul>
<li>
<p>Bucket policies and IAM policies related issue</p>
</li>
<li>
<p>AWS s3 ACL setting</p>
</li>
<li>
<p>S3 block Public Access setting</p>
</li>
<li>
<p>S3 object lock settings</p>
</li>
<li>
<p>VPC endpoint policy</p>
</li>
<li>
<p>AWS organization policies</p>
</li>
<li>
<p>Access point setting</p>
</li>
</ul>
<p><strong>Troubleshooting Steps:</strong></p>
<ul>
<li>
<p>Check the application logs to confirm the error Status Code: 403; Error Code<!-- -->:AccessDenied<!-- --> and check the probable root causes</p>
</li>
<li>
<p>Generate <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/get-request-ids.html" target="_blank" rel="noopener noreferrer">Amazon S3 request ID for AWS support to help in debugging the issue</a></p>
</li>
</ul>
<p><strong>Best practices and Recommendations:</strong></p>
<ul>
<li>
<p>Aws recommends using &quot;s3://&quot; instead of S3a:// S3n:// which is based on EMRFS as it is build and maintained by AWS and supports additional features.</p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/troubleshoot-403-errors.html" target="_blank" rel="noopener noreferrer">403 S3 Error troubleshooting</a></p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html#access-policies-walkthrough-example2a" target="_blank" rel="noopener noreferrer">Cross Account Bucket Access</a></p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5253-cluster-mode-job-failure">5.2.5.3 Cluster mode job failure<a href="#5253-cluster-mode-job-failure" class="hash-link" aria-label="Direct link to 5.2.5.3 Cluster mode job failure" title="Direct link to 5.2.5.3 Cluster mode job failure">​</a></h3>
<p><strong>Symptom</strong>: In the absence of correct parameters and the dependencies installed before running a job in the cluster mode, you can encounter cluster mode job failures as below:</p>
<ul>
<li>
<p>ERROR Client: Application diagnostics message: User application exited with status 1</p>
</li>
<li>
<p>exitCode: 13</p>
</li>
<li>
<p>User application exited with status 1</p>
</li>
</ul>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>
<p>Inspect the container of application master logs containers/application_xx_0002/container_xx_0002_01_000001</p>
</li>
<li>
<p>Look for the probable keywords like &quot;exitCode: 13&quot; or <code>module not found error</code> or <code>User application exited with status 1</code></p>
</li>
<li>
<p>Look into the application master container stdout and controller log of the Application master to get more information on the error.</p>
</li>
<li>
<p>Make sure to pass the right parameters with the dependencies  installed correctly before running the job in the cluster mode.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5254-spark-troubleshooting-spark-fetchfailedexceptions-memory-issues-and-scaling-problems-in-applications">5.2.5.4 Spark Troubleshooting Spark FetchFailedExceptions, Memory Issues and Scaling Problems in Applications<a href="#5254-spark-troubleshooting-spark-fetchfailedexceptions-memory-issues-and-scaling-problems-in-applications" class="hash-link" aria-label="Direct link to 5.2.5.4 Spark Troubleshooting Spark FetchFailedExceptions, Memory Issues and Scaling Problems in Applications" title="Direct link to 5.2.5.4 Spark Troubleshooting Spark FetchFailedExceptions, Memory Issues and Scaling Problems in Applications">​</a></h3>
<p><strong>Symptom</strong>:</p>
<p><code>org.apache.spark.shuffle.FetchFailedException: java.util.concurrent.TimeoutException: Timeout waiting for task.at org.</code>
<code>org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748 Caused by: java.io.IOException: Map_1: Shuffle failed with too many fetch failures and insufficient progress</code></p>
<p><strong>Probable Root cause:</strong></p>
<ol>
<li>
<p>If the executors are busy or under high GC pressure, then executor cannot cater to the shuffle request. You can use the external shuffle service hosted on Yarn Node manager, which will help any shuffle state written by an executor to be served beyond the executor’s lifetime.</p>
</li>
<li>
<p>NodeManager(NM) may have been killed and restarted multiple times while running your job, with   java.lang.OutOfMemoryError: GC overhead limit exceeded and  java.lang.OutOfMemoryError: Java heap space errors, indicating not enough heap space to address the shuffle request. As YarnShuffleService is launched by the Node Manager, insufficient memory allocated for the Node Manager can cause aggressive GC during the shuffle phase.Amazon EMR enables the External Shuffle Service by default, the shuffle output is written to local disk. Since LocalDiskEncryption is enabled, if thousands of blocks are being fetched from local disk, it could lead to higher memory utilization on Nodemanager JVM as well.</p>
</li>
</ol>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>Check the application logs, yarn resource manager logs and instance-controller logs to get more information on the error.</li>
<li>Find the node which is causing the shuffle data loss and make note of the event timeline.</li>
<li>Review the application logs of the Managed Scaling resize request cluster manager to identify the nodes included in the shrink policy. Verify whether the update to the EMR DB for writing the shrink policy was successful.</li>
<li>Check the Instance controller to exactly find new resize target and which instance being send by the cluster manager to Instance controller to keep and to Remove list</li>
<li>If node is not in toKeep but toRemove list or if they are empty although it might have shuffle data, dig into Spark Shuffle enricher task how was it added in toRemove list for log.</li>
<li>In some cases, the shuffle metrics may altogether be missing around the time of resize affecting toKeep and toRemove list.</li>
<li>Check the spark metric collector logs to find out and share with the premium support if there was an issue collecting the metrics at cluster</li>
</ol>
<p><em>Note- Emr Managed scaling is shuffle aware</em></p>
<p><strong>Configuration to consider:</strong></p>
<ul>
<li>
<p>spark.reducer.maxBlocksInFlightPerAddress:  limits the number of remote blocks being fetched per reduce task from a given host port.</p>
</li>
<li>
<p>spark.reducer.maxSizeInFlight:  Maximum size of map outputs to fetch simultaneously from each reduce task.</p>
</li>
<li>
<p>spark.shuffle.io.retryWait (Netty only): How long to wait between retries of fetches.</p>
</li>
<li>
<p>spark.shuffle.io.maxRetries (Netty only): Fetches that fail due to IO-related exceptions are automatically retried if this is set to a non-zero value.</p>
</li>
<li>
<p>spark.shuffle.io.backLog: Length of the accept queue for the shuffle service.</p>
</li>
<li>
<p>Increasing the memory for NodeManager Deamon on EMR cluster startup <code> yarn-env = YARN_NODEMANAGER_HEAPSIZE&quot;: &quot;10000&quot;</code></p>
</li>
<li>
<p>Tuning number of Partitions and Spark parallelism(<code>spark.default.parallelism</code>) for your cluster/job. This may limit the number of shuffles that&#x27;s happening per job and consequently reduce the load on all NodeManager&#x27;s.
Note that changing configuration can have performance impact.</p>
</li>
<li>
<p>Keep the EMR default <code>spark.shuffle.service.enabled true(spark-defaults.conf)</code>
<code>spark.dynamicAllocation.enabled set to true (EMR default)</code></p>
</li>
</ul>
<p><strong>Additional resource:</strong></p>
<p><a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html" target="_blank" rel="noopener noreferrer">Spark EMR configuration</a></p>
<p><a href="https://aws.amazon.com/about-aws/whats-new/2022/03/amazon-emr-managed-scaling-shuffle-data-aware/" target="_blank" rel="noopener noreferrer">Spark Shuffle Data Aware</a></p>
<p><a href="https://repost.aws/knowledge-center/emr-spark-yarn-memory-limit" target="_blank" rel="noopener noreferrer">Container killed by Yarn for exceeding memory limit troubleshooting</a></p>
<p><a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html" target="_blank" rel="noopener noreferrer">Optimize Spark performance</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="526----spark-sql-issue"><strong>5.2.6  - Spark SQL issue</strong><a href="#526----spark-sql-issue" class="hash-link" aria-label="Direct link to 526----spark-sql-issue" title="Direct link to 526----spark-sql-issue">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-query-failures-for-jupyter-notebook-python-3">Spark Query failures for jupyter notebook Python 3<a href="#spark-query-failures-for-jupyter-notebook-python-3" class="hash-link" aria-label="Direct link to Spark Query failures for jupyter notebook Python 3" title="Direct link to Spark Query failures for jupyter notebook Python 3">​</a></h3>
<p><strong>Symptom</strong>: If EMR job fails with an error message <a href="https://repost.aws/knowledge-center/emr-exit-status-100-lost-node" target="_blank" rel="noopener noreferrer">Exit status: -100. Diagnostics: Container released on a lost node</a>, it might be because of</p>
<ul>
<li>
<p>A core or task node is terminated because of high disk space utilization.</p>
</li>
<li>
<p>A node becomes unresponsive due to prolonged high CPU utilization or low available memory.</p>
</li>
</ul>
<p><strong>Troubleshooting Steps:</strong></p>
<ol>
<li>
<p>Check the following <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html#UsingEMR_ViewingMetrics_Access" target="_blank" rel="noopener noreferrer">Cloudwatch Metrics</a> for metrics MR unhealthy nodes and MR lost nodes to determine root cause.</p>
</li>
<li>
<p>Add more Amazon Elastic Block Store (Amazon EBS) capacity to the new EMR clusters. You can do this when you launch a new cluster or by modifying a running cluster. You can also add more core or task nodes. If larger EBS volumes don&#x27;t resolve the problem, <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-attaching-volume.html" target="_blank" rel="noopener noreferrer">attach more EBS volumes</a> to the core and task nodes.</p>
</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bestpractices/5 - Applications/Spark/troubleshooting_and_tuning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">5.1 - Spark General</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#521-----spark-structured-streaming-applications-have-high-connection-create-rate-to-amazon-msk" class="table-of-contents__link toc-highlight">5.2.1  -  Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK</a></li><li><a href="#522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data" class="table-of-contents__link toc-highlight"><strong>5.2.2 - spark.driver.maxResultSize error on an EMR heterogeneous cluster but the driver is not collecting data</strong></a></li><li><a href="#523-----issues-related-to-spark-logs" class="table-of-contents__link toc-highlight"><strong>5.2.3  -  Issues related to Spark logs</strong></a><ul><li><a href="#5231-spark-events-logs-not-getting-pushed-to-s3" class="table-of-contents__link toc-highlight">5.2.3.1 Spark events logs not getting pushed to S3</a></li><li><a href="#5232----spark-history-server-not-opening-up" class="table-of-contents__link toc-highlight">5.2.3.2  - Spark History Server not opening up</a></li><li><a href="#5233---spark-application-failure-or-spark-streaming-application-failure-with-137-error-code" class="table-of-contents__link toc-highlight">5.2.3.3  -Spark application failure or Spark streaming application failure with 137 error code</a></li><li><a href="#5234---spark-application-event-with-incomplete-logs" class="table-of-contents__link toc-highlight">5.2.3.4  -Spark application event with incomplete logs</a></li><li><a href="#5235---access-spark-history-server-for-old-cluster" class="table-of-contents__link toc-highlight">5.2.3.5  -Access spark history server for old cluster</a></li></ul></li><li><a href="#524----issues-related-to-spark-session" class="table-of-contents__link toc-highlight"><strong>5.2.4  - Issues Related to Spark session</strong></a><ul><li><a href="#5241---spark-session-not-getting-generated-with-missing-std-out" class="table-of-contents__link toc-highlight">5.2.4.1  -Spark session not getting generated with Missing std out</a></li><li><a href="#5242---spark-session-with-livy-is-failing" class="table-of-contents__link toc-highlight">5.2.4.2  -Spark session with livy is failing</a></li><li><a href="#5243---spark-application-failure-due-to-disk-space-or-cpu-load" class="table-of-contents__link toc-highlight">5.2.4.3  -Spark application failure due to Disk space or CPU Load</a></li></ul></li><li><a href="#525----spark-application-failure" class="table-of-contents__link toc-highlight"><strong>5.2.5  - Spark application failure</strong></a><ul><li><a href="#5251-spark-app-class-not-found-exception" class="table-of-contents__link toc-highlight">5.2.5.1 Spark app class not found exception</a></li><li><a href="#5252-spark-app-failing-to-write-to-s3-or-spark-job-failure-due-access-denied-exception" class="table-of-contents__link toc-highlight">5.2.5.2 Spark app failing to write to s3 or Spark job failure due access denied exception</a></li><li><a href="#5253-cluster-mode-job-failure" class="table-of-contents__link toc-highlight">5.2.5.3 Cluster mode job failure</a></li><li><a href="#5254-spark-troubleshooting-spark-fetchfailedexceptions-memory-issues-and-scaling-problems-in-applications" class="table-of-contents__link toc-highlight">5.2.5.4 Spark Troubleshooting Spark FetchFailedExceptions, Memory Issues and Scaling Problems in Applications</a></li></ul></li><li><a href="#526----spark-sql-issue" class="table-of-contents__link toc-highlight"><strong>5.2.6  - Spark SQL issue</strong></a><ul><li><a href="#spark-query-failures-for-jupyter-notebook-python-3" class="table-of-contents__link toc-highlight">Spark Query failures for jupyter notebook Python 3</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws/aws-emr-best-practices/tree/main" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
</body>
</html>