<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bestpractices/Applications/Spark/best_practices" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">5.1 - Spark General | AWS Open Data Analytics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" name="twitter:image" content="https://aws.github.io/aws-emr-best-practices/img/AWS_logo_RGB.png"><meta data-rh="true" property="og:url" content="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="5.1 - Spark General | AWS Open Data Analytics"><meta data-rh="true" name="description" content="BP 5.1.1  -  Use the most recent version of EMR"><meta data-rh="true" property="og:description" content="BP 5.1.1  -  Use the most recent version of EMR"><link data-rh="true" rel="icon" href="/aws-emr-best-practices/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices" hreflang="en"><link data-rh="true" rel="alternate" href="https://aws.github.io/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices" hreflang="x-default"><link rel="stylesheet" href="/aws-emr-best-practices/assets/css/styles.7a6c5961.css">
<script src="/aws-emr-best-practices/assets/js/runtime~main.9d84fab9.js" defer="defer"></script>
<script src="/aws-emr-best-practices/assets/js/main.3ce9c081.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/aws-emr-best-practices/"><div class="navbar__logo"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/aws-emr-best-practices/img/logo.svg" alt="AWS Open Data Analytics" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AWS Open Data Analytics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/aws-emr-best-practices/docs/bestpractices/">Best Practices</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/benchmarks/introduction">Benchmarks</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/utilities/">Utilities</a><a class="navbar__item navbar__link" href="/aws-emr-best-practices/docs/migration/introduction">Migration</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/aws-emr-best-practices/docs/bestpractices/">EMR Best Practices Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/Introduction">Cost Optimizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Reliability/introduction">Reliability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Security/introduction">Security</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction">Applications</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction">Hbase</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Hive/introduction">Hive</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Spark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices">5.1 - Spark General</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning">5.2 - Spark troubleshooting and performance tuning</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction">Architecture</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/aws-emr-best-practices/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Applications</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Spark</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">5.1 - Spark General</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>5.1 - Spark General</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-511-----use-the-most-recent-version-of-emr">BP 5.1.1  -  Use the most recent version of EMR<a href="#bp-511-----use-the-most-recent-version-of-emr" class="hash-link" aria-label="Direct link to BP 5.1.1  -  Use the most recent version of EMR" title="Direct link to BP 5.1.1  -  Use the most recent version of EMR">​</a></h2>
<p>Amazon EMR provides several Spark optimizations out of the box with <a href="https://aws.amazon.com/blogs/big-data/run-apache-spark-3-0-workloads-1-7-times-faster-with-amazon-emr-runtime-for-apache-spark/" target="_blank" rel="noopener noreferrer">EMR Spark runtime</a> which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to <a href="https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-emr-6-4-supports-apache-spark-3-1-2/" target="_blank" rel="noopener noreferrer">improve</a> the performance of this Spark runtime engine for new releases. Several <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html" target="_blank" rel="noopener noreferrer">optimizations</a> such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference).</p>
<p><img loading="lazy" alt="BP - 33" src="/aws-emr-best-practices/assets/images/spark-bp-33-4ce8c6991aac1dde4636a948b1c4d96c.png" width="752" height="449" class="img_ev3q"></p>
<p>As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits.</p>
<p>Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of <a href="https://aws.amazon.com/emr/serverless/" target="_blank" rel="noopener noreferrer">EMR Serverless</a> (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws --region us-east-1 emr-serverless create-application \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --release-label emr-6.5.0-preview \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --type &#x27;SPARK&#x27; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --name spark-6.5.0-demo-application</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Detailed documentation for running Spark jobs using EMR Serverless can be found <a href="https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/spark-jobs.html" target="_blank" rel="noopener noreferrer">here</a>. Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-512-----determine-right-infrastructure-for-your-spark-workloads">BP 5.1.2  -  Determine right infrastructure for your Spark workloads<a href="#bp-512-----determine-right-infrastructure-for-your-spark-workloads" class="hash-link" aria-label="Direct link to BP 5.1.2  -  Determine right infrastructure for your Spark workloads" title="Direct link to BP 5.1.2  -  Determine right infrastructure for your Spark workloads">​</a></h2>
<p>Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html" target="_blank" rel="noopener noreferrer">several instance types</a> to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memory-optimized">Memory-optimized<a href="#memory-optimized" class="hash-link" aria-label="Direct link to Memory-optimized" title="Direct link to Memory-optimized">​</a></h3>
<p>Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively.</p>
<p><img loading="lazy" alt="BP - 1" src="/aws-emr-best-practices/assets/images/spark-bp-1-d68f174e99abf50f88900af503961459.png" width="724" height="946" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cpu-optimized">CPU-optimized<a href="#cpu-optimized" class="hash-link" aria-label="Direct link to CPU-optimized" title="Direct link to CPU-optimized">​</a></h3>
<p>CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia.</p>
<p><img loading="lazy" alt="BP - 2" src="/aws-emr-best-practices/assets/images/spark-bp-2-62ac31475da4916a2887ff0cf63c657d.png" width="822" height="478" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-purpose">General purpose<a href="#general-purpose" class="hash-link" aria-label="Direct link to General purpose" title="Direct link to General purpose">​</a></h3>
<p>General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU<!-- -->:Memory<!-- --> ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU<!-- -->:memory<!-- --> ratio based on your workload needs.</p>
<table><thead><tr><th>Instance Type</th><th>Instance</th><th>EC2 price</th><th>EMR price</th><th>Cores</th><th>Memory in GiB</th><th>CPU<!-- -->:memory<!-- --> ratio</th></tr></thead><tbody><tr><td>Compute</td><td>c5.18xlarge</td><td>$3.06</td><td>$0.27</td><td>72</td><td>144</td><td>2</td></tr><tr><td>Memory</td><td>r5.12xlarge</td><td>$3.02</td><td>$0.27</td><td>48</td><td>384</td><td>8</td></tr><tr><td>General</td><td>m5.16xlarge</td><td>$3.07</td><td>$0.27</td><td>64</td><td>256</td><td>4</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="storage-optimized">Storage-optimized<a href="#storage-optimized" class="hash-link" aria-label="Direct link to Storage-optimized" title="Direct link to Storage-optimized">​</a></h3>
<p>Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-instances">GPU instances<a href="#gpu-instances" class="hash-link" aria-label="Direct link to GPU instances" title="Direct link to GPU instances">​</a></h3>
<p>GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-rapids.html" target="_blank" rel="noopener noreferrer">Nvidia RAPIDS accelerator</a> plugin to improve your GPU instance performance without any changes to your code or data processing pipelines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="graviton-instances">Graviton instances<a href="#graviton-instances" class="hash-link" aria-label="Direct link to Graviton instances" title="Direct link to Graviton instances">​</a></h3>
<p>Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance.</p>
<p><img loading="lazy" alt="BP - 3" src="/aws-emr-best-practices/assets/images/spark-bp-3-433c6f0dee92f98974818ec632a7c8a0.png" width="1836" height="818" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-513-----choose-the-right-deploy-mode">BP 5.1.3  -  Choose the right deploy mode<a href="#bp-513-----choose-the-right-deploy-mode" class="hash-link" aria-label="Direct link to BP 5.1.3  -  Choose the right deploy mode" title="Direct link to BP 5.1.3  -  Choose the right deploy mode">​</a></h2>
<p>Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application&#x27;s Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="client-deploy-mode">Client deploy mode<a href="#client-deploy-mode" class="hash-link" aria-label="Direct link to Client deploy mode" title="Direct link to Client deploy mode">​</a></h3>
<p>This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed.</p>
<p><img loading="lazy" alt="BP - 4" src="/aws-emr-best-practices/assets/images/spark-bp-4-0ac9c7cc0bb2cff70ac01efa69455604.png" width="1576" height="712" class="img_ev3q"></p>
<p>Use this deploy mode if:</p>
<ul>
<li>You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention.</li>
<li>You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications.</li>
<li>You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application.</li>
<li>You want to write output to the console i.e., send the results back to the client program where you submitted your application.</li>
<li>Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-deploy-mode">Cluster deploy mode<a href="#cluster-deploy-mode" class="hash-link" aria-label="Direct link to Cluster deploy mode" title="Direct link to Cluster deploy mode">​</a></h3>
<p>In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from.</p>
<p><img loading="lazy" alt="BP - 5" src="/aws-emr-best-practices/assets/images/spark-bp-5-2bc36e065f3f17ca9e604e73484b03d0.png" width="1470" height="762" class="img_ev3q"></p>
<p>Use cluster deploy mode if:</p>
<ul>
<li>You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes.</li>
<li>There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors.</li>
<li>You are saving results in S3/HDFS and there is no need to print output to the console.</li>
<li>You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes.</li>
<li>You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property <em><code>spark.yarn.maxAppAttempts</code></em>. You can increase this value further if needed.</li>
<li>You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property <em><code>spark.yarn.submit.waitAppCompletion</code></em> is set to &quot;false&quot;.</li>
</ul>
<p>Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-514-----use-right-file-formats-and-compression-type">BP 5.1.4  -  Use right file formats and compression type<a href="#bp-514-----use-right-file-formats-and-compression-type" class="hash-link" aria-label="Direct link to BP 5.1.4  -  Use right file formats and compression type" title="Direct link to BP 5.1.4  -  Use right file formats and compression type">​</a></h2>
<p>Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support.</p>
<p>When writing Parquet files to S3, EMR Spark will use <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3-optimized-committer.html" target="_blank" rel="noopener noreferrer">EMRFSOutputCommitter</a> which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc.</p>
<p>Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors.</p>
<p><img loading="lazy" alt="BP - 6" src="/aws-emr-best-practices/assets/images/spark-bp-6-b9a7158171cdee924ec4f8676f11c0e5.png" width="590" height="266" class="img_ev3q"></p>
<p>Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults.</p>
<p><img loading="lazy" alt="BP - 7" src="/aws-emr-best-practices/assets/images/spark-bp-7-89937807f0d272059b3daa865ad322da.png" width="507" height="282" class="img_ev3q"></p>
<p>You can also apply columnar encryption on Parquet files using KMS:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.encryption.kms.client.class&quot; ,&quot;org.apache.parquet.crypto.keytools.mocks.InMemoryKMS&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.encryption.key.list&quot; ,&quot;keyA:AAECAwQFBgcICQoLDA0ODw ,  keyB:AAECAAECAAECAAECAAECAA&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Activate Parquet encryption, driven by Hadoop properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sc.hadoopConfiguration.set(&quot;parquet.crypto.factory.class&quot; ,&quot;org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Write encrypted dataframe files.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Column &quot;square&quot; will be protected with master key &quot;keyA&quot;.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Parquet file footers will be protected with master key &quot;keyB&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">squaresDF.write.option(&quot;parquet.encryption.column.keys&quot; , &quot;keyA:square&quot;).option(&quot;parquet.encryption.footer.key&quot; , &quot;keyB&quot;).parquet(&quot;/path/to/table.parquet.encrypted&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Read encrypted dataframe files</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df2 = spark.read.parquet(&quot;/path/to/table.parquet.encrypted&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-515-----partitioning">BP 5.1.5  -  Partitioning<a href="#bp-515-----partitioning" class="hash-link" aria-label="Direct link to BP 5.1.5  -  Partitioning" title="Direct link to BP 5.1.5  -  Partitioning">​</a></h2>
<p>Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans.</p>
<p>You can choose one or more partition fields from your dataset or table columns based on:</p>
<ul>
<li>Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field.</li>
<li>Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions).</li>
<li>Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields.</li>
<li>File sizes per partition. It is recommended that your individual file sizes within each partition are &gt;=128 MB.</li>
</ul>
<p>The number of shuffle partitions will determine the number of output files per table partition.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df.repartition(400).write.partitionBy(&quot;datecol&quot;).parquet(&quot;s3://bucket/output/&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property <em><code>spark.sql.shuffle.partitions</code></em>. You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property <em><code>spark.sql.files.maxPartitionBytes</code></em>.</p>
<p>Partitioning ensures that <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic" target="_blank" rel="noopener noreferrer">dynamic partition pruning</a> takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark.</p>
<p>For example, following query will push partition filters for better performance. <code>l_shipdate</code> and <code>l_shipmode</code> are partition fields of the table &quot;testdb.lineitem_shipmodesuppkey_part&quot;.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.sql(&quot;select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate=&#x27;1993-12-03&#x27; and l_shipmode=&#x27;SHIP&#x27;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.queryExecution.toString</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Printing the query execution plan where we can see pushed filters for the two partition fields in where clause:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AdaptiveSparkPlan isFinalPlan=true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- == Final Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   *(2) HashAggregate(keys=[], functions=[count(1)], output=[count(1)#320])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- ShuffleQueryStage 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#198]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         +- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#318L])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            +- *(1) Project</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">               +- *(1) ColumnarToRow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  +- FileScan orc testdb.lineitem_shipmodesuppkey_part[l_shipdate#313,l_shipmode#314] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[s3://vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- == Initial Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   HashAggregate(keys=[], functions=[count(1)], output=[count(1)#320])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#179]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#318L])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         +- Project</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            +- FileScan orc testdb.lineitem_shipmodesuppkey_part[l_shipdate#313,l_shipmode#314] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[s3://vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-516----tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources">BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources<a href="#bp-516----tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources" class="hash-link" aria-label="Direct link to BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources" title="Direct link to BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources">​</a></h2>
<p>Amazon EMR configures Spark defaults during the cluster launch based on your cluster&#x27;s infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning.</p>
<p>For a starting point, generally, it is advisable to set <em><code>spark.executor.cores</code></em> to 4 or 5 and tune <em><code>spark.executor.memory</code></em> around this value. Also, when you calculate the <em><code>spark.executor.memory</code></em>, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of <em><code>spark.executor.memory</code></em>). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations.</p>
<p>Based on <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html" target="_blank" rel="noopener noreferrer">Task Configurations</a> r4.8xlarge node has YARN memory of 241664 MB (based on the value of <em><code>yarn.nodemanager.resource.memory-mb</code></em>). The instance has 32 vCores. If we set <em><code>spark.executor.cores</code></em> as 4, we can run 8 executors at a time. So, the configurations will be following.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.executor.cores = 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor = 0.1875)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each.</p>
<p>Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation" target="_blank" rel="noopener noreferrer">maximizeResourceAllocation</a>. Setting this property to &quot;true&quot; will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed.</p>
<p>After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener noreferrer">Spark JMX metrics</a> provides JMX level visibility which is the best way to determine resource utilization.</p>
<p>While using instance fleets, it is generally advisable to request worker nodes with similar vCore<!-- -->:memory<!-- --> ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration.</p>
<p>To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different.</p>
<table><thead><tr><th>Instance</th><th>YARN memory in MB</th></tr></thead><tbody><tr><td>c5.4xlarge</td><td>24576</td></tr><tr><td>c5.12xlarge</td><td>90112</td></tr><tr><td>m5.4xlarge</td><td>57344</td></tr><tr><td>m5.12xlarge</td><td>188416</td></tr><tr><td>r5.4xlarge</td><td>122880</td></tr><tr><td>r5.12xlarge</td><td>385024</td></tr></tbody></table>
<p>Now, let us calculate executor memory after setting <em><code>spark.executor.cores</code></em> = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by <em><code>spark.executor.cores</code></em> to get the total container size -&gt; 24576 / 4 = 6144.</p>
<p><em><code>spark.executor.memory</code></em> = 6144 - (6144 * 0.1875) = 4992 MB</p>
<p>Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources.</p>
<p>In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property <em><code>spark.yarn.heterogeneousExecutors.enabled</code></em> and is set to &quot;true&quot; by default. Further, you will be able to control the maximum resources allocated to each executor with properties <em><code>spark.executor.maxMemory</code></em> and <em><code>spark.executor.maxCores</code></em>. Minimum resources are calculated with <em><code>spark.executor.cores</code></em> and <em><code>spark.executor.memory</code></em>. For uniform instance groups or for flexible fleets with instance types having similar vCore<!-- -->:memory<!-- --> ratio, you can try setting <em><code>spark.yarn.heterogeneousExecutors.enabled</code></em> to &quot;false&quot; and see if you get better performance.</p>
<p>Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn&#x27;t matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions:</p>
<ol>
<li>Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver.</li>
<li>Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune <em><code>spark.driver.maxResultSize</code></em>.</li>
</ol>
<p>You can use smaller driver memory (or use the default <em><code>spark.driver.memory</code></em>) if you are running multiple jobs in parallel.</p>
<p>Now, coming to <em><code>spark.sql.shuffle.partitions</code></em> for Dataframes and Datasets and <em><code>spark.default.parallelism</code></em> for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI.</p>
<p><img loading="lazy" alt="BP - 16" src="/aws-emr-best-practices/assets/images/spark-bp-16-9c85d0b01740ac27e75ba9195a5a1af7.png" width="331" height="961" class="img_ev3q"></p>
<p>From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce <em><code>spark.sql.shuffle.partitions</code></em> to increase partition size during the exchange.</p>
<p>Apart from this, if you want to use tools to receive tuning suggestions, consider using <a href="https://aws.amazon.com/blogs/big-data/tune-hadoop-and-spark-performance-with-dr-elephant-and-sparklens-on-amazon-emr/" target="_blank" rel="noopener noreferrer">Sparklens and Dr. Elephant</a> with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-517----use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas">BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas<a href="#bp-517----use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="hash-link" aria-label="Direct link to BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas" title="Direct link to BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas">​</a></h2>
<p>Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .builder</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .appName(&quot;my spark application name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .config(getConfig)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  // use this if you need to increment Kryo buffer size. Default 64k</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .config(&quot;spark.kryoserializer.buffer&quot;, &quot;1024k&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  // use this if you need to increment Kryo buffer max size. Default 64m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;1024m&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  /*</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  * Use this if you need to register all Kryo required classes.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .config(&quot;spark.kryo.registrationRequired&quot;, &quot;true&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  .getOrCreate</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val conf = new SparkConf()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        conf.registerKryoClasses(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.myPackage.FlightDataset],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.myPackage.BookingDataset],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[scala.collection.mutable.WrappedArray.ofRef[_]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.types.StructType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[Array[org.apache.spark.sql.types.StructType]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.types.StructField],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[Array[org.apache.spark.sql.types.StructField]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.StringType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.LongType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.BooleanType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.DoubleType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.types.Metadata],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.types.ArrayType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.execution.joins.UnsafeHashedRelation&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.catalyst.InternalRow],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[Array[org.apache.spark.sql.catalyst.InternalRow]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.catalyst.expressions.UnsafeRow],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.execution.joins.LongHashedRelation&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.execution.joins.LongToUnsafeRowMap&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.util.collection.BitSet],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[org.apache.spark.sql.types.DataType],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            classOf[Array[org.apache.spark.sql.types.DataType]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.NullType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.IntegerType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.types.TimestampType$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;scala.collection.immutable.Set$EmptySet$&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;scala.reflect.ClassTag$$anon$1&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Class.forName(&quot;java.lang.Class&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can also optionally fine tune the following Kryo configs :-</p>
<p><em><code>spark.kryo.unsafe</code></em> - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet).
<em><code>spark.kryoserializer.buffer.max</code></em> - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m.
<em><code>spark.kryoserializer.buffer</code></em> - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-518------tune-garbage-collector">BP 5.1.8  -   Tune Garbage Collector<a href="#bp-518------tune-garbage-collector" class="hash-link" aria-label="Direct link to BP 5.1.8  -   Tune Garbage Collector" title="Direct link to BP 5.1.8  -   Tune Garbage Collector">​</a></h2>
<p>By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility.</p>
<p>Following is the spark configuration:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.executor.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#x27;kill -9 %p&#x27;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.driver.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError=&#x27;kill -9 %p&#x27;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters <a href="https://www.oracle.com/technical-resources/articles/java/g1gc.html" target="_blank" rel="noopener noreferrer">here</a> for G1GC and <a href="https://docs.oracle.com/en/java/javase/11/gctuning/parallel-collector1.html" target="_blank" rel="noopener noreferrer">here</a> for ParallelGC. Some useful ones are below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">-XX:ConcGCThreads=n</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:ParallelGCThreads=n</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:InitiatingHeapOccupancyPercent=45</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-XX:MaxGCPauseMillis=200</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can monitor GC performance using Spark UI. The GC time should be ideally <code>&lt;=</code> 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance.</p>
<p><img loading="lazy" alt="BP - 8" src="/aws-emr-best-practices/assets/images/spark-bp-8-68da14d9be672fcadb43a9d7bbdf5cc1.png" width="956" height="602" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-519------use-optimal-apis-wherever-possible">BP 5.1.9  -   Use optimal APIs wherever possible<a href="#bp-519------use-optimal-apis-wherever-possible" class="hash-link" aria-label="Direct link to BP 5.1.9  -   Use optimal APIs wherever possible" title="Direct link to BP 5.1.9  -   Use optimal APIs wherever possible">​</a></h2>
<p>When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="repartition-vs-coalesce">repartition vs coalesce<a href="#repartition-vs-coalesce" class="hash-link" aria-label="Direct link to repartition vs coalesce" title="Direct link to repartition vs coalesce">​</a></h3>
<p>Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df.coalesce(1) //instead of df.repartition(1)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="groupbykey-vs-reducebykey">groupByKey vs reduceByKey<a href="#groupbykey-vs-reducebykey" class="hash-link" aria-label="Direct link to groupByKey vs reduceByKey" title="Direct link to groupByKey vs reduceByKey">​</a></h3>
<p>Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="orderby-vs-sortby-or-sortwithinpartitions">orderBy vs sortBy or sortWithinPartitions<a href="#orderby-vs-sortby-or-sortwithinpartitions" class="hash-link" aria-label="Direct link to orderBy vs sortBy or sortWithinPartitions" title="Direct link to orderBy vs sortBy or sortWithinPartitions">​</a></h3>
<p>orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5110-----leverage-spot-nodes-with-managed-autoscaling">BP 5.1.10 -   Leverage spot nodes with managed autoscaling<a href="#bp-5110-----leverage-spot-nodes-with-managed-autoscaling" class="hash-link" aria-label="Direct link to BP 5.1.10 -   Leverage spot nodes with managed autoscaling" title="Direct link to BP 5.1.10 -   Leverage spot nodes with managed autoscaling">​</a></h2>
<p>Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity.</p>
<p>Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes.</p>
<p><img loading="lazy" alt="BP - 9" src="/aws-emr-best-practices/assets/images/spark-bp-9-9c701af5768238592a1283385ad7044b.png" width="502" height="319" class="img_ev3q"></p>
<p>Following experimentation illustrates the performance gains using Managed Autoscaling.</p>
<p><img loading="lazy" alt="BP - 10" src="/aws-emr-best-practices/assets/images/spark-bp-10-184ae0ba08403078004fe72e94783ad8.png" width="817" height="203" class="img_ev3q"></p>
<p>For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads.</p>
<p><img loading="lazy" alt="BP - 11" src="/aws-emr-best-practices/assets/images/spark-bp-11-2a5fc7f91f958c598571794609f9a11b.png" width="817" height="233" class="img_ev3q"></p>
<p>If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5111------for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation">BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation<a href="#bp-5111------for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="hash-link" aria-label="Direct link to BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation" title="Direct link to BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation">​</a></h2>
<p>Dynamic allocation is enabled in EMR by default. It is a great feature for following cases:</p>
<ol>
<li>Workloads processing variable amount of data</li>
<li>When your cluster uses autoscaling</li>
<li>Dynamic processing requirements or unpredictable workload patterns</li>
<li>Streaming and ad-hoc workloads</li>
<li>When your cluster runs multiple concurrent applications</li>
<li>Your cluster is long-running</li>
</ol>
<p>The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where:</p>
<ol>
<li>Workloads have a very predicatable pattern</li>
<li>Amount of data processed is predictable and consistent throughout the application</li>
<li>Cluster runs Spark application in batch mode</li>
<li>Clusters are transient and are of fixed size (no autoscaling)</li>
<li>Application processing is relatively uniform. Workload is not spikey in nature.</li>
</ol>
<p>For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs.</p>
<p>For such use cases, you can consider disabling dynamic allocation along with setting the precise  number and size of executors and cores like below.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.dynamicAllocation.enabled&quot;: &quot;false&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.executor.instances&quot;: &quot;12&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.executor.memory&quot;: &quot;8G&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.executor.cores&quot;: &quot;4&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well.</p>
<p>However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5112------leverage-hdfs-as-temporary-storage-for-io-intensive-workloads">BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads<a href="#bp-5112------leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="hash-link" aria-label="Direct link to BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads" title="Direct link to BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads">​</a></h2>
<p>Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes.</p>
<p><img loading="lazy" alt="BP - 12" src="/aws-emr-best-practices/assets/images/spark-bp-12-de4d3cdee6e9295bbcb2c93aa97fe9e3.png" width="508" height="99" class="img_ev3q"></p>
<p>For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors.</p>
<p><img loading="lazy" alt="BP - 13" src="/aws-emr-best-practices/assets/images/spark-bp-13-ce40b366b2751caef72fe9874cfc475b.png" width="852" height="203" class="img_ev3q"></p>
<p>Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size.</p>
<p><img loading="lazy" alt="BP - 14" src="/aws-emr-best-practices/assets/images/spark-bp-14-2d94786b60420eb612071c0bfb6ff2ed.png" width="901" height="202" class="img_ev3q"></p>
<p>However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations.</p>
<p>Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d’s and r6gd’s instead of r5s and r6g’s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5113------spark-speculation-with-emrfs">BP 5.1.13  -   Spark speculation with EMRFS<a href="#bp-5113------spark-speculation-with-emrfs" class="hash-link" aria-label="Direct link to BP 5.1.13  -   Spark speculation with EMRFS" title="Direct link to BP 5.1.13  -   Spark speculation with EMRFS">​</a></h2>
<p>In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, <em><code>spark.speculation</code></em> is turned off. Only enable <em><code>spark.speculation</code></em> if you are doing one of the following.</p>
<ul>
<li>Writing Parquet files to S3 using EMRFSOutputCommitter</li>
<li>Using HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCp</li>
<li>Using HDFS as storage (not recommended)</li>
</ul>
<p>Do not enable <em><code>spark.speculation</code></em> if none of the above criteria is met since it will lead to incorrect or missing or duplicate data.</p>
<p>You can consider enabling <em><code>spark.speculation</code></em> especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, <em><code>spark.speculation</code></em> will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met).</p>
<p>You can set <em><code>spark.speculation</code></em> to true in spark-defaults or pass it as a command line option (--conf <em><code>spark.speculation</code></em>=&quot;true&quot;).</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.speculation&quot;: &quot;true&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please do not enable <em><code>spark.speculation</code></em> if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5114-----data-quality-and-integrity-checks-with-deequ">BP 5.1.14 -   Data quality and integrity checks with deequ<a href="#bp-5114-----data-quality-and-integrity-checks-with-deequ" class="hash-link" aria-label="Direct link to BP 5.1.14 -   Data quality and integrity checks with deequ" title="Direct link to BP 5.1.14 -   Data quality and integrity checks with deequ">​</a></h2>
<p>Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running.</p>
<p>In order to check data integrity, consider using <a href="https://github.com/awslabs/deequ" target="_blank" rel="noopener noreferrer">Deequ</a> for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads.</p>
<p><a href="https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/" target="_blank" rel="noopener noreferrer">Test data quality at scale with Deequ | AWS Big Data Blog</a></p>
<p><a href="https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/" target="_blank" rel="noopener noreferrer">Testing data quality at scale with PyDeequ | AWS Big Data Blog</a></p>
<p>Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on &quot;Status&quot; and check for error message of failed tasks. If you are seeing too many random unexpected errors such as &quot;ArrayIndexOutOfBounds&quot; or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5115-----use-dataframes-wherever-possible">BP 5.1.15 -   Use DataFrames wherever possible<a href="#bp-5115-----use-dataframes-wherever-possible" class="hash-link" aria-label="Direct link to BP 5.1.15 -   Use DataFrames wherever possible" title="Direct link to BP 5.1.15 -   Use DataFrames wherever possible">​</a></h2>
<p>WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example -</p>
<ul>
<li>Datasets perform many serializations and deserializations that Dataframes tries to skip.</li>
<li>Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced.</li>
<li>Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes.</li>
</ul>
<p>Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">case class DeviceIoTData (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  battery_level: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  c02_level: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cca2: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cca3: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cn: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  device_id: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  device_name: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  humidity: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  latitude: Double,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  longitude: Double,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  scale: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  temp: Long,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  timestamp: Long</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5116------data-skew">BP 5.1.16  -   Data Skew<a href="#bp-5116------data-skew" class="hash-link" aria-label="Direct link to BP 5.1.16  -   Data Skew" title="Direct link to BP 5.1.16  -   Data Skew">​</a></h2>
<p>Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues.</p>
<p><img loading="lazy" alt="BP - 15" src="/aws-emr-best-practices/assets/images/spark-bp-15-e6d784e6f83c9981cf097478055217bd.png" width="1660" height="1002" class="img_ev3q"></p>
<p>When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="salting">Salting<a href="#salting" class="hash-link" aria-label="Direct link to Salting" title="Direct link to Salting">​</a></h3>
<p>Salting is one of the most common skew mitigation techniques where you add a &quot;salt&quot; to the skewed column say &quot;col1&quot;. You can split it into multiple columns like &quot;col1_0&quot;,&quot;col1_1&quot;,&quot;col1_2&quot; and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved.</p>
<p>Original data</p>
<p><img loading="lazy" alt="BP - 17" src="/aws-emr-best-practices/assets/images/spark-bp-17-115f9dd7daddd1e59b3447a66eff58e7.png" width="643" height="512" class="img_ev3q"></p>
<p>Salted 4  times</p>
<p><img loading="lazy" alt="BP - 18" src="/aws-emr-best-practices/assets/images/spark-bp-18-90689f7887ef2c50750b2ccf8815b180.png" width="702" height="512" class="img_ev3q"></p>
<p>Salted 8 times</p>
<p><img loading="lazy" alt="BP - 19" src="/aws-emr-best-practices/assets/images/spark-bp-19-6f06b4ea27d0af7b875eae0acfdbcc8d.png" width="681" height="512" class="img_ev3q"></p>
<p>A typical Salting workflow looks like below:</p>
<p><img loading="lazy" alt="BP - 20" src="/aws-emr-best-practices/assets/images/spark-bp-20-c7dcbdbaec548fa391bc8da4b1a8d491.png" width="1674" height="658" class="img_ev3q"></p>
<p>For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms).</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">n = 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">salted_df = df.withColumn(&quot;salt&quot;, (rand * n).cast(IntegerType))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now, aggregation is performed on this salt column and the results are reduced by keys</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unsalted_df = salted_df.groupBy(&quot;salt&quot;, groupByFields).agg(aggregateFields).groupBy(groupByFields).agg(aggregateFields)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Similar logic can  be applied for windowing functions as well.</p>
<p>A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="isolated-salting">Isolated Salting<a href="#isolated-salting" class="hash-link" aria-label="Direct link to Isolated Salting" title="Direct link to Isolated Salting">​</a></h3>
<p>In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we  will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results.</p>
<p>Isolated Salting workflow looks like below:</p>
<p><img loading="lazy" alt="BP - 21" src="/aws-emr-best-practices/assets/images/spark-bp-21-ce2628117a834c5411ad2e9aad06574e.png" width="1839" height="760" class="img_ev3q"></p>
<p>Example code looks like below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val count = 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val salted = df.withColumn(&quot;salt&quot;, when(&#x27;col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val replicaDF = skewDF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .withColumn(&quot;replica&quot;, when(&#x27;col === &quot;A&quot;, (0 until count) toArray) otherwise Array(0))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .withColumn(&quot;salt&quot;, explode(&#x27;replica&#x27;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .drop(&#x27;replica&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val merged = salted.join(replicaDF, joinColumns :+ &quot;salt&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="isolated-broadcast-join">Isolated broadcast join<a href="#isolated-broadcast-join" class="hash-link" aria-label="Direct link to Isolated broadcast join" title="Direct link to Isolated broadcast join">​</a></h3>
<p>In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the ”normal” keys and perform map-side join on isolated ”skewed” keys. Finally, we can merge the results of skewed and normal joins</p>
<p>Isolated map-side join workflow looks like below:</p>
<p><img loading="lazy" alt="BP - 22" src="/aws-emr-best-practices/assets/images/spark-bp-22-fa916500caa5d75f93573d718bc31b81.png" width="1827" height="672" class="img_ev3q"></p>
<p>Example code looks like below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val count = 8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val salted = skewDF.withColumn(&quot;salt&quot;, when(&#x27;col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0).repartition(&#x27;col&#x27;, &#x27;salt&#x27;) // Re-partition to remove skew</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val broadcastDF = salted.join(broadcast(sourceDF), &quot;symbol&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hashing-for-sparksql-queries">Hashing for SparkSQL queries<a href="#hashing-for-sparksql-queries" class="hash-link" aria-label="Direct link to Hashing for SparkSQL queries" title="Direct link to Hashing for SparkSQL queries">​</a></h3>
<p>While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes.</p>
<p>Following could be an example query working on top of a skewed dataset.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">union</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">union</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">union</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">limit 10;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5117-----choose-the-right-type-of-join">BP 5.1.17  -  Choose the right type of join<a href="#bp-5117-----choose-the-right-type-of-join" class="hash-link" aria-label="Direct link to BP 5.1.17  -  Choose the right type of join" title="Direct link to BP 5.1.17  -  Choose the right type of join">​</a></h2>
<p>There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join &quot;hints&quot; as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="broadcast-join">Broadcast Join<a href="#broadcast-join" class="hash-link" aria-label="Direct link to Broadcast Join" title="Direct link to Broadcast Join">​</a></h3>
<p>Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (<code>=</code>) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is <code>&lt;=</code> 10 MB, this join strategy is chosen. This is based on the parameter <em><code>spark.sql.autoBroadcastJoinThreshold</code></em> which is defaulted to 10 MB.</p>
<p>If one of your join tables are larger than 10 MB, you can either modify <em><code>spark.sql.autoBroadcastJoinThreshold</code></em> or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI.</p>
<p><img loading="lazy" alt="BP - 24" src="/aws-emr-best-practices/assets/images/spark-bp-24-43d629db3a1b7057bc1ffe17856f33da.png" width="962" height="1024" class="img_ev3q"></p>
<p>Please note that you should not use this join if your &quot;small&quot; table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is <code>&lt;</code>1 GB.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-merge-join">Sort Merge Join<a href="#sort-merge-join" class="hash-link" aria-label="Direct link to Sort Merge Join" title="Direct link to Sort Merge Join">​</a></h3>
<p>This is the most common join used by Spark. If you are joining two large tables (&gt;10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI.</p>
<p><img loading="lazy" alt="BP - 25" src="/aws-emr-best-practices/assets/images/spark-bp-25-8a43b70381ca0b9cff1ed6c13867dcfc.png" width="2074" height="1002" class="img_ev3q"></p>
<p>Spark configuration <em><code>spark.sql.join.preferSortMergeJoin</code></em> is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets.</p>
<p>Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is &lt; 1 GB. All join types are supported.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-hash-join">Shuffle Hash Join<a href="#shuffle-hash-join" class="hash-link" aria-label="Direct link to Shuffle Hash Join" title="Direct link to Shuffle Hash Join">​</a></h3>
<p>Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless <em><code>spark.sql.join.preferSortMergeJoin</code></em> is set to &quot;false&quot; or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="broadcast-nested-loop-join">Broadcast Nested Loop Join<a href="#broadcast-nested-loop-join" class="hash-link" aria-label="Direct link to Broadcast Nested Loop Join" title="Direct link to Broadcast Nested Loop Join">​</a></h3>
<p>Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions <code>(&lt;,&gt;,&lt;=,&gt;=,</code>like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice.</p>
<p><img loading="lazy" alt="BP - 26" src="/aws-emr-best-practices/assets/images/spark-bp-26-9369fa8eadbad45328a8e1bd12eb5fa1.png" width="566" height="1041" class="img_ev3q"></p>
<p>Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable.</p>
<p>For example, below code leads to Broadcast Nested Loop Join.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val df1 = spark.sql(&quot;select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate=&#x27;1993-12-03&#x27; and l_shipmode=&#x27;SHIP&#x27;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df2 = spark.sql(&quot;select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate=&#x27;1993-12-04&#x27; and l_shipmode=&#x27;SHIP&#x27;&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val nestedLoopDF = df1.join(df2, df1(&quot;l_partkey&quot;) === df2(&quot;l_partkey&quot;) || df1(&quot;l_linenumber&quot;) === df2(&quot;l_linenumber&quot;))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Instead, you can change the code like below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val result1 = df1.join(df2, df1(&quot;l_partkey&quot;) === df2(&quot;l_partkey&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val result2 = df1.join(df2, df1(&quot;l_linenumber&quot;) === df2(&quot;l_linenumber&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val resultDF = result1.union(result2)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met <em><code>spark.sql.autoBroadcastJoinThreshold</code></em>.</p>
<p><img loading="lazy" alt="BP - 27" src="/aws-emr-best-practices/assets/images/spark-bp-27-784c14346630e55195ee3d15e50a71c0.png" width="797" height="869" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cartesian-join">Cartesian Join<a href="#cartesian-join" class="hash-link" aria-label="Direct link to Cartesian Join" title="Direct link to Cartesian Join">​</a></h3>
<p>Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running &quot;inner like&quot; queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">for l_key in lhs_table:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  for r_key in rhs_table:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    #Execute join condition</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance.</p>
<p>Following code will lead to a Cartesian product provided the tables do not meet <em><code>spark.sql.autoBroadcastJoinThreshold</code></em>.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val crossJoinDF = df1.join(df2, df1(&quot;l_partkey&quot;) &gt;= df2(&quot;l_partkey&quot;))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img loading="lazy" alt="BP - 28" src="/aws-emr-best-practices/assets/images/spark-bp-28-4b66f297da8d09208ad1a2045826e07d.png" width="734" height="1023" class="img_ev3q"></p>
<p>Now, passing a broadcast hint which leads to Broadcast Nested Loop Join</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val crossJoinDF = df1.join(broadcast(df2), df1(&quot;l_partkey&quot;) &gt;= df2(&quot;l_partkey&quot;))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img loading="lazy" alt="BP - 29" src="/aws-emr-best-practices/assets/images/spark-bp-29-ed4917851a48603ef36e10c00f9268c6.png" width="582" height="967" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5118----consider-spark-blacklisting-for-large-clusters">BP 5.1.18  - Consider Spark Blacklisting for large clusters<a href="#bp-5118----consider-spark-blacklisting-for-large-clusters" class="hash-link" aria-label="Direct link to BP 5.1.18  - Consider Spark Blacklisting for large clusters" title="Direct link to BP 5.1.18  - Consider Spark Blacklisting for large clusters">​</a></h2>
<p>Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14.</p>
<p>This blacklisting is enabled by default in Amazon EMR with the <em><code>spark.blacklist.decommissioning.enabled</code></em> property set to true. You can control the time for which the node is blacklisted using <em><code>spark.blacklist.decommissioning.timeout property</code></em>, which is set to 1 hour by default, equal to the default value for <em><code>yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs</code></em>. It is recommended to set <em><code>spark.blacklist.decommissioning.timeout</code></em> to a value equal to or greater than <em><code>yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs</code></em> to make sure that Amazon EMR blacklists the node for the entire decommissioning period.</p>
<p>Following are some <em>experimental</em> blacklisting properties.</p>
<p><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code> determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2.</p>
<p><code>spark.blacklist.task.maxTaskAttemptsPerNode</code> determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2.</p>
<p><code>spark.blacklist.stage.maxFailedTasksPerExecutor</code> is same as <em><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code></em> but the executor is blacklisted for the entire stage.</p>
<p><code>spark.blacklist.stage.maxFailedExecutorsPerNode</code> determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2.</p>
<p><code>spark.blacklist.application.maxFailedTasksPerExecutor</code> is same as <em><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code></em> but the executor is blacklisted for the entire application.</p>
<p><code>spark.blacklist.application.maxFailedExecutorsPerNode</code> is same as <em><code>spark.blacklist.stage.maxFailedExecutorsPerNode</code></em> but the worker node is blacklisted for the entire application.</p>
<p><code>spark.blacklist.killBlacklistedExecutors</code> when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring.</p>
<p><code>spark.blacklist.application.fetchFailure.enabled</code> when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring.</p>
<p>The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;spark-defaults&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.blacklist.killBlacklistedExecutors&quot;: &quot;true&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;spark.blacklist.application.fetchFailure.enabled&quot;: &quot;true&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs.</p>
<p><img loading="lazy" alt="BP - 30" src="/aws-emr-best-practices/assets/images/spark-bp-30-40f380cf8cfd58a0c805cfc1e1bdcb6d.png" width="826" height="580" class="img_ev3q"></p>
<p>When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by <em><code>spark.stage.maxConsecutiveAttempts</code></em>. This is determined by the setting <em><code>spark.stage.attempt.ignoreOnDecommissionFetchFailure</code></em> being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5119----debugging-and-monitoring-spark-applications">BP 5.1.19  - Debugging and monitoring Spark applications<a href="#bp-5119----debugging-and-monitoring-spark-applications" class="hash-link" aria-label="Direct link to BP 5.1.19  - Debugging and monitoring Spark applications" title="Direct link to BP 5.1.19  - Debugging and monitoring Spark applications">​</a></h2>
<p>EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR.</p>
<ol>
<li>Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates.</li>
<li>Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated.</li>
<li>EMR Persistent UI - Amazon EMR provides <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html" target="_blank" rel="noopener noreferrer">Persistent User Interface for Spark</a>. This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active.</li>
</ol>
<p>Spark UI options are  also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on &quot;stderr&quot; or &quot;stdout&quot; from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs.</p>
<p><img loading="lazy" alt="BP - 31" src="/aws-emr-best-practices/assets/images/spark-bp-31-7576cd74ba8fad114ba8137e02b265e6.png" width="3083" height="1096" class="img_ev3q"></p>
<p>Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step&#x27;s stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live.</p>
<p><img loading="lazy" alt="BP - 32" src="/aws-emr-best-practices/assets/images/spark-bp-32-9a0b22dbd1c6e2b07a469cdc53926c37.png" width="728" height="457" class="img_ev3q"></p>
<p>If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property <em><code>yarn.log-aggregation.retain-seconds</code></em>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5120-----spark-observability-platforms">BP 5.1.20  -  Spark Observability Platforms<a href="#bp-5120-----spark-observability-platforms" class="hash-link" aria-label="Direct link to BP 5.1.20  -  Spark Observability Platforms" title="Direct link to BP 5.1.20  -  Spark Observability Platforms">​</a></h2>
<p>Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener noreferrer">here</a>. You can choose an observability platform based on your requirements. Following are some example native options.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-managed-services-for-prometheus-and-grafana">Amazon Managed Services for Prometheus and Grafana<a href="#amazon-managed-services-for-prometheus-and-grafana" class="hash-link" aria-label="Direct link to Amazon Managed Services for Prometheus and Grafana" title="Direct link to Amazon Managed Services for Prometheus and Grafana">​</a></h3>
<p>AWS offers <a href="https://aws.amazon.com/prometheus/" target="_blank" rel="noopener noreferrer">Amazon Managed Prometheus (AMP)</a> which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. <a href="https://aws.amazon.com/grafana/" target="_blank" rel="noopener noreferrer">Amazon Managed Grafana (AMG)</a> is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the <a href="https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/" target="_blank" rel="noopener noreferrer">deployment instructions</a> available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener noreferrer">PrometheusServlet</a> and <a href="https://github.com/prometheus/jmx_exporter" target="_blank" rel="noopener noreferrer">prometheus/jmx_exporter</a>. However, some bootstrapping is necessary for this integration.</p>
<p>###Amazon Opensearch
<a href="https://aws.amazon.com/blogs/opensource/introducing-opensearch/" target="_blank" rel="noopener noreferrer">Amazon Opensearch</a> is a community-driven open source fork of <a href="https://aws.amazon.com/blogs/opensource/stepping-up-for-a-truly-open-source-elasticsearch/" target="_blank" rel="noopener noreferrer">Elasticsearch and Kibana</a>. It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs.</p>
<p>Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5121-----potential-resolutions-for-not-so-common-errors">BP 5.1.21  -  Potential resolutions for not-so-common errors<a href="#bp-5121-----potential-resolutions-for-not-so-common-errors" class="hash-link" aria-label="Direct link to BP 5.1.21  -  Potential resolutions for not-so-common errors" title="Direct link to BP 5.1.21  -  Potential resolutions for not-so-common errors">​</a></h2>
<p>Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions.</p>
<p>###Potential strategies to mitigate S3 throttling errors
For mitigating S3 throttling errors (503: Slow Down), consider increasing <em><code>fs.s3.maxRetries</code></em> in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;classification&quot;: &quot;emrfs-site&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;fs.s3.maxRetries&quot;: &quot;20&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;fs.s3n.multipart.uploads.split.size&quot;: &quot;268435456&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Consider using Iceberg format <a href="https://iceberg.apache.org/docs/latest/aws/#object-store-file-layout" target="_blank" rel="noopener noreferrer">ObjectStoreLocationProvider</a> to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job&#x27;s processing requirements increase and thus help mitigate the S3 throttling errors.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> CREATE TABLE my_catalog.my_ns.my_table</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> ( id bigint,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   data string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   category string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   USING iceberg OPTIONS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ( &#x27;write.object-storage.enabled&#x27;=true,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     &#x27;write.data.path&#x27;=&#x27;s3://my-table-data-bucket&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     PARTITIONED BY (category);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> 2021-11-01 05:39:24  809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 2021-11-01 06:00:10    6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 2021-11-01 04:33:24    6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProvider<a href="https://github.com/vasveena/IcebergPOC/blob/main/Iceberg-EMR-POC.ipynb" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: <code>s3://bucket/000-fff/ or s3://bucket/&lt;date fields from 2020-01-20 to 2030-01-20&gt;/.</code></p>
<p>###Precautions to take while running too many executors
If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>For this issue, you can increase <em><code>spark.scheduler.listenerbus.eventqueue.size</code></em> from default of 10000 to 2x or more until you do not see dropped events anymore.</p>
<p>Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing <em><code>spark.executor.heartbeatInterval</code></em> from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources.</p>
<p>If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing <em><code>spark.yarn.containerLauncherMaxThreads</code></em> which is defaulted to 25. You may also want to increase <em><code>spark.yarn.am.memory</code></em> (default: 512 MB) and <em><code>spark.yarn.am.cores</code></em> (default: 1).</p>
<p>###Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows
You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster.</p>
<p>In hadoop-env.sh, you can see heap sizes for HDFS daemons.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export HADOOP_OPTS=&quot;$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export HADOOP_NAMENODE_HEAPSIZE=25190</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export HADOOP_DATANODE_HEAPSIZE=4096</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In yarn-env.sh, you can see heap sizes for YARN daemons.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export YARN_NODEMANAGER_HEAPSIZE=2048</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export YARN_RESOURCEMANAGER_HEAPSIZE=7086</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like &quot;MissingBlocksException&quot; in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f0beb662000, 12288, 0) failed; error=&#x27;Cannot allocate memory&#x27; (errno=12)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># There is insufficient memory for the Java Runtime Environment to continue.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># An error report file with more information is saved as:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># /tmp/hs_err_pid14730.log</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Classification&quot;: &quot;hadoop-env&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Configurations&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;Classification&quot;: &quot;export&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;Properties&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          &quot;HADOOP_DATANODE_OPTS&quot;: &quot;-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          “HADOOP_DATANODE_HEAPSIZE”: &quot;4096&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;Configurations&quot;: []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Additionally, you can also consider reducing <em><code>yarn.nodemanager.resource.memory-mb</code></em> by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from <em><code>yarn.nodemanager.resource.memory-mb</code></em> for your instance types.</p>
<p>###Precautions to take for highly concurrent workloads</p>
<p>When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like “Caused by: java.util.zip.ZipException: error in opening zip file” or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:45 INFO Client: Preparing resources for our AM container</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:48 INFO Client: Uploading resource file:/mnt/tmp/spark-b0fe28f9-17e5-42da-ab8a-5c861d81e25b/__spark_libs__3016570917637060246.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/__spark_libs__3016570917637060246.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:49 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/hive-site.xml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:49 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/pyspark.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:49 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/py4j-0.10.9-src.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:39:50 INFO Client: Uploading resource file:/mnt/tmp/spark-b0fe28f9-17e5-42da-ab8a-5c861d81e25b/__spark_conf__7549408525505552236.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/__spark_conf__.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set <em><code>spark.yarn.archive</code></em> to that location. Below is an example:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">zip -r spark-dependencies.zip /mnt/jars/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hdfs dfs -mkdir /user/hadoop/deps/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>/mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.yarn.archive  hdfs:///user/hadoop/deps/spark-dependencies.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can see that this file size is large.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-rw-r--r--   1 hadoop hdfsadmingroup  287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node&#x27;s CPU to ensure that the utilization has been brought down.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Preparing resources for our AM container</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Source and destination file systems are the same. Not copying hdfs:/user/hadoop/deps/spark-dependencies.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/hive-site.xml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/pyspark.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/py4j-0.10.9-src.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">22/02/25 21:56:08 INFO Client: Uploading resource file:/mnt/tmp/spark-0fbfb5a9-7c0c-4f9f-befd-3c8f56bc4688/__spark_conf__5472705335503774914.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/__spark_conf__.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop’s <a href="https://github.com/apache/hadoop/blob/branch-2.10.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java#L227" target="_blank" rel="noopener noreferrer">ShutdownHook</a>. If this happens, increase <em><code>hadoop.service.shutdown.timeout</code></em> property from 30s to to a larger value.</p>
<p>Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5122-----how-the-number-of-partitions-are-determined-when-reading-a-raw-file">BP 5.1.22  -  How the number of partitions are determined when reading a raw file<a href="#bp-5122-----how-the-number-of-partitions-are-determined-when-reading-a-raw-file" class="hash-link" aria-label="Direct link to BP 5.1.22  -  How the number of partitions are determined when reading a raw file" title="Direct link to BP 5.1.22  -  How the number of partitions are determined when reading a raw file">​</a></h2>
<p>When reading a raw file, that can be a text file, csv, etc. the count behind the number of partitions created from Spark depends from many variables as the methods used to read the file, the default parallelism and so on. Following an overview of how these factors are related between each other so to better understand how files are processed.</p>
<p>Here a brief summary of relationship between core nodes - executors - tasks:</p>
<ul>
<li>each File is composed by blocks that will be parsed according to the InputFormat corresponding to the specific data format, and generally combines several blocks into one input slice, called InputSplit</li>
<li>InputSplit and Task are one-to-one correspondence relationship</li>
<li>each of these specific Tasks will be assigned to one executor of the nodes on the cluster</li>
<li>each node can have one or more Executors, depending on the node resources and executor settings</li>
<li>each Executor consists of cores and memory whose default is based on the node type. Each executor can only execute one task at time.</li>
</ul>
<p>So based on that, the number of threads/tasks will be based on the number of partitions while reading.</p>
<p>Please note that the S3 connector takes some configuration option (e.g. s3a: fs.s3a.block.size) to simulate blocks in Hadoop services, but the concept of blocks in S3 does not really exists. Unlike HDFS that is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior, EMRFS is an object store, not a file system. For more information, see Hadoop documentation for <a href="https://hadoop.apache.org/docs/stable2/hadoop-project-dist/hadoop-common/filesystem/introduction.html#Object_Stores_vs._Filesystems" target="_blank" rel="noopener noreferrer">Object Stores vs. Filesystems</a>.</p>
<p>Now, there are several factors that dictate how a dataset or file is mapped to a partition. First is the method used to read the file (e.g. text file), that changes if you&#x27;re working with rdds or dataframes:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sc.textFile(...) returns a RDD[String]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   textFile(String path, int minPartitions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   file system URI, and return it as an RDD of Strings.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.read.text(...) returns a DataSet[Row] or a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   text(String path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Loads text files and returns a DataFrame whose schema starts with a string column named &quot;value&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   and followed by partitioned columns if there are any.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-core-api-rdds">Spark Core API (RDDs)<a href="#spark-core-api-rdds" class="hash-link" aria-label="Direct link to Spark Core API (RDDs)" title="Direct link to Spark Core API (RDDs)">​</a></h3>
<p>When using <em><code>sc.textFile</code></em> Spark uses the block size set for the filesysytem protocol it&#x27;s reading from, to calculate the number of partitions in input:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/core/src/main/scala/org/apache/spark/SparkContext.scala#L819-L832" target="_blank" rel="noopener noreferrer">SparkContext.scala</a></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  /</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * Read a text file from HDFS, a local file system (available on all nodes), or any</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * Hadoop-supported file system URI, and return it as an RDD of Strings.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @param path path to the text file on a supported file system</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @param minPartitions suggested minimum number of partitions for the resulting RDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * @return RDD of lines of the text file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  def textFile(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      path: String,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    assertNotStopped()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><a href="https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L373" target="_blank" rel="noopener noreferrer">FileInputFormat.java</a></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">        if (isSplitable(fs, path)) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          long blockSize = file.getBlockSize();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>When using the <em>S3A</em> protocol the block size is set through the <em><code>fs.s3a.block.size parameter</code></em> (default 32M), and when using <em>S3</em> protocol through <em><code>fs.s3n.block.size</code></em> (default 64M). Important to notice here is that with <em>S3</em> protocol the parameter used is <em><code>fs.s3n.block.size</code></em> and not <em><code>fs.s3.block.size</code></em> as you would expect. In EMR indeed, when using EMRFS, which means using <em>s3</em> with <em>s3://</em> prefix, <em><code>fs.s3.block.size</code></em> will not have any affect on the EMRFS configration.</p>
<p>Following some testing results using these parameters:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">CONF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Input: 1 file, total size 336 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 1 (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3A protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3a.block.size = 32M (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/32 = 11</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3 protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3n.block.size = 64M (default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/64 = 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 2 (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3A protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3a.block.size = 64M (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/64 = 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> S3protocol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- fs.s3n.block.size = 128M (modified)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 336/128 = 3</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-sql-dataframes">Spark SQL (DATAFRAMEs)<a href="#spark-sql-dataframes" class="hash-link" aria-label="Direct link to Spark SQL (DATAFRAMEs)" title="Direct link to Spark SQL (DATAFRAMEs)">​</a></h3>
<p>When using <em><code>spark.read.text</code></em> no. of spark tasks/partitions depends on default parallelism:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L423-L430" target="_blank" rel="noopener noreferrer">DataSourceScanExec.scala</a></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    val defaultMaxSplitBytes =</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val totalBytes = selectedPartitions.flatMap(_.files.map (_.getLen + openCostInBytes)).sum</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val bytesPerCore = totalBytes / defaultParallelism</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The default Parallelism is determined via:</p>
<p><a href="https://github.com/apache/spark/blob/v2.4.8/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L457-L459" target="_blank" rel="noopener noreferrer">CoarseGrainedSchedulerBackend.scala</a></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  override def defaultParallelism(): Int = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If <em>defaultParallelism</em> is too large, <em>bytesPerCore</em> will be small, and <em>maxSplitBytes</em> can be small, which can result in more no. of spark tasks/partitions. So if there&#x27;re more cores, <em><code>spark.default.parallelism</code></em> can be large, <em>defaultMaxSplitBytes</em> can be small, and no. of spark tasks/partitions can be large.</p>
<p>In order to tweak the input no. of partitions the following parameters need to be set:</p>
<table><thead><tr><th>Classification</th><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>spark-default</td><td><em><code>spark.default.parallelism</code></em></td><td>default: max(total number of vCores, 2)</td></tr><tr><td>spark-default</td><td><em><code>spark.sql.files.maxPartitionBytes</code></em></td><td>default: 128MB</td></tr></tbody></table>
<p>If these parameters are modified, <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation" target="_blank" rel="noopener noreferrer"><em>maximizeResourceAllocation</em></a> need to be disabled, as it would override <em><code>spark.default.parallelism parameter</code></em>.</p>
<p>Following some testing results using these parameters:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">CONF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Total number of vCores = 16 -&gt; spark.default.parallelism = 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- spark.sql.files.maxPartitionBytes = 128MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Input: 1 CSV file, total size 352,3 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Partition size = 352,3/16 = ∼22,09 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEST 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Input: 10 CSV files, total size 3523 MB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Spark no. partitions: 30</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Partition size = 3523/30 = ∼117,43 MB</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<p>Disclaimer</p>
<p>When writing a file the number of partitions in output will depends from the number of partitions in input that will be maintained if no shuffle operations are applied on the data processed, changed otherwise based on <em><code>spark.default.parallelism</code></em> for RDDs and <em><code>spark.sql.shuffle.partitions</code></em> for dataframes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5123----common-heath-checks-recommendations">BP 5.1.23  - Common heath checks recommendations<a href="#bp-5123----common-heath-checks-recommendations" class="hash-link" aria-label="Direct link to BP 5.1.23  - Common heath checks recommendations" title="Direct link to BP 5.1.23  - Common heath checks recommendations">​</a></h2>
<p>Below are the recommended steps to monitor the health of your cluster:</p>
<p>●	Resource Manager UI : Yarn is the resource manager of the EMR cluster. You can access the Resource Manage persistent UI to get a high level cluster metrics like Apps submitted, Apps Pending, Containers Running,Physical Mem Used % etc. You can also check Cluster node level metric, Scheduler Metrics and application level information and access the Resource Manager UI from the application tab of EMR cluster.</p>
<p>●	NameNode UI: NameNode is the HDFS master. You can access this UI to get information on HDFS status such as Configured Capacity, DFS Used, DFS Remaining, Live Nodes,Decommissioning Nodes. Datanode information tab tells the status of the datanode,datanode volume failures, snapshot and startup progress. The UI also gives utilities like metrics, log level information etc about the HDFS cluster. You can access the namenode UI from the application tab of EMR cluster</p>
<p>●	You can get cluster performance graphs from the Monitoring Tab, the metrics there are of three categories, Cluster Status, Node Status and Inputs and outputs.</p>
<p>Cloudwatch Metrics : <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html#UsingEMR_ViewingMetrics_MetricsReported" target="_blank" rel="noopener noreferrer">EMR cluster Metrics</a> can be monitored while key metrics for lookout are YarnavailableMemoryPercentage, IsIdel, ContainerPendingRatio, CoreNodesPending and CoreNodesRunning. You can create custom <a href="https://repost.aws/knowledge-center/emr-custom-metrics-cloudwatch" target="_blank" rel="noopener noreferrer">cloudwatch metrics as well</a>.</p>
<p>●	EMR metrics Dashboard can be created directly form the EMR monitoring tab or from the Cloudwatch by picking and choosing the EMR metric for the dashboard.</p>
<p>●	Set alarms by specifying metrics and conditions. Search the EMR cluster that you would like to create an alarm on then select the metric of EMR. Select the statistics and time period. Then select the condition for the alarm. Select the Alarm trigger, create or choose the SNS topic, subscribe to the SNS topic. You will get an email for the confirmation, confirm the subscription of the topic. Name the alarm and select create alarm.</p>
<p>●	<a href="https://repost.aws/knowledge-center/emr-troubleshoot-failed-spark-jobs" target="_blank" rel="noopener noreferrer">Why did my Spark job in Amazon EMR fail?</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bp-5124-----support-reach-out-best-practice">BP 5.1.24  -  Support Reach Out Best Practice<a href="#bp-5124-----support-reach-out-best-practice" class="hash-link" aria-label="Direct link to BP 5.1.24  -  Support Reach Out Best Practice" title="Direct link to BP 5.1.24  -  Support Reach Out Best Practice">​</a></h2>
<p>Try to troubleshoot your spark issues based on the above steps mentioned in the document and refer to additional <a href="https://repost.aws/knowledge-center/emr-troubleshoot-failed-spark-jobs" target="_blank" rel="noopener noreferrer">troubleshooting steps. Reach out to Support if needed by following the below best practices:</a></p>
<ol>
<li>Make sure to open the Support Case using an IAM User / Role in the account(s) with affected resources. Cross Account Support is provided for some customers.</li>
<li>Open a support case with right severity level and clearly identify the business impact, urgency and add sufficient contact details in the ‘Description’ field in your ticket.</li>
<li>NEVER include keys, credentials, passwords, or other sensitive information.</li>
<li>Provide the system impact explanation and include necessary details such as logs, regions, AZ’s instance ID’s, Cluster Ids, resource ARNs, etc.</li>
<li>For a faster response use the Chat / Phone options, and use a dedicated resource on your end who will be the point of contact. Escalate to your Technical Account Manager (TAM) if needed.</li>
</ol>
<p>Sample template that can be used while raising support case for AWS EMR service</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Issue Details:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Cluster ID:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Error timeline:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Use case Description:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Similar/new occurrence</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Please attach the logs as needed. Find the below logs and log location:</p>
<table><thead><tr><th>Log Name</th><th>Log Location</th></tr></thead><tbody><tr><td>Container logs</td><td><code>&lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/containers </code></td></tr><tr><td>Step logs</td><td><code>&lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/&lt;Step-ID&gt;/stederr</code></td></tr><tr><td>Driver logs</td><td>S3 bucket → Container → <code>/application_id/container_XXX_YYYY_00001</code> In case of client mode → step id and check the step logs which will have driver logs.</td></tr><tr><td>Executor logs</td><td>Container → <code>/application_id/container_XXX_YYYY_0000X</code></td></tr><tr><td>YARN ResourceManager (Master Node) logs</td><td>node/application/yarn → yarn-resourcemanager-XXX.gz</td></tr><tr><td>NodeManager Logs(Core Node) logs</td><td><code>sudo stop spark-history-server</code> <code>sudo start spark-history-server</code> Amazon EMR 5.30.0 and later release versions <code>systemctl stop spark-history-server</code> <code>systemctl start spark-history-server</code></td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bestpractices/5 - Applications/Spark/best_practices.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">5.2 - Spark troubleshooting and performance tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#bp-511-----use-the-most-recent-version-of-emr" class="table-of-contents__link toc-highlight">BP 5.1.1  -  Use the most recent version of EMR</a></li><li><a href="#bp-512-----determine-right-infrastructure-for-your-spark-workloads" class="table-of-contents__link toc-highlight">BP 5.1.2  -  Determine right infrastructure for your Spark workloads</a><ul><li><a href="#memory-optimized" class="table-of-contents__link toc-highlight">Memory-optimized</a></li><li><a href="#cpu-optimized" class="table-of-contents__link toc-highlight">CPU-optimized</a></li><li><a href="#general-purpose" class="table-of-contents__link toc-highlight">General purpose</a></li><li><a href="#storage-optimized" class="table-of-contents__link toc-highlight">Storage-optimized</a></li><li><a href="#gpu-instances" class="table-of-contents__link toc-highlight">GPU instances</a></li><li><a href="#graviton-instances" class="table-of-contents__link toc-highlight">Graviton instances</a></li></ul></li><li><a href="#bp-513-----choose-the-right-deploy-mode" class="table-of-contents__link toc-highlight">BP 5.1.3  -  Choose the right deploy mode</a><ul><li><a href="#client-deploy-mode" class="table-of-contents__link toc-highlight">Client deploy mode</a></li><li><a href="#cluster-deploy-mode" class="table-of-contents__link toc-highlight">Cluster deploy mode</a></li></ul></li><li><a href="#bp-514-----use-right-file-formats-and-compression-type" class="table-of-contents__link toc-highlight">BP 5.1.4  -  Use right file formats and compression type</a></li><li><a href="#bp-515-----partitioning" class="table-of-contents__link toc-highlight">BP 5.1.5  -  Partitioning</a></li><li><a href="#bp-516----tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources" class="table-of-contents__link toc-highlight">BP 5.1.6 -  Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources</a></li><li><a href="#bp-517----use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas" class="table-of-contents__link toc-highlight">BP 5.1.7 -  Use Kryo serializer by registering custom classes especially for Dataset schemas</a></li><li><a href="#bp-518------tune-garbage-collector" class="table-of-contents__link toc-highlight">BP 5.1.8  -   Tune Garbage Collector</a></li><li><a href="#bp-519------use-optimal-apis-wherever-possible" class="table-of-contents__link toc-highlight">BP 5.1.9  -   Use optimal APIs wherever possible</a><ul><li><a href="#repartition-vs-coalesce" class="table-of-contents__link toc-highlight">repartition vs coalesce</a></li><li><a href="#groupbykey-vs-reducebykey" class="table-of-contents__link toc-highlight">groupByKey vs reduceByKey</a></li><li><a href="#orderby-vs-sortby-or-sortwithinpartitions" class="table-of-contents__link toc-highlight">orderBy vs sortBy or sortWithinPartitions</a></li></ul></li><li><a href="#bp-5110-----leverage-spot-nodes-with-managed-autoscaling" class="table-of-contents__link toc-highlight">BP 5.1.10 -   Leverage spot nodes with managed autoscaling</a></li><li><a href="#bp-5111------for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation" class="table-of-contents__link toc-highlight">BP 5.1.11  -   For workloads with predictable pattern, consider disabling dynamic allocation</a></li><li><a href="#bp-5112------leverage-hdfs-as-temporary-storage-for-io-intensive-workloads" class="table-of-contents__link toc-highlight">BP 5.1.12  -   Leverage HDFS as temporary storage for I/O intensive workloads</a></li><li><a href="#bp-5113------spark-speculation-with-emrfs" class="table-of-contents__link toc-highlight">BP 5.1.13  -   Spark speculation with EMRFS</a></li><li><a href="#bp-5114-----data-quality-and-integrity-checks-with-deequ" class="table-of-contents__link toc-highlight">BP 5.1.14 -   Data quality and integrity checks with deequ</a></li><li><a href="#bp-5115-----use-dataframes-wherever-possible" class="table-of-contents__link toc-highlight">BP 5.1.15 -   Use DataFrames wherever possible</a></li><li><a href="#bp-5116------data-skew" class="table-of-contents__link toc-highlight">BP 5.1.16  -   Data Skew</a><ul><li><a href="#salting" class="table-of-contents__link toc-highlight">Salting</a></li><li><a href="#isolated-salting" class="table-of-contents__link toc-highlight">Isolated Salting</a></li><li><a href="#isolated-broadcast-join" class="table-of-contents__link toc-highlight">Isolated broadcast join</a></li><li><a href="#hashing-for-sparksql-queries" class="table-of-contents__link toc-highlight">Hashing for SparkSQL queries</a></li></ul></li><li><a href="#bp-5117-----choose-the-right-type-of-join" class="table-of-contents__link toc-highlight">BP 5.1.17  -  Choose the right type of join</a><ul><li><a href="#broadcast-join" class="table-of-contents__link toc-highlight">Broadcast Join</a></li><li><a href="#sort-merge-join" class="table-of-contents__link toc-highlight">Sort Merge Join</a></li><li><a href="#shuffle-hash-join" class="table-of-contents__link toc-highlight">Shuffle Hash Join</a></li><li><a href="#broadcast-nested-loop-join" class="table-of-contents__link toc-highlight">Broadcast Nested Loop Join</a></li><li><a href="#cartesian-join" class="table-of-contents__link toc-highlight">Cartesian Join</a></li></ul></li><li><a href="#bp-5118----consider-spark-blacklisting-for-large-clusters" class="table-of-contents__link toc-highlight">BP 5.1.18  - Consider Spark Blacklisting for large clusters</a></li><li><a href="#bp-5119----debugging-and-monitoring-spark-applications" class="table-of-contents__link toc-highlight">BP 5.1.19  - Debugging and monitoring Spark applications</a></li><li><a href="#bp-5120-----spark-observability-platforms" class="table-of-contents__link toc-highlight">BP 5.1.20  -  Spark Observability Platforms</a><ul><li><a href="#amazon-managed-services-for-prometheus-and-grafana" class="table-of-contents__link toc-highlight">Amazon Managed Services for Prometheus and Grafana</a></li></ul></li><li><a href="#bp-5121-----potential-resolutions-for-not-so-common-errors" class="table-of-contents__link toc-highlight">BP 5.1.21  -  Potential resolutions for not-so-common errors</a></li><li><a href="#bp-5122-----how-the-number-of-partitions-are-determined-when-reading-a-raw-file" class="table-of-contents__link toc-highlight">BP 5.1.22  -  How the number of partitions are determined when reading a raw file</a><ul><li><a href="#spark-core-api-rdds" class="table-of-contents__link toc-highlight">Spark Core API (RDDs)</a></li><li><a href="#spark-sql-dataframes" class="table-of-contents__link toc-highlight">Spark SQL (DATAFRAMEs)</a></li></ul></li><li><a href="#bp-5123----common-heath-checks-recommendations" class="table-of-contents__link toc-highlight">BP 5.1.23  - Common heath checks recommendations</a></li><li><a href="#bp-5124-----support-reach-out-best-practice" class="table-of-contents__link toc-highlight">BP 5.1.24  -  Support Reach Out Best Practice</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws/aws-emr-best-practices/tree/main" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
</body>
</html>