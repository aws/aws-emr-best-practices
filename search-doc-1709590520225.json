{"searchDocs":[{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/introduction","content":"Introduction The purpose of this guide is to provide a methodology for running Spark benchmarks on EMR. By following this guide, you will be able to identify the lowest price-performance option for running Spark workloads, considering various variables such as engine type (EMR, OSS), deployment models (EC2, EKS, Serverless), or hardware options (M, C, R, family). The focus of this guide is on price-performance. Other considerations, such as features, user experience, or compatibility with other services, are out of scope. However, it's essential to evaluate these aspects based on your customers' use cases and needs.","keywords":"","version":"Next"},{"title":"Benchmarking Checklist","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Running/benchmarking_checklist","content":"Benchmarking Checklist ##Environment and Infrastructure Checklist** The following checklist assumes you are running benchmarks across deployment models (EC2 vs EKS vs Serverless) or vendors (EMR vs Databricks vs OSS). Comparing at the deployment model or vendor level takes into consideration a number of variables such as runtime performance, scaling and pricing model. If running a benchmark for other purposes such as difference in hardware within the same deployment model, items in the checklist will not apply. Checklist NotesAre all instances On Demand\t🔲\tSpot interruptions are unpredictable and impacts price-performance. Only use spot when taking into consideration how your benchmark handles spot interruptions and getting spot capacity. Deployment models EMR on EC2 have product differentiators that select instances with are most likely to not get interrupted. Are all instances the same family, size and generation\t🔲\tThe total amount of compute (vCPU and Memory) should be consistent across benchmark runs. Compute will determine the performance of the application. Additionally, instances can vary in network performance. Additionally, if using Karpenter or Instancefleet, you should ensure the set of instances provided are the same. Note that depending on when the job is submitted, your results may vary If cluster scaling is enabled, does each deployment model have the same scaling configurations. (min, max)\t🔲\tThe efficiency of scaling between deployment models and vendors can differ but the configurations as it relates to compute should be consistent Is the EMR cluster or image using the latest EMR version?\t🔲\tThe latest versions of EMR will contain the best runtime performance Are the Application versions the same across deployment models, OSS and vendors?\t🔲\tSpark versions should be the same or the latest version that's offered Is the same data catalog being used across benchmarks?\t🔲\tPerformance between local and remote hivemetastore and glue data catalog can differ Is the infrastructure being deployed in the same AZ?\t🔲\tAZ's may have differences in network latency or instance availability. Are the benchmarks starting from the same state and size. For example, cold start vs warm pool and the # of starting instances\t🔲\tInitializing compute resources impact price-performance. When comparing benchmarking, ensure applications are starting from the same state Is the amount and type of local disk consistent?\t🔲\tSize and type of local disk volumes impact workloads, especially shuffle heavy ones Are the security settings consistent across deployment models ? This includes IAM role, security groups, data and in transit encryption\t🔲\tSecurity configurations such as encryption can impact performance Are network settings consistent across deployment models?\t🔲\tThis includes VPC endpoints, NAT Gateways, public or private endpoints, or proxies. The flow of network traffic to access storage, catalog or endpoints impacts performance Are there differences in the AMI, bootstrap actions or container Image?\t🔲\tThis can impact compute initialization as well as job startup. For example, eliminating the need to load a specific library before executing the job Are JDK settings consistent across deployment models\t🔲\tWe've seen improved performance with JDK17. Ensure the versions are consistent across benchmarks ##Workload Checklist** Checklist NotesIs the input and output data the same (size, location, type, structure)?\t🔲\tAs a best practice, all benchmark runs should point to the same input data set Are the applications being submitted the same?\t🔲\tSQL file or application should be the same Are the applications libraries the same?\t🔲\tThis includes external libraries, python versions, or anything the application requires to run Are the applications parameters the same?\t🔲\tThese are application specific parameters passed in the job. These should be identical to ensure the same job is running Are the applications configurations the same?\t🔲\tThis refers to Spark configuration settings such as executor size, shuffle partitions or Dynamic Resource Allocation settings Is EMR using EMRFS library to write to S3\t🔲\tTo take advantage of EMR's optimized run time, EMRFS (s3://) should be used. s3a is not supported and should only be used in OSS If an Open Table Format (OTF) is being used, is it consistent across benchmarks\t🔲\tUsing OTF's can improve read, write and processing performance. Is the application running in isolation?\t🔲\tResource contention can impact benchmark results because Spark workloads will run on any resource that is available. A best practice is to run each job independently. Also ensure that if submitting multiple jobs, jobs are submitted in the same sequence or sequentially. Is there any data or library caching that impacts future runs?\t🔲\tGenerally, the first run will be slower than future runs because of caching. Keep this in mind when determining how many iterations of a run you want to do. Additional runs will negate any impact of caching but has a trade off of cost and time Is the applications JVM settings the same?\t🔲\tPerformance is different across JDK version. JDK17 has seen to have the best performance. JVM settings also extend to GC settings. Is the applications logging configurations the same?\t🔲\tLogging parameters that are not the same such as level (DEBUG, INFO) can impact performance or storage requirements Are the applications being submitted the same way?\t🔲\tEnsure the entry point for job submission is the same. There are many ways to submit spark jobs such as EMR APIs, Livy, Airflow, Spark-submit. These can result in differences with how jobs are run","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/","content":"","keywords":"","version":"Next"},{"title":"Contributing​","type":1,"pageTitle":"Introduction","url":"/aws-emr-best-practices/docs/bestpractices/#contributing","content":" We encourage you to contribute to these guides. If you have implemented a practice that has proven to be effective, please share it with us by opening an issue or a pull request. Similarly, if you discover an error or flaw in the guidance we've already published, please submit a PR to correct it. ","version":"Next","tagName":"h2"},{"title":"Benchmarking Utilities","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Utilities","content":"Benchmarking Utilities EMR Spark Benchmark: https://github.com/aws-samples/emr-spark-benchmark EMR on EKS Benchmark: https://github.com/aws-samples/emr-on-eks-benchmark","keywords":"","version":"Next"},{"title":"Benchmarking Variables","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/benchmarking_variables","content":"Benchmarking Variables The goal of EMR benchmarking is to determine the impact of variables on price-performance. Variables can be categorized as either Controlled or Independent. Independent variables are manipulated in the benchmark and are the ones that change. Controlled variables are kept consistent to accurately measure the effect of the independent variables. The purpose of your benchmark will determine which variables are considered independent or controlled. For example, if I wanted to benchmark the difference in price-performance between OSS Spark and EMR Spark, my independent variables would be the OSS and EMR Spark runtime engines, while my controlled variables would include workload, hardware type, input/output data, and purchasing options. However, if I wanted to benchmark the difference in price-performance between M family instances and R family instances on EMR Spark, then hardware now becomes an independent variable, while the runtime engine becomes a controlled variable. To accurately measure the effect of the variables of interest (independent), it's important to understand which variables should be controlled and which ones should be kept consistent. The variables of interest are typically product differentiators, and only by keeping other variables consistent can you effectively measure the impact of these differentiators on price-performance. Let's examine each of these variables below. Pricing Model The pricing model refers to how workloads are billed for infrastructure, storage, and service overhead based on the usage amount. We will examine all EMR deployment models, OSS, and vendors. Infrastructure Cost\tService Cost\tStorage CostEMR on EC2\t- Price dependent on Infrastructure Size - Billed per-second, with a one-minute minimum\t- Price dependent on Infrastructure Size - Billed per-second, with a one-minute minimum\t- Standard EBS pricing dependent on size of EBS volumes attached to instances EMR on EKS\t- Price dependent on Infrastructure Size - Billed per-second, with a one-minute minimum\t- vCPU and memory resources used from the time you start to download your EMR application image until the EKS Pod terminates, rounded up to the nearest second. Pricing is based on requested vCPU and memory resources for the Task or Pod.\t- Standard EBS pricing dependent on size of EBS volumes attached to instances/pods EMR Serverless\tN/A\t- aggregate vCPU, memory, and storage resources used from the time workers are ready to run your workload until the time they stop, rounded up to the nearest second with a 1-minute minimum\t- 20 GB of ephemeral storage is available for all workers by default—you pay only for any additional storage that you configure per worker. Databricks\t- Price dependent on Infrastructure Size - Billed per-second, with a one-minute minimum\t- Databricks has multiple compute types. SQL, All Purpose ETL, ML and more. Each compute type has a different price per Databricks Billing Unit (DBU) depending on the features offered. - Every instance has their own DBU/hour. Depending on the instance selected, the cost will be the instances [DBU/Hr] x [the compute type price]\t- Standard EBS pricing dependent on size of EBS volumes attached to instances/pods OSS\t- Price dependent on Infrastructure Size - Billed per-second, with a one-minute minimum\tN/A\t- Standard EBS pricing dependent on size of EBS volumes attached to instances/pods Lets look at an example to help understand the differences. Example:Suppose you run a Spark application that requires two r5.4xlarge (16 vCPU, 128 GB) EC2 Instances and it runs at 100% utilization. The application runs for 3 hours. The total compute used is: 25 instances x 3 hours x 16 vCPU = 1200 vCPU hours 25 instance x 3 hours x 128 GB = 9600 GB hours Infrastructure Cost\tService Cost\tTotal\t% increase compared to EMR on EC2EMR on EC2\t26 instances x 3 hours x r5.4xlarge EC2 price/hour = 26 x 3 x $1.008 = $78.62\t26 instances x 3 hours x r5.4xlarge EMR price/hour = 26 x 3 x $0.252 = $19.66\t$98.28\t0 EMR on EKS\t25 instances x 3 hours x r5.4xlarge EC2 price/hour = 25 x 3 x $1.008 = $75.6\t1200 vCPU Hours x $0.01012 / vCPU / Hours = $12.14 9600 GB hours x $0.00111125 / GB / Hours = $10.69\t$98.43\t0.15% EMR Serverless\tN/A\t1200 vCPU Hours x $0.052624 / vCPU / Hours = $63.15 9600 GB hours x $0.0057785 / GB / Hours = $55.47\t$118.62\t17.15% OSS\t25 instances x 3 hours x r5.4xlarge EC2 price/hour = 25 x 3 x $1.008 = $75.6\tN/A\t$75.60 Assumptions Assumed engine performance is the same across all deployment modelsAssumed 100% utilization across all EMR deployment modelsAssumed x86, on-demand pricing in US-WEST-2EMR on EC2 requires 1 extra instance because of primary nodePricing for EMR-S is x86No Storage costs consideredNo provisioning costs considered Key Takeaway: Assuming that the amount of compute to complete a workload is identical, all deployment models and vendors will have a different cost for that same amount of usage. Those with higher cost would need to have better performance to make up the difference in pricing. Pricing is a key differentiator between vendors and deployment models. Purchase Option Amazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs: On-Demand Instances – Pay, by the second, for the instances that you launch.Savings Plans – Reduce your Amazon EC2 costs by making a commitment to a consistent amount of usage, in USD per hour, for a term of 1 or 3 years.Reserved Instances – Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years.Spot Instances – Request unused EC2 instances, which can reduce your Amazon EC2 costs significantly.Capacity Reservations – Reserve capacity for your EC2 instances in a specific Availability Zone for any duration. For more details, see here Purchase options can significantly reduce the overall costs of a workload. However, when conducting benchmarking, it's crucial to maintain control over this variable. Specifically, the benchmark should exclusively utilize On-Demand Instances and avoid the use of Spot Instances. Spot Instances come with unpredictable interruption rates that can impact both the performance and cost of the job. When considering discounts such as savings plans, ensure that they are applied consistently across all deployment models. One exception to this guideline arises when you wish to assess how certain deployment models or vendors handle spot instance interruptions and capacity acquisition. For instance, EMR on EC2 supports Instance Fleets with various allocation strategies designed to select instances with the lowest likelihood of interruption. If this is a variable you intend to incorporate into your price-performance analysis, you can run your benchmark with Spot Instances. Key Takeaway: Only use on-demand instances for benchmarking. Spot has unpredictable interruptions that impact price-performance. Ensure all discounts are applied appropriately across services (ec2 vs emr). Hardware Selection Hardware selection refers to the choice of instance types and storage utilized for benchmarking. To ensure consistency when benchmarking across deployment models and vendors, it's important to maintain hardware selection as a controlled variable. The hardware selection determines several critical aspects, including the number of containers that can run in parallel and the utilization of compute resources, as well as the speed and volume of data that can be written to local disk. These factors directly impact the overall cost of the job. In the case of EMR Serverless, where you don't manually select hardware, it's essential to ensure that the total compute allocated matches the hardware provisioned at the EC2 level. As a controlled variable, instance family, size, generation, and local storage should be kept consistent. Exceptions to this rule may occur if a vendor or deployment model offers instances that are not available in the others. For instance, if EMR on EC2 or Serverless introduces a new instance type, you can consider it a differentiator and treat it as an independent variable in your benchmarking analysis. Hardware can be an independent variable when you want to measure the difference in price-performance between instance types. This is useful if you are benchmarking the same deployment model (keeping Engine/deployment as a controlled variable ) to determine the most optimal hardware to use for your application. Key Takeaway: Use the same instance type, family and size while benchmarking. Changes in these variables will result in differences in price-performance. Changing hardware is only useful when isolating the change to hardware. For example, comparing performance of R and M with EMR on EC2. Workload Workload refers to the specific job being benchmarked, encompassing various elements such as the input data being read, the job type (streaming, batch, SQL), the processing or logic within the code, and the output data being written. All of these variables have a substantial impact on price-performance and must be maintained consistently throughout the benchmarking process. For instance, consider a scenario where two benchmarks involve the same amount of data but exhibit slightly different data skew. In such cases, completing the same job may require more compute or incur higher costs due to the variations in data distribution. Similarly, if one benchmark writes data in Parquet format while the other uses Avro, and Parquet, based on the data distribution, can produce more compact files, it may require less compute and result in lower costs. Another important workload consideration is if an Open Table Format (OTF) is being used. Iceberg, Delta and Hudi are increasingly more common in customers workloads and can significantly impact the performance of reading and writing. When it comes to OTF, we also want to keep this variable consistent across benchmarks. There are instances when you might want to treat Workload as an independent variable, such as when comparing performance across different types of applications for a given engine or deployment model. For example, the behavior of an I/O-bound, CPU-bound, or memory-bound job can differ across Spark engines. Key Takeaway: Keep everything with the workload constant between benchmarks. This extend beyond application code and also includes data input, output, OTF, compression, data distribution and caching Application Configuration Application configurations impact the way a job is run. These configurations include Spark configs such as executor memory or dynamic resource allocation (DRA), Hadoop configs such as yarn memory and JVM configs, such as GC or JDK version. Differences in application configurations impact how price-performant a job is. For example, an IO bound job may have a higher cost with spark executor sizes of 1vCPU:8Gb Memory vs 1vCPU:2Gb Memory because the job does not utilize all the memory. In addition to application configurations, there are features controlled by configurations such as Spark’s Dynamic Resource allocation. This allows spark applications to scale contain increasing the parallelism of task processing. These factor impact price-performance and should be a controlled variable during benchmarking. Note that application configuration that do not apply or exist between deployment models and vendors can be skipped. Most Spark configurations will exist on all deployment models and vendors. Application configuration can be an independent variable when trying to optimize your job for a given engine and deployment model. For example, If you have an application that is running on EMR-S and want to understand the impact of varying spark executor container sizes. Key Takeaway: Maintain the same set of application configurations across all benchmarks. When no application configurations are known, start with the default configurations provided by the deployment model. Runtime Performance Runtime performance refers to the speed at which a job is completed. It represents one of the key distinguishing factors between EMR, various vendors, and OSS (Open Source Software) solutions. Runtime performance is an independent variable directly influencing the cost of the job. Improved runtime performance reduces the amount of compute resources required to complete the task. Across EMR deployment models, the engine's runtime remains consistent. The impact of runtime performance is contingent on the type of workload. For instance, jobs with heavy I/O demands may not experience the same level of performance improvement as those that are memory or CPU-bound. The outcome can also be influenced by factors like the APIs in use, join conditions, filter criteria, and more. Many of the Spark optimizations carried out by the EMR team are based on TPC-DS, an industry-standard benchmark representative of customer workloads. While TPC-DS serves as a solid baseline, the most accurate assessment of runtime performance comes from analyzing real customer workloads. Key Takeaway: EMR deployment models and vendors may also introduce specific features unrelated to runtime but impacting the overall cost. These features may involve enhancements to Spark libraries, such as write improvements to EMRFS when interacting with S3 or read optimizations due to OTF (Open Table Format) compaction. Additionally, differences in external services like shuffle service or how Spark containers are scheduled can all influence overall price-performance. While benchmarking, these deployment or vendor-specific features can be considered as part of runtime performance. Consider runtime as a controlled variable when you're not evaluating other engines and aim to optimize the price-performance of the chosen engine across variables such as hardware or application configurations. Key Takeaway: Runtime performance is a key differentiator that has a significant impact to price-performance. By having runtime performance as your independent variable and keeping all other variables controlled, you can properly measure the effect of runtime on price-performance. Runtime performance is not applicable when comparing across deployment models because all deployments use the same engine. Infrastructure Provisioning and Scaling Infrastructure provisioning and scaling refer to the time it takes for compute resources to become available for applications to run and the time it takes for compute resources to terminate. The longer it takes for infrastructure to provision or scale, the higher the associated cost. Provisioning and scaling up represent compute time that cannot be used, contributing to under utilization. The same holds true for scaling down. Infrastructure provisioning also encompasses the time required to install applications. Consequently, deployments based on container images will have shorter provisioning times compared to virtual machines, which download and install libraries after the infrastructure is ready. If deployment models are employed as long-running compute solutions, infrastructure provisioning is minimized. In addition to provisioning and termination times for scaling, another critical aspect is scaling efficiency. This includes factors like how quickly scaling responds to changes in usage and the accuracy of scaling to meet demand. Prolonged scaling or excessive scaling can have a detrimental impact on overall costs. Regarding benchmarking, infrastructure provisioning and scaling processes are unique to each deployment model. These are control plane features that serve as key differentiators and should be regarded as independent variables. Key Takeaway: Similar to Runtime performance, infrastructure provisioning and scaling are key differentiators but at the control plane layer instead of data plane. The impact to price-performance will be reflected in the overall cost of the job through compute utilization. Summary - Benchmark Variable Checklist Independent = Variables that are manipulated or what changes in the benchmark Controlled = Variables that are kept consistent to properly measure the effect of independent variables. What are you Benchmarking?\tPricing Model\tPurchase Option\tHardware Selection\tWorkload\tApplication Configuration\tRuntime Performance\tInfrastructure Provisioning and Scaling\tSummaryVendors (OSS, EMR)\tIndependent\tControlled\tControlled\tControlled\tControlled\tIndependent\tControlled\tWhen benchmarking vendors, you only want to know how the vendor specific runtime and their pricing model impacts price-performance. Keep everything about the workload, configurations, hardware and purchasing options the same. Deployment Models\tIndependent\tControlled\tControlled\tControlled\tControlled\tControlled\tIndependent\tWhen benchmarking deployment models, you only want to know how the infrastructure provisioning, scaling and pricing model impacts price-performance. Keep everything about the workload, configurations, hardware and purchasing options the same. Application configurations\tControlled\tControlled\tControlled\tControlled\tIndependent\tControlled\tControlled\tOnly the changes to your application configurations are independent to determine how they impact price performance Hardware configurations\tControlled\tControlled\tIndependent\tControlled\tControlled\tControlled\tControlled\tOnly the changes to your hardware selections are independent to determine how they impact price performance","keywords":"","version":"Next"},{"title":"Best Practice","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice","content":"","keywords":"","version":"Next"},{"title":"EMR Multi Master​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#emr-multi-master","content":" When working with HBase on Amazon EMR, it is good practice to enable the EMR Multi Master feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down.  Nevertheless, this functionality is highly recommended both when using HDFS or S3 as storage layer for your HBase cluster. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason (e.g. human error, hardware impairment, etc.), it will not be possible to recover any data from the HDFS storage on the cluster as the HDFS metadata will be lost after the termination of the EMR master.  ","version":"Next","tagName":"h2"},{"title":"EMR Termination Protection​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#emr-termination-protection","content":" Using termination protection in Amazon EMR is highly recommended both when using HDFS or Amazon S3 for your HBase cluster.  Amazon EMR periodically checks the Apache Hadoop YARN status of nodes running on CORE and TASK nodes in a cluster. The health status is reported by the YARN NodeManager health checker service. If a node reports an UNHEALTHY status, it will not be possible to allocate YARN containers to it until it becomes healthy again. A common reason for unhealthy nodes is that disk utilization goes above 90%. If the node stays in this state for more than 45 minutes and Termination Protection is disabled, the EMR service terminates the node and launch a fresh new one as replacement.  When a node is in an UNHEALTHY state, with the termination protection enabled the nodes will not be terminated and replaced by the EMR service. This prevents to lose HDFS data blocks in case the utilization of the disks of a CORE node goes above 90%, so preventing data integrity issues in HBase tables.  ","version":"Next","tagName":"h2"},{"title":"HBase RPC Listeners​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#hbase-rpc-listeners","content":" One of the most important parameters to configure in your HBase cluster is the number of active RPC listeners defined per Region Server. Tuning the parameter hbase.regionserver.handler.count (default: 30) can increase the number of requests that you can concurrently serve in each region server and so the overall throughput of your cluster. To modify the default number of RPC listeners you can use the following EMR configuration:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.regionserver.handler.count&quot;: &quot;120&quot; } } ]   However, please be mindful that this parameter should be tuned accordingly to the average size of data stored or retrieved from your tables. As rule of thumb, you should increase this number when the payload of your data is lower than 100KB, while you should stick to the default, or decrease it when the payload size is &gt;= 1MB. For small payloads (&lt;= 1KB), you can push this value up to 4 times the number of vCpu available in your Region Servers.  To determine the average payload of data stored in your tables, see Determine average row size.  ","version":"Next","tagName":"h2"},{"title":"HBase Heap Memory​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#hbase-heap-memory","content":" On Amazon EMR, when you install HBase, the memory will be evenly re-partitioned between Hadoop YARN and HBase services. For a list of the default memory settings used per instance type see Task configuration in the EMR documentation.  However, when working with HBase it might be convenient to override the default parameters and increase the available memory for our HBase services. This might be required if we want to host a higher number of Regions per Region Server. To modify the default memory, you should modify the HBase environmental variables defined in the hbase-env which defines the default heap memory available for each HBase service. The following list highlight the variables that should be modified by service:  HBASE_MASTER_OPTS JVM options for the HBase masterHBASE_REGIONSERVER_OPTS JVM options for the HBase Region ServersHBASE_THRIFT_OPTS JVM options for the HBase Thrift serviceHBASE_REST_OPTS JVM options for the HBase REST service  It’s best practice to modify the memory of each component using its own dedicated variable, rather than using the more general HBASE_OPTS, which is used to apply common JVM options across all HBase services.  To override the default memory we should specify the following java parameter in our environmental variable: -Xmx&lt;size&gt;[g|G|m|M|k|K]. Please also make sure to add a self reference in the environmental variable to avoid loosing other parameters that are set in the script. Besides, if we modify the default HBase memory, we should also lower accordingly the memory specified for the YARN Node Manager service to avoid incurring in Out Of Memory errors.  Please note that either if you’re just installing HBase, it might still be convenient to keep some memory reserved for YARN. This can be useful as some HBase utility runs on YARN (e.g. HBase export utility).  The example below highlights the configurations that should be modified in an EMR cluster while tuning the HBase heap memory. Please make sure that the sum of the YARN and HBase memory is not greater than the memory available on the node. Also make sure to keep at least 2GB of available memory for the Operating System and other internal components running on the node.  [ { &quot;Classification&quot;: &quot;yarn-site&quot;, &quot;Properties&quot;: { &quot;yarn.scheduler.maximum-allocation-mb&quot;: &quot;MAX_MEMORY_BYTES&quot;, &quot;yarn.nodemanager.resource.memory-mb&quot;: &quot;MAX_MEMORY_BYTES&quot; } }, { &quot;Classification&quot;: &quot;hbase-env&quot;, &quot;Configurations&quot;: [ { &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;HBASE_MASTER_OPTS&quot;: &quot;\\&quot;$HBASE_MASTER_OPTS -Xmx30g\\&quot;&quot;, &quot;HBASE_REGIONSERVER_OPTS&quot;: &quot;\\&quot;$HBASE_REGIONSERVER_OPTS -Xmx30g\\&quot;&quot; } } ], &quot;Properties&quot;: {} } ]   ","version":"Next","tagName":"h2"},{"title":"HBase MultiWal Provider​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#hbase-multiwal-provider","content":" By default, HBase uses a single Write Ahead Log file (WAL) per Region Server to persist mutate operations that are performed against Regions hosted on the node. This implementation can be a bottleneck as WALs are stored on the HDFS and each operation is performed sequentially against the same file.  In write intensive clusters, you might increase the HBase throughput by adopting a multiwal strategy. In this scenario is recommended to have multiple disks attached to the node to get the most out of this feature. This configuration can be enabled specifying the following properties while launching an EMR cluster:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.wal.provider&quot;: &quot;multiwal&quot;, &quot;hbase.wal.regiongrouping.numgroups&quot;: &quot;2&quot; } } ]   The parameter hbase.wal.regiongrouping.numgroups determines the number of WALs that will be created per Region Server. By default, this parameter is set to two, but you can tune this parameter accordingly to the number of disks attached to the node for better performance.  ","version":"Next","tagName":"h2"},{"title":"HBase OffHeap Caching​","type":1,"pageTitle":"Best Practice","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice#hbase-offheap-caching","content":" The following example, shows how to enable OffHeap memory caching on HBase. This configuration, can be used both when using Amazon S3 or HDFS as storage layer. The example below sets an offheap memory of 5GB while the bucket cache allocated for this memory will be 4GB.  [ { &quot;Classification&quot;: &quot;hbase-env&quot;, &quot;Properties&quot;: {}, &quot;Configurations&quot;: [ { &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;HBASE_OFFHEAPSIZE&quot;: &quot;5G&quot; }, &quot;Configurations&quot;: [] } ] }, { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.bucketcache.size&quot;: &quot;4096&quot;, &quot;hbase.bucketcache.ioengine&quot;: &quot;offheap&quot; } } ]   In order to use the configured cache, make sure to enable the following configurations in the tables you want to cache. For example, from the HBase shell:  # creating new table t with column family info0 hbase&gt; create 't', {NAME =&gt; 'info0', CONFIGURATION =&gt; {CACHE_DATA_IN_L1 =&gt; 'true'}} # modify existing table t with column family info0 hbase&gt; alter 't', {NAME =&gt; 'info0', CONFIGURATION =&gt; {CACHE_DATA_IN_L1 =&gt; 'true'}}  ","version":"Next","tagName":"h2"},{"title":"Retrieve Spark Event Logs","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Analyzing/retrieve_event_logs","content":"Retrieve Spark Event Logs When you want to analyze the performance of your workloads, you’ll typically need to check the Spark Web UI to identify areas of improvement or just to detect events that are de-gradating the performance in your application. The Spark Web UI uses the Event Logs that are generated by each job running in your cluster to provide detailed information about Jobs, Stages and Tasks of your application that provides aggregated metrics that can help you to troubleshoot performance issues. These files are extremely portable, as they can be collected across different engines or environments and stored in the same Spark History Server to have a single interface where you can review results of different benchmark results across different environment or cloud providers. When using Amazon EMR, the Spark Event logs are enabled by default and are automatically stored on the HDFS of the cluster where the job was running under the HDFS path /var/log/spark/apps/ [hadoop@ip-172-31-3-8 ~]$ hdfs dfs -ls -R /var/log/spark/apps/ -rw-rw---- 1 hadoop spark 408384 2023-09-08 21:00 /var/log/spark/apps/application_1694206676971_0001 If you have Event Logs coming from a different environment or cluster, you can easily store them in this folder, and the Spark Web History Server will automatically pick them and you’ll be able to review the information of the job on the Spark History Server. As alternative, if you want to export the Event Logs from a running cluster, you can also download them manually from the Spark Web History server from the main page as shown in the image below. Finally, if you’re using on premise cluster or any third-party Spark environment, you can automatically enable the Spark Event logs using the following Spark configurations: spark.eventLog.enabled (Boolean) Determine if you want to enable or disable event logs collection. False by defaultspark.eventLog.dir (String) Location where to store the event logs. Can be an Object Store as Amazon S3, Azure Filesystem, or any path recognized by the Hadoop Filesystem API (e.g. HDFS, Local Filesystem, etc.) Below an example to manually enable the Spark event logs in your Spark application: spark-submit \\ --name &quot;Example App&quot; \\ --conf spark.eventLog.enabled=true \\ --conf spark.eventLog.dir=hdfs:///tmp/spark \\ ... ","keywords":"","version":"Next"},{"title":"Benchmark Results","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results","content":"","keywords":"","version":"Next"},{"title":"Spark​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#spark","content":" EMR 6.10: https://aws.amazon.com/blogs/big-data/amazon-emr-on-eks-widens-the-performance-gap-run-apache-spark-workloads-5-37-times-faster-and-at-4-3-times-lower-cost/  EMR 6.9: https://aws.amazon.com/blogs/big-data/run-apache-spark-workloads-3-5-times-faster-with-amazon-emr-6-9/  EMR 6.5: https://aws.amazon.com/blogs/big-data/amazon-emr-on-amazon-eks-provides-up-to-61-lower-costs-and-up-to-68-performance-improvement-for-spark-workloads/  ","version":"Next","tagName":"h3"},{"title":"Graviton​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#graviton","content":" EMR Graivton 2:  https://aws.amazon.com/blogs/big-data/achieve-up-to-27-better-price-performance-for-spark-workloads-with-aws-graviton2-on-amazon-emr-serverless/  EMR Graviton 3:  EMR on EKS - https://aws.amazon.com/blogs/big-data/amazon-emr-on-eks-gets-up-to-19-performance-boost-running-on-aws-graviton3-processors-vs-graviton2/  EMR on EC2 - https://aws.amazon.com/blogs/big-data/amazon-emr-launches-support-for-amazon-ec2-c7g-graviton3-instances-to-improve-cost-performance-for-spark-workloads-by-7-13/  ","version":"Next","tagName":"h3"},{"title":"Intel​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#intel","content":" EMR Intel (C6i, M6i, I4i, R6i, and R6id): https://aws.amazon.com/blogs/big-data/amazon-emr-launches-support-for-amazon-ec2-c6i-m6i-i4i-r6i-and-r6id-instances-to-improve-cost-performance-for-spark-workloads-by-6-33/  ","version":"Next","tagName":"h3"},{"title":"AMD​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#amd","content":" EMR AMD (m6a, r6a): https://aws.amazon.com/blogs/big-data/amazon-emr-launches-support-for-amazon-ec2-m6a-r6a-instances-to-improve-cost-performance-for-spark-workloads-by-15-50/  ","version":"Next","tagName":"h3"},{"title":"Managed Scaling​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#managed-scaling","content":" Managed Scaling Improvements: https://aws.amazon.com/blogs/big-data/reduce-amazon-emr-cluster-costs-by-up-to-19-with-new-enhancements-in-amazon-emr-managed-scaling/  ","version":"Next","tagName":"h3"},{"title":"EMR on EKS​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#emr-on-eks","content":" EMR on EKS vs OSS: https://aws.amazon.com/blogs/big-data/amazon-emr-on-amazon-eks-provides-up-to-61-lower-costs-and-up-to-68-performance-improvement-for-spark-workloads/  ","version":"Next","tagName":"h3"},{"title":"Hive​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#hive","content":" Hive Rename Feature: https://aws.amazon.com/blogs/big-data/up-to-15-times-improvement-in-hive-write-performance-with-the-amazon-emr-hive-zero-rename-feature/  ","version":"Next","tagName":"h3"},{"title":"Customer Examples​","type":1,"pageTitle":"Benchmark Results","url":"/aws-emr-best-practices/docs/benchmarks/Resources/Benchmark_results#customer-examples","content":" EMR Serverless: https://aws.amazon.com/blogs/big-data/godaddy-benchmarking-results-in-up-to-24-better-price-performance-for-their-spark-workloads-with-aws-graviton2-on-amazon-emr-serverless/  Amazon Athena  Athena V3: https://aws.amazon.com/blogs/big-data/upgrade-to-athena-engine-version-3-to-increase-query-performance-and-access-more-analytics-features/  Athena V2: https://aws.amazon.com/blogs/big-data/run-queries-3x-faster-with-up-to-70-cost-savings-on-the-latest-amazon-athena-engine/  Athena CBO: https://aws.amazon.com/blogs/big-data/speed-up-queries-with-cost-based-optimizer-in-amazon-athena/ ","version":"Next","tagName":"h3"},{"title":"Setting up the Benchmark Environment","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Running/setting_up_environment","content":"Setting up the Benchmark Environment EMR on EC2 Getting started guide: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html Benchmark guide: https://github.com/aws-samples/emr-spark-benchmark EMR on EKS Getting started guide: https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up.html Benchmark guide: https://github.com/aws-samples/emr-on-eks-benchmark/tree/main EMR Serverless Getting started guide: https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/getting-started.html Benchmark guide: https://github.com/aws-samples/emr-spark-benchmark","keywords":"","version":"Next"},{"title":"Price Performance","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/price_performance","content":"Price Performance In the context of this guide, &quot;price-performance&quot; refers to the cost (in dollars) of running a workload for a specified level of performance (measured in run time, in seconds). Utilizing price-performance is crucial for benchmarking to grasp the implications of variables that cannot be standardized. These variables may include deployment models, competitor pricing, container scheduling, or engines. For variables that are within our control, such as infrastructure sizing or application configurations, it's essential to maintain consistency across all benchmarks. Below examples illustrates the importance of price-performance. Example 1: Customer wants to compare OSS Spark vs EMR Spark with different cluster sizes Cluster #1\tCluster #2Runtime (s)\t12\t30 # of nodes\t50\t10 Engine\tOSS Spark Runtime\tEMR Spark Runtime Cost ($)\t600\t300 In the above example, Cluster #1 is running OSS spark and completes in 12s with 50 nodes, while EMR Spark completes in 30s with 10 nodes. However, when we look at total cost, cluster #2 total cost is lower than cluster #1 making it a better option. Comparing cost in relation to the work being done considers the difference in # of nodes and engine. Assuming performance is linear, lets look at what happens when we increase the # of nodes in cluster 2. Example 2: Customer wants to compare Open Source Software (OSS) Spark vs EMR Spark with same cluster sizes Cluster #1\tCluster #2Runtime (s)\t12\t6 # of nodes\t50\t50 Engine\tOSS Spark Runtime\tEMR Spark Runtime Cost ($)\t600\t300 After increasing the # of nodes to be the same across both clusters, runtime is reduced to 6seconds on Cluster #2 and cost remains the same at 300$. Our conclusion from the first example remains the same. Cluster #2 is the best option from a price-performance perspective. It’s important to note that price-performance is not always linear. This is often seen when workloads have data skew. In these cases, adding more compute does not reduce runtime proportionally and adds costs. Example 3: Same workload across different # of nodes - data skew Run #1\tRun #2Runtime (s)\t100\t75 # of nodes\t10\t20 Engine\tEMR Spark Runtime\tEMR Spark Runtime Cost ($)\t1000\t1500 In the above example, performance is not linear. While runtime reduced to 75s, overall cost increased. In these cases, it’s important ensure the # of nodes are the same for both comparisons. Another scenario where price-performance is useful is when comparing different pricing models or vendors. Take the example below: Example 4: Same workload across different pricing models EMR Spark Runtime\tVendorRuntime (s)\t50\t40 # of nodes\t10\t10 $/s\t1\t1.5 Cost ($)\t500\t600 In the above , the same workload on vendor runs in 40s, while EMR runs in 50s. While vendor may seem faster, when we factor in price-performance, we see total cost is lower with EMR. If runtime is a key requirement, we can increase the # of nodes in relation to performance as illustrated in example 5. Example 5: Same workload across different pricing models with different # of nodes EMR Spark Runtime\tEMR Spark Runtime linear performance\tVendorRuntime (s)\t50\t25\t40 # of nodes\t10\t20\t10 $/s\t1\t1\t1.5 Cost ($)\t500\t500\t600 The goal with benchmarking should always be to have like-for-like comparisons. This is especially true for factors such as application configuration settings such as executor sizes, input and output dataset, cluster size and instances. However, factors like vendor/aws pricing model, engine optimizations, and schedulers cannot be made the same. As such, it’s important to use price-performance as a key factor.","keywords":"","version":"Next"},{"title":"Spark UI and Spark History Server Analysis","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/benchmarks/Analyzing/read_spark_UI","content":"Spark UI and Spark History Server Analysis In this section we’re going to explore the Spark Web UI to understand what are the main sections and information it provides, to better understand how we can leverage this tool to review our benchmark results. Overview​ The Spark Web UI is a web interface that can be found on each EMR on EC2 cluster where Spark is installed. The interface can be accessed in two ways: Spark Driver Web UI* The Spark Driver by default exposes the Spark Web UI using information from the live application. This is the most comprehensive interface, as you can see live data of your applications in terms of disk and memory utilization. You can easily locate and connect to this intefacte by connecting on the YARN Resource Manger and then opening the Application Master URL available for the running Spark application.* Spark History Server* This is a service that Spark provides to review completed or running applications on the cluster. The interface is available on the EMR Primary (MASTER) node of the cluster and can be reached at the following address: http://master-public-dns-name:18080/. It’s important to note that this interface uses the Spark Event Logs to populate the information that are available only at runtime (e.g. caching) might not be present in this interface.* Spark History Server​ Once opened, the service will show us a summary of all the applications that are stored in the default Spark Event Log directory (hdfs:///var/log/spark/apps). Each application provides summary information as the Spark version used for the job (typically the same version installed on the cluster), the YARN application ID, it’s name (if you defined in your code) and time details to determine the application duration. Specifically the duration field is a useful field as it provides a metrics for the duration of the application since the Spark driver was launched and terminated, excluding additional submission details that are specific for different deployment models, so it can be useful to compare the Spark runtime across different versions or providers. Once selected an application, you’ll be redirected to the corresponding application page where you can have a summarized view of all the Spark Jobs executed in your application. A Spark Job is triggered by a Spark RDD Action (or any Spark Dataset API which internally relies on the previously mentioned API) so it can give you a good indication of which portions of your code took more time to execute and you can also use it to compare two jobs executed in different environments or with different configurations to spot differences in terms of time processing. In those last cases is useful to sort your Jobs by Duration using the interface. Additionally, this page provides additional information on the top left page: User* The user who launched the application. In Amazon EMR this typically match the hadoop user unless you’re using the Hadoop impersonation.Total Uptime* Time since the Spark application started till the completion of the last JobScheduling Mode* The internal scheduler used within the Spark Application to execute the Spark Jobs. By default Spark uses a FIFO (First In First Out) scheduler.* Lastly, this page allows you you to review the lifecycle of your application, by expanding theEvent Timeline** section where you can review how the different Spark Jobs were executed during the time, as also Spark Executors launch and termination, that can give you useful information to detect slow-downs due to the lack of resources (e.g. you’re using the Spark Dynamic allocation along with a cluster managed scaler and the nodes took too much time to be added to the cluster). Finally the top bar on top of the page (see picture), allows you to review additional information related to the Spark Application. In the next sections, we’re going to review each of them. Stages​ As for the Jobs page, you can review also all the Stages that have been processed in your application. This pages can be reached directly from the Web UI, and in this case it will display all the Stages of the application, or you can select a single Spark Job in theJobs** section if you’re only interested to the Stages processed in an individual Spark Job. The main Stage page provides information about duration and submission of a stage, along with related tasks processed and aggregated metrics for input/output data read from the stage, along with related shuffle metrics. If you expand an individual Stage, you’ll be redirected on a more detailed page where you can examine aggregated metrics of the Tasks processed in the stage. Storage​ TheStorage page** contains information about RDD blocks that have been cached or persisted in memory or on the local disks of the cluster. This page will show some details only if you explicitly invoke a persist or cache operation against a Spark Dataframe. More in detail, the page shows for each RDD: Name* Typically a logical name identified by the spark relation that created the RDD blocksStorage Level* Where the data has been cached (Memory only) or persisted (Memory / Disk / Memory and Disk)Cache Partitions* Number of RDD partitions cachedFraction Cached* Percentage of the data in the cacheSize In Memory* Portion of data that have been stored on the Spark Executor MemorySize On Disk* Portion of data stored on the local disks of the nodes of the cluster* Below an example of a Spark DataFrame persisted both in Memory and on local Disks, with serialized and replicated data (StorageLevel.MEMORY_AND_DISK_SER_2). Besides, you can can also access a more detailed paged about the cached element by selecting the RDD of interest. In this case a new page will be opened, with additional information about the distribution and storage footprint of the object along with storage locations of the cached blocks. As mentioned, this information are typically available only on a running application, so either if your application is caching some data, you might not be able to see this information if you open a completed application in the Spark History UI. Environment​ TheEnvironment** page display all the main configurations that have been used to execute the Spark Application. Within this section there are seven sections: Runtime Information* Core details about the Java and Scala versions used to run the jobSpark Properties* Contains Spark configurations defined while launching the application and retrieved from the /etc/spark/conf/spark-defaults.conf configuration file. Additional Spark main configurations that are mandatory to run a job are also visualized with their defaults if not overridden.Resource Profiles* Contains profiles for the Spark Executors requests that are going to be submitted to the Resource Manager where the job will run (e.g. Hadoop, Kubernetes, etc.)Hadoop Properties* As for the Spark section, it contains Hadoop configurations required by the job as detected by the Hadoop configurations files, typically stored in /etc/hadoop/conf/.System Properties* Environmental variables and java properties that are configured in the Spark Drivers and Executors.Metric Properties* Spark metrics configurations as defined while launching the application, or inferred from the /etc/spark/conf/metrics.properties configuration file.Classpath Entries* Contains JAR dependencies used by Spark and the Application* Executors​ Within theExecutors** page, you can find aggregated statistics and details about all the Spark Task that have been processed in the job. In detail, the page contains two main sections: Summary and Executors. TheSummary** section provide aggregated information across all the Spark Executors. This main box can give you an idea of Executors terminated and that were active within the application lifecycle. On the other side, theExecutor** box gives you the same information (with additional details) aggregated for each executor (driver included) that was used in the application. There are several information available in this page, and they can typically be useful when sizing or trying to optimize an application in terms of consumption. Below a list of the most important parameters: Storage Memory* Identifies the memory used / total memory available in the executor memory for storing cached data. By default this corresponds to 0.5 of the Spark Memory defined as (Container Heap Memory - Reserved Memory) * spark.memory.fractionOn Heap Storage Memory* Similar to the previous entry, but also provides a split across On Heap and Off Heap Storage Memory usedDisk Used **Peak JVM Memory Provides an aggregated view of the memory utilization across the Spark Driver/Executor utilization. This includes both Spark Storage and Execution memory and Reserved Memory.Peak Execution Memory* Shows the memory utilization for the Spark Execution Memory. This portion of the memory is used by Spark to process tasks and is the memory the get spilled on the disks when Spark requires more memory to complete a taskPeak Storage Memory* This highlights the peak memory utilization ot the Storage Memory used for caching RDDsCores* Number of CPU cores available in the executors launched. The number of cores is typically a static value defined while launching a Spark application (unless you’re using the EMR hetherogeneous executors), and defines the maximum number of tasks that can be processed in parallel on the ExecutorActive Tasks* Number of Spark tasks currently running on the executorFailed Tasks* Number of Spark tasks that failed on the executorComplete Tasks* Aggregated total count of all the tasks processed by an executorTotal Tasks* Overall total number of Spark tasks processed (active + failed + complete)Task Time (GC Time)* Aggregated time required to process the task that were running on the executors. This also includes Garbage Collection aggregated time. When the GC Time is greater than 10% of the Task time the box will be displayed with a red background to highlight an excessive number of GC operationsInput* Bytes read from the Executors from a Storage source (e.g. HDFS, Amazon S3, etc.)Shuffle Read* Total bytes of shuffle data read from an Executor. This includes both local and remote shuffle data *Shuffle Write**Total bytes of shuffle data written on the local disk of the Spark Executor ** SQL/DataFrame​ Finally, the SQL/DataFrame page is another important page, which summarize all Spark Queries executed in a Spark Application. This page is only visible in the UI if your application is using Dataset or DataFrame Spark APIs. If you don’t see this page in your application, you can infer the following information: Your application is using the old RDD APIsYour application migth not be using Spark at its best as most of the Spark optimizations techniques are applied by the Spark Catalyst Optimizer that is only used when working with Dataset/ DataFrame APIs. So if possible try to upgrade your application to use the new APIs. The summary page provides, a description of the Query ID, its duration and the Spark Jobs that were launched to execute the specific query (Job IDs) If you select a single query, you’ll be redirected to a more detailed page where you can examine a visual graph of the Spark operations performed in the application. The graph can simplify the analysis on an application as each stage provides aggregated information about the number of rows and data processed. Additionally, you can also review the Spark Plans generated by the application expanding the Details section below the graph.","keywords":"","version":"Next"},{"title":"Management","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/management","content":"","keywords":"","version":"Next"},{"title":"Create command alias​","type":1,"pageTitle":"Management","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/management#create-command-alias","content":" If you administrate your HBase cluster mainly from the shell of the EMR master, it might be convenient to define a command alias to avoid permission issues you might incur by typing erroneous commands using a different user (e.g. root).  As best practice, you should always run HBase commands as hbase user. In order to do that, you can add the following alias in the ~/.bashrc profile for the user that you use to administrate your cluster (e.g. hadoop)  alias hbase='sudo -u hbase hbase'   Once done, you can safely run HBase commands as usual  hbase shell   ","version":"Next","tagName":"h2"},{"title":"Determine average row size​","type":1,"pageTitle":"Management","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/management#determine-average-row-size","content":" If you want to determine the average size of a row stored in a HBase table, you can use the following commands to retrieve the payload from a storefile of the table. For example:  # Simple notation hbase hfile -m -f $HBASE_PATH # Extended notation hbase org.apache.hadoop.hbase.io.hfile.HFile -m -f $HBASE_PATH     The class org.apache.hadoop.hbase.io.hfile.HFile allows you to analyze HBase store files that are persisted on HDFS or S3. The option -m returns the metadata for the file analyzed that reports the average size (bytes) of the Row Key in that particular file, and the average size (bytes) of the values stored in that file.  To get a rough estimation of the average payload of a single row, you can sum the parameters avgKeyLen and avgValueLen that are returned in the previous command, to get the average size in bytes of a row. For example:  # row_avg_size = avgKeyLen + avgValueLen row_avg_size = 19 + 7 = 26   This command might be useful to get a rough estimate of your data payload when you are not sure about it. You can later on use this value to fine-tuning your cluster (e.g. increase/decrease RPC Listeners)  ","version":"Next","tagName":"h2"},{"title":"Reduce number of Regions​","type":1,"pageTitle":"Management","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/management#reduce-number-of-regions","content":" The HBase community introduced a new utility in the hbase-tools package that helps to reduce the number of regions for tables stored in HBase. This utility is available in the class org.apache.hbase.RegionsMerger and can help you to automatically merge the number of regions to a value that you define, if you have a high count in your cluster (e.g. wrong table pre-split, or high split rate due to incorrect settings)  # copy library in classpath sudo cp /usr/lib/hbase-operator-tools/hbase-tools-*.jar /usr/lib/hbase/lib/ # merge regions hbase org.apache.hbase.RegionsMerger &lt;TABLE_NAME&gt; &lt;TARGET_NUMBER_OF_REGIONS&gt;   This tool is available in HBase versions &gt;= 2.x.x and should only be used with these versions. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction","content":"","keywords":"","version":"Next"},{"title":"Which storage layer should I use?​","type":1,"pageTitle":"Introduction","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction#which-storage-layer-should-i-use","content":" Typically, to understand which storage layer you should use in your HBase cluster, you must determine what are your application requirements and decide what is most important between these two main decision drivers: performance or costs. Generally speaking, on a large cluster setup, HDFS provides better performance in most cases, while Amazon S3 provides better cost savings due to the reduced amount of storage required to persist all your data, and is the right option when you want to decouple your storage from compute.  Using HDFS allows you to achieve the best performance for latency responses. This is true if you need milliseconds / sub-milliseconds read responses from HBase. You can also achieve similar results using Amazon S3 as storage layer, but this will require to rely on HBase caching features. Depending on your tables sizes, this can increase costs when provisioning resources for cache, as you’ll have to provision more EBS volumes or use bigger instances to cache your data locally on the nodes, thus losing the main advantages of using Amazon S3. This requires to fine tune HBase to find the right balance between performance and cost for your workload.  Another common use case to choose HDFS over S3 is a data migration from an on premise cluster. This is typically recommended as first migration step, as this solution provides similar performance compared to your existing cluster. You can more easily migrate your infrastructure to the cloud, and later decide if it makes sense to use Amazon S3. Besides, using the HDFS for a data migration can be a requirement before moving to Amazon S3. Specifically this can help to optimize the underlying layout of your HBase tables if they have a considerable amount of small HBase regions, and you want to merge them. This operation can be more quickly be performed on an HDFS cluster, and you can later migrate the data to Amazon S3. For more details, see the sections Reduce number of Regions and Data Migration.  Finally, using HDFS is also the right choice if you have a cluster that is mostly used for write intensive workloads. This is because write intensive clusters are subject to intensive compaction and region splitting operations that are performed internally by HBase to manage the underlying data storage. In these cases, using Amazon S3 might not be the right option, because of data movements that occur between Amazon S3 and the cluster to perform compaction processes. This increases the time required to perform such operations, thus impacting the overall cluster performance resulting in higher latencies.  On the other side, Amazon S3 is a good option for read-intensive HBase clusters. One of the best use cases where S3 excels is when the data that is most frequently accessed (read or modified) is the most recent, while old data is rarely modified. You can use the pre-configured bucket cache, to store a hot copy of the most recent data on local disks of your cluster, thus maintaining a good compromise in terms of costs and performance. For more details, see Bucket Cache.  Another good use case for using Amazon S3 is when you have tables that rarely change over time, and you need to serve a large amount of read requests. In this case, you can opt for Amazon S3 in combination with the EMR HBase read-replica, to distribute your read requests across multiple clusters. For more details about this approach kindly see Data Integrity. Moreover, Amazon S3 provides stronger SLA for data durability and availability transparently at the storage level and will not be impacted by failures on EMR instances.  Finally, one major benefit of relying on S3 for storage is cost saving. If you have significant costs in your cluster due to large amount of data stored on EBS volumes, moving to S3 can reduce costs drastically. Moreover, HDFS uses block replication to provide fault tolerance, which increases the footprint of data stored locally in your cluster. In Amazon EMR, the default HDFS replication factor is defined automatically when launching the cluster (or you can override it manually using the EMR configuration API). For large tables size this can drastically increase EBS storage costs, so you might want to leverage S3 where replication is handled natively by the service for a more convenient cost.  ","version":"Next","tagName":"h2"},{"title":"Which instance should I use?​","type":1,"pageTitle":"Introduction","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction#which-instance-should-i-use","content":" When talking about hardware requirements for HBase, it is very important to choose the right EC2 instance type when using HDFS as storage layer, as it might be prohibitive to change it once you have a live production cluster. On the other side, changing instances for an HBase cluster running on Amazon S3 is much easier as data is persisted on S3. This allows us to more easily terminate an EMR cluster without losing data and launch a new one using a different instance type. Below you can find some details that can help you to choose the right instances based on your use case / workloads requirements.  HBase typically performs better with small instances and when you spread the overall requests across multiple instances. This is because there are some limitations in the number of HBase regions a single Region Server can handle, and having a huge amount of regions on a single node can lead to issues and unexpected behavior. For more details on determining the right number of regions for a specific instance, see the section Number of HBase Regions.  Generally speaking, if you want to achieve the best possible performance in your HBase cluster, it’s highly recommended to use EC2 instances powered with an Instance Store volume. This is especially true for write intensive / mixed (50% writes 50% reads) workloads. For such use cases, if you have significant write requests, you’ll need disks that can provide a large amount of IOPS in order to accommodate all background operations performed by HBase (compaction, WAL writes). Using disk optimized instances allows you to sustain high volumes of write operations either if HBase is performing compaction or other background operations on disks. Some example of instances that are recommended for such workloads are:  i3 / i3en provide dense SSD storage for data-intensive workloads. They provide the best performance for write intensive workloads but can be prohibitive depending on the amount of storage you want to use. They are recommended if you want to achieve the best possible performance, and if you want to cache several data in memory.m5d / r5d / c5d all these families provide NVMe SSD disks to deliver high random I/O performance. They can be used in different ways to exploit HBase features. For example, r5d can be used in combination with HBase off heap caching to maintain a significant amount of data cached in a performant memory (instead of reading data from the disks). On the other side, c5d comes with a higher proportion of vCPU compared to the memory, so they can be a better match if you need to serve huge volumes of requests on a single region server.  To decide the right instance size, it’s important to understand how many regions you’re going to serve on a single region server. As general rule however, for large HBase tables, it’s recommended to choose an instance type that can provide at least 32GB of memory dedicated for the HBase services (HMaster and Region Servers). Please note that by default Amazon EMR split the available memory of an instance between the YARN Node Manager and the HBase Region Server. For a list of default memory settings, see Default values for task configuration settings. You can always override the default EMR behavior using the EMR Configuration API. For more details see Modify Heap Memory.  ","version":"Next","tagName":"h2"},{"title":"Number of HBase Regions​","type":1,"pageTitle":"Introduction","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/introduction#number-of-hbase-regions","content":" As described in the HBase documentation, you can use the following formula to compute the number of HBase regions that should be hosted on a single region server. You should note that this is gives more of guideline about number of regions, but you should investigate and experiment on your workload to tune the number of regions:  (REGION_SERVER_MEM_SIZE * MEMSTORE_FRACTION) / (MEMSTORE_SIZE * NUM_COLUMN_FAMILIES)   REGION_SERVER_MEM_SIZE Memory allocated for the Region Server, as defined by the parameter -Xmx in hbase-env.shMEMSTORE_FRACTION Memstore memory fraction, defined by hbase.regionserver.global.memstore.size (default 0.4)MEMSTORE_SIZE Memstore flush size (default 128MB)NUM_COLUMN_FAMILIES Number of column families defined for the table  For example for a Region Server configured with 32GB of Heap memory and hosting a table with a single column family with the default HBase settings, we'll have an ideal allocation of regions equals to:  # Number Recommended Regions (32GB * 0.4) / (128MB * 1) = 100   As previosly mentioned, this is a recommended setting that you can use as a starting point. For example, is not unfrequent to have a region server with 3 / 4 times the recommended value. However, to avoid impacting the performance it’s better that you’re not extensively using these extra regions for write operations to avoid extensive GC operations that might degrade performance or in worst cases failures that will force a Region Server restart. ","version":"Next","tagName":"h2"},{"title":"Security","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/security","content":"","keywords":"","version":"Next"},{"title":"Authentication​","type":1,"pageTitle":"Security","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/security#authentication","content":" By default when launching an Amazon EMR cluster with HBase installed, the service will configure HBase without enabling any type of authentication. This allows every client connecting to HBase to read / write tables stored in the cluster without the need to provide any credentials. In this context it is a best practice to limit access to the cluster by scoping access to the cluster using firewalls or EMR Security Groups attached to the cluster. For more details see Networking  However, if you require to enable a strong authentication system, you can use Kerberos authentication to secure your cluster. HBase implements the Simple Authentication and Security Layer (SASL) at the RPC level, that will handle authentication and encryption negotiation for each connection established with the service.  Amazon EMR automatically configures HBase with the required configurations when you launch a cluster with a Security Configuration where Kerberos authentication is enabled. The following highlights the main HBase configurations set by the service when launching an EMR cluster with Kerberos enabled (generated using Amazon EMR 6.9.0):  Configuration\tValuehbase.security.authentication\tkerberos hbase.security.authorization\ttrue hbase.master.kerberos.principal\thbase/_HOST@&lt;YOUR_KERBEROS_REALM&gt; hbase.master.keytab.file\t/etc/hbase.keytab hbase.regionserver.kerberos.principal\thbase/_HOST@&lt;YOUR_KERBEROS_REALM&gt; hbase.regionserver.keytab.file\t/etc/hbase.keytab hbase.thrift.kerberos.principal\thbase/_HOST@&lt;YOUR_KERBEROS_REALM&gt; hbase.thrift.keytab.file\t/etc/hbase.keytab hbase.thrift.security.qop\tauth hbase.rest.authentication.type\tkerberos hbase.rest.authentication.kerberos.principal\tHTTP/_HOST@&lt;YOUR_KERBEROS_REALM&gt; hbase.rest.authentication.kerberos.keytab\t/etc/hbase.keytab hbase.rest.kerberos.principal\thbase/_HOST@&lt;YOUR_KERBEROS_REALM&gt; hbase.rest.keytab.file\t/etc/hbase.keytab hbase.rest.support.proxyuser\ttrue hadoop.proxyuser.hbase.groups\t* hadoop.proxyuser.hbase.hosts\t*  To launch an EMR cluster with Kerberos Authentication, please refer to Configuring Kerberos on Amazon EMR  ","version":"Next","tagName":"h2"},{"title":"Authorization​","type":1,"pageTitle":"Security","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/security#authorization","content":" Once the users are authenticated through Kerberos, we can now implement our Authorization policies to allow restricted access for specific user to our tables. To enable this functionality, it’s required to enable the Access Controller Coprocessor, by adding additional configurations when launching the EMR cluster. Below an example EMR configuration:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.coprocessor.master.classes&quot;: &quot;org.apache.hadoop.hbase.security.access.AccessController&quot;, &quot;hbase.coprocessor.region.classes&quot;: &quot;org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController&quot;, &quot;hbase.security.authorization&quot;: &quot;true&quot;, &quot;hbase.security.exec.permission.checks&quot;: &quot;true&quot; } } ]   In order to grant permissions to specific users in the cluster, you must define the ACL policies using the hbase admin user. For example, the below command add the READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A') permissions to the hadoop user:  sudo -s kdestroy kinit hbase/`hostname -f`@YOUR_KERBEROS_REALM -k -t /etc/hbase.keytab hbase shell grant 'hadoop', 'RWXCA'   For additional details, please see the Administration section in official HBase documentation.  ","version":"Next","tagName":"h2"},{"title":"Networking​","type":1,"pageTitle":"Security","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/security#networking","content":" It’s always a good practice to restrict network access to the cluster to reduce the exposure of the services to external attacks. When using Amazon EMR, you can specify additional Security Groups attached to the cluster to enable network communication with the cluster from pre-defined ranges of IPs or other AWS Security Groups. The tables below provides HBase ports you can control in the EMR Security Groups to allow interactions with trusted parties.  For additional information see Control network traffic with security groups in the EMR documentation.  HBase Services  Port\tSecurity Group\tDescription2181 / TCP\tMaster\tZookeeper client port 16000 / TCP\tMaster\tHMaster 16020 / TCP\tCore &amp; Task\tRegion Server 8070 / TCP\tMaster\tREST server 9090 / TCP\tMaster\tThrift Server  HBase Web UI  Port\tSecurity Group\tDescription16010 / TCP\tMaster\tHMaster Web UI 16030 / TCP\tCore &amp; Task\tRegion Server Web UI 8085 / TCP\tMaster\tREST Server UI 9095 / TCP\tMaster\tThrift Server UI ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hive/introduction","content":"Introduction This section offers best practices and tuning guidance for running Apache Hive workloads on Amazon EMR.","keywords":"","version":"Next"},{"title":"Data Migration","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration","content":"","keywords":"","version":"Next"},{"title":"HBase snapshots​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#hbase-snapshots","content":" This is the most straight forward approach that doesn't require a complex setup and can easily be achieved using simple bash scripts. This approach is suitable if your data does not change frequently or when you can tolerate downtimes in your production systems to perform the data migration.  Below a list of steps that can be used to create a HBase Snapshot and transfer it to an Amazon S3 bucket. Please note that you can use the same approach to store snapshots on an HDFS cluster. If this is the case, replace the S3 target path in the following commands with the destination HDFS path (e.g. hdfs://NN_TARGET:8020/user/hbase) where you want to store the snapshots.  Create a snapshot of a single HBase table  When creating a snapshot, it’s good practice to also add an identifier in the snapshot name to have a reference date of when the snapshot was created. Before launching this command please replace the variable TABLE_NAME with the corresponding table you want to generate the snapshot for. If the table is in a namespace different from default use the following convention NAMESPACE:TABLE_NAME. From the SOURCE cluster submit the following commands:  DATE=`date +&quot;%Y%m%d&quot;` TABLE_NAME=&quot;YOUR_TABLE_NAME&quot; hbase snapshot create -n &quot;${TABLE_NAME/:/_}-$DATE&quot; -t ${TABLE_NAME}   To verify the snapshot just created, use the following command  hbase snapshot info -list-snapshots   Copy the snapshot to an Amazon S3 bucket  NoteWhen migrating from an on premise cluster, make sure that you have Hadoop YARN installed in your cluster, as the commands rely on MR jobs to perform the copy to S3. Besides, you need to make sure that your Hadoop installation provides the hadoop-aws module that is required to communicate with Amazon S3.  Note If you're planning to use HBase with Amazon S3 as storage layer, you should use as TARGET_BUCKET the same S3 path that will be used as HBase S3 Root Directory while launching the EMR cluster. This minimize copies on S3 that are required when restoring the snapshots, thus reducing the restore time of your tables. To avoid any conflict during the snapshot copy, you should not start the EMR cluster (if using Amazon S3 as storage layer) before the end of the snapshot copy.  TARGET_BUCKET=&quot;s3://BUCKET/PREFIX/&quot; hbase snapshot export -snapshot ${TABLE_NAME/:/_}-$DATE -copy-to $TARGET_BUCKET   Restore Table when using Amazon S3 as storage layer for HBase  If you followed the notes in the previous step, you'll find the snapshot already available in HBase after launching the cluster.  Note If your snapshot was created from a namespace different from the default one, make sure to pre create it, to avoid failures while restoring the snapshot. From the EMR master node:  # Verify snapshot availability HBASE_CMD=&quot;sudo -u hbase hbase&quot; $HBASE_CMD snapshot info -list-snapshots # Review snapshot info and details SNAPSHOT_NAME=&quot;YOUR_SNAPSHOT_NAME&quot; $HBASE_CMD snapshot info -snapshot $SNAPSHOT_NAME -size-in-bytes -files -stats -schema # Optional - Create namespaces required by the snapshot echo &quot;create_namespace \\&quot;$NAMESPACE_NAME\\&quot;&quot; | $HBASE_CMD shell # Restore table from snapshot echo &quot;restore_snapshot \\&quot;$SNAPSHOT_NAME\\&quot;&quot; | $HBASE_CMD shell   Scripts  The following scripts allows you to migrate and restore HBase tables an namespaces using the snapshot procedure previously described.  Snapshot export - Generate HBase snapshots for all the tables stored in all the namespaces, and copy them on an Amazon S3 bucket.Snapshot import - Restore all the snapshots stored in an Amazon S3 bucket.  ","version":"Next","tagName":"h2"},{"title":"Snapshots with Incremental Export​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#snapshots-with-incremental-export","content":" This approach might help in those situations where you want to migrate your data but at the same time you cannot tolerate much downtime in your production system. This approach helps to perform an initial bulk migration using the HBase snapshot procedure previously described, and then reconcile data received after the HBase snapshot generating incremental exports from the SOURCE table.  This approach works when the volume of ingested data is not high, as the procedure to reconcile the data in the DESTINATION cluster might require multiple iterations to synchronize the two clusters, along with the fact that might be error prone. The following highlights the overall migration procedure.  In the SOURCE cluster:  Create a snapshot of the HBase table you want to migrate. Collect the epoch time when the snapshot was taken, as this will be used to determine new data ingested in the cluster. Export the snapshot on Amazon S3 org.apache.hadoop.hbase.snapshot.ExportSnapshot  In the DESTINATION cluster:  Import the snapshot in the cluster and restore the table  In the SOURCE cluster:  Generate an incremental export to S3 for data arrived in the cluster after taking the snapshot using the HBase utility org.apache.hadoop.hbase.mapreduce.Export  In the DESTINATION cluster:  Restore the missing data in the destination cluster using the HBase utility org.apache.hadoop.hbase.mapreduce.Import  Example Export Commands  ## Configurations HBASE_CMD=&quot;sudo -u hbase hbase&quot; BUCKET_NAME=&quot;YOUR_BUCKET_NAME&quot; SNAPSHOT_PATH=&quot;s3://$BUCKET_NAME/hbase-snapshots/&quot; TABLE_NAME=&quot;TestTable&quot; # ============================================================================== # (Simulate) Create TestTable with 1000 rows # ============================================================================== $HBASE_CMD pe --table=$TABLE_NAME --rows=1000 --nomapred sequentialWrite 1 # ============================================================================== # Take initial table snapshot and copy it to S3 # ============================================================================== DATE=`date +&quot;%Y%m%d&quot;` EPOCH_MS=`date +%s%N | cut -b1-13` LABEL=&quot;$DATE-$EPOCH_MS&quot; # snapshot creation # Note: HBase performs a FLUSH by default when creating a snapshot # You can change this behaviour specifying the -s parameter $HBASE_CMD snapshot create -n &quot;${LABEL}-${TABLE_NAME}&quot; -t $TABLE_NAME # copy to S3 $HBASE_CMD org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot &quot;${LABEL}-${TABLE_NAME}&quot; -copy-to $SNAPSHOT_PATH # ============================================================================== # (Simulate) Data mutations to simulate data arrived after taking the snapshot # ============================================================================== # overwrite the first 100 elements of the table $HBASE_CMD pe --rows=100 --nomapred sequentialWrite 1 # check first 100 rows will have an higher timestamp compared to the 101 element echo &quot;scan '$TABLE_NAME', {LIMIT =&gt; 101}&quot; | $HBASE_CMD shell # ============================================================================== # Generate incremental data export # ============================================================================== # Retrieve the epoch time from the snapshot name that was previously created. # This allow us to only export data modified since that moment in time. $HBASE_CMD snapshot info -list-snapshots # Incremental updates LATEST_SNAPSHOT_EPOCH=&quot;$EPOCH_MS&quot; NEW_EPOCH_MS=`date +%s%N | cut -b1-13` INCREMENTAL_PATH=&quot;s3://$BUCKET_NAME/hbase-delta/${TABLE_NAME}/${NEW_EPOCH_MS}&quot; $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Export ${TABLE_NAME} $INCREMENTAL_PATH 1 $LATEST_SNAPSHOT_EPOCH   Example Import Commands  ## Configurations HBASE_CMD=&quot;sudo -u hbase hbase&quot; BUCKET_NAME=&quot;YOUR_BUCKET_NAME&quot; SNAPSHOT_PATH=&quot;s3://$BUCKET_NAME/hbase-snapshots/&quot; HBASE_CONF=&quot;/etc/hbase/conf/hbase-site.xml&quot; HBASE_ROOT=$(xmllint --xpath &quot;//configuration/property/*[text()='hbase.rootdir']/../value/text()&quot; $HBASE_CONF) # ============================================================================== # Import and Restore HBase snapshot # ============================================================================== ## List Snapshots on S3 and take note of the snapshot you want to restore $HBASE_CMD snapshot info -list-snapshots -remote-dir $SNAPSHOT_PATH SNAPSHOT_NAME=&quot;SNAPSHOT_NAME&quot; # e.g. &quot;20220817-1660726018359-TestTable&quot; ## Copy snapshot on the cluster $HBASE_CMD snapshot export \\ -D hbase.rootdir=$SNAPSHOT_PATH \\ -snapshot $SNAPSHOT_NAME \\ -copy-to $HBASE_ROOT # Restore initial snapshot echo &quot;restore_snapshot '$SNAPSHOT_NAME'&quot; | $HBASE_CMD shell # ============================================================================== # Replay incremental updates # ============================================================================== TABLE_NAME=$(echo $SNAPSHOT_NAME | awk -F- '{print $3}') INCREMENTAL_PATH=&quot;s3://$BUCKET_NAME/hbase-delta/${TABLE_NAME}/${NEW_EPOCH_MS}&quot; $HBASE_CMD org.apache.hadoop.hbase.mapreduce.Import ${TABLE_NAME} ${INCREMENTAL_PATH}   ","version":"Next","tagName":"h2"},{"title":"Snapshots with HBase Replication​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#snapshots-with-hbase-replication","content":" This approach describes how to migrate data using the HBase cluster replication feature that allows you to establish a peering between two (or more) HBase clusters so that they can replicate incoming data depending on how the peering was established.  In order to use this approach, a network connection between the SOURCE and DESTINATION cluster should be present. If you're transferring data from an on premise cluster and you have large volumes of data to replicate, you might establish the connection between the two clusters using AWS Direct Connect or you can establish a VPN connection if this is a one time migration.  The below section highlight the overall procedure to establish the replication.  In the SOURCE cluster, create a HBase peering with the DESTINATION cluster and then disable the peering so that data is accumulated in the HBase WALs.In the SOURCE cluster, take a snapshot of the table you want to migrate and export it to S3.In the DESTINATION cluster, import and restore the snapshot. This creates the metadata (table description) required for the replication and also restore the data present in the snapshot.In the SOURCE cluster, re-enable the HBase peering with the DESTINATION cluster, so that data modified up to that moment will start to be replicated in the DESTINATION cluster.Monitor the replication process from the HBase shell to verify the lag of replication before completely switch on the DESTINATION cluster, and shutdown the SOURCE cluster.  Create one-way peering: SOURCE → DESTINATION  Note The configuration for the replication should be enabled by default in HBase. To double check, verify hbase.replication is set to true in the hbase-site.xml in the SOURCE cluster.  To create the HBase peering, you need to know the DESTINATION ip or hostname of the node where the Zookeeper ensemble used by HBase is located. If the destination cluster is an Amazon EMR cluster this coincides with the EMR master node.  Once collected this information, from the SOURCE cluster execute the following commands to enable the peering with the destination cluster and start accumulating new data in the HBase WALs:  # The HBase command might be different in your Hadoop environment depending on # how HBase was installed and which user is used to properly launch the cli. # In most installations, it's sufficient to use the `hbase` command only. HBASE_CMD=&quot;sudo -u hbase hbase&quot; MASTER_IP=&quot;**YOUR_MASTER_IP**&quot; # e.g. ip-xxx-xx-x-xx.eu-west-1.compute.internal PEER_NAME=&quot;aws&quot; TABLE_NAME=&quot;**YOUR_TABLE_NAME**&quot; ## Create peering with the destination cluster echo &quot;add_peer '$PEER_NAME', CLUSTER_KEY =&gt; '$MASTER_IP:2181:/hbase'&quot; | $HBASE_CMD shell ## List peers in the source cluster echo &quot;list_peers&quot; | $HBASE_CMD shell ## Disable the peer just created, so that we can keep new data in the LOG (HBase WALs) until the snapshots are restored in the DESTINATION cluster echo &quot;disable_peer '$PEER_NAME'&quot; | $HBASE_CMD shell ## enable replication for the tables to replicate echo &quot;enable_table_replication '$TABLE_NAME'&quot; | $HBASE_CMD shell   Now you can switch to the DESTINATION cluster and restore the initial snapshot taken for the table. Once the restore is complete, switch again on the SOURCE cluster and enable the HBase peering to start replicating new data ingested in the SOURCE cluster since the initial SNAPSHOT was taken.  HBASE_CMD=&quot;sudo -u hbase hbase&quot; PEER_NAME=&quot;aws&quot; echo &quot;enable_peer '$PEER_NAME'&quot; | $HBASE_CMD shell   To monitor the replication status you could use the hbase command status 'replication' from the HBase shell on the SOURCE cluster.  ","version":"Next","tagName":"h2"},{"title":"Migrate HBase 1.x to HBase 2.x​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#migrate-hbase-1x-to-hbase-2x","content":" ","version":"Next","tagName":"h2"},{"title":"When using HDFS​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#when-using-hdfs","content":" The migration path from HBase 1.x to HBase 2.x, can be accomplished using HBase snapshots if you're using HDFS as storage layer. In this case you can take a snapshot on the HBase 1.x cluster and then restore it on the HBase 2.x one. Although it is highly recommended to migrate to the latest version of HBase 1.4.x before migrating to HBase 2.x, it is still possible to migrate from older version of the 1.x branch (1.0.x, 1.1.x, 1.2.x, etc).  ","version":"Next","tagName":"h3"},{"title":"When using Amazon S3​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#when-using-amazon-s3","content":" If you're using Amazon S3 as storage layer for HBase, you can directly migrate any EMR cluster using an HBase version &gt;= 1.x to an Amazon EMR release using HBase &lt;= 2.2.x.  Note If you try to update to a more recent version of HBase (e.g. HBase 2.4.4 from HBase 1.x), the HBase master will fail to correctly start due to some breaking changes in the way HBase load the meta table information in newest releases. You might see a similar error in your HMaster logs:  Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException): org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family table does not exist in region hbase:meta,,1.1588230740 in table 'hbase:meta', {TABLE_ATTRIBUTES =&gt; {IS_META =&gt; 'true', coprocessor$1 =&gt; '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'}}, {NAME =&gt; 'info', VERSIONS =&gt; '3', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'NONE', IN_MEMORY =&gt; 'true', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '8192', METADATA =&gt; {'CACHE_DATA_IN_L1' =&gt; 'true'}} at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:8685) at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:3125) at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:3110)   In this case to migrate to the latest version, you can perform a two step migration:  First, disable all your HBase tables in the Amazon EMR cluster using HBase 1.x. Once all the tables are disabled, terminate this cluster.Launch a new Amazon EMR cluster using EMR 6.3.0 as release and wait for all the tables/regions to be assigned. Once completed, disable all the tables again and shutdown the cluster.Finally, launch the latest EMR Version you want to use.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Data Migration","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_migration#summary","content":" Approach\tWhen to use?\tComplexityBatch - HBase Snapshots\tData doesn't change frequently or when you can tolerate high service downtime\tEasy Incremental - HBase Snapshots + Export\tThe data doesn't change frequently and you have large tables\tMedium Online - HBase Snapshots + Replication\tData changes frequently and high service downtime cannot be tolerated\tAdvanced ","version":"Next","tagName":"h2"},{"title":"Best Practice for Amazon S3","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3","content":"","keywords":"","version":"Next"},{"title":"HBase - Speed up region assignment / opening / closing​","type":1,"pageTitle":"Best Practice for Amazon S3","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3#hbase---speed-up-region-assignment--opening--closing","content":" HBase 1.x  Set the below configurations to speed up region assignment, opening and closure on HBase 1.x clusters. These configurations specifically disable the use of zookeeper for the region assignment by setting to false the property hbase.assignment.usezk. Additionally, you can increase the thread pools the Region Servers use for opening the assigned regions. For Regions Servers handling many regions (in the order of thousands), you can set the thread pools up to 10 times the available number of vCpu on the Region Server. Below, an example EMR Configuration:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.assignment.usezk&quot;: &quot;false&quot;, &quot;hbase.regionserver.executor.openregion.threads&quot;: &quot;120&quot;, &quot;hbase.regionserver.executor.closeregion.threads&quot;: &quot;120&quot; } } ]   HBase 2.x  HBase 2.x introduced a more robust and efficient workflow to manage regions transitions which leverage the ProcedureV2 introduced in HBASE-14614. In this case, it is only sufficient to increase the default region server thread pools to speed up the initialization of the regions.  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.regionserver.executor.openregion.threads&quot;: &quot;120&quot;, &quot;hbase.regionserver.executor.closeregion.threads&quot;: &quot;120&quot; } } ]   ","version":"Next","tagName":"h2"},{"title":"HBase - Bucket Cache​","type":1,"pageTitle":"Best Practice for Amazon S3","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3#hbase---bucket-cache","content":" When using Amazon S3 as storage layer for HBase, EMR configures the service to use a Bucket Cache for persisting data blocks on the L2 Cache of each region server. The default cache implementation used for Amazon S3 persists blocks on the local volumes of the node as defined by the hbase.bucketcache.ioengine property. This parameter defines the location of the files used to store the cached data. For example, the following snippet shows the default configurations for a node with 4 EBS volumes attached.   &lt;property&gt; &lt;name&gt;hbase.bucketcache.ioengine&lt;/name&gt; &lt;value&gt;files:/mnt1/hbase/bucketcache,/mnt2/hbase/bucketcache,/mnt3/hbase/bucketcache&lt;/value&gt; &lt;/property&gt;   By default, EMR configures N - 1 volumes for caching data, so in our example only 3 volumes out of 4 will be used for the cache. This feature can be useful to persist HOT data on the local disks of the cluster to reduce the latency introduced when accessing HFiles stored on S3. However, by default the cache size is set as 8GB, so you might need to increase it depending on the amount of data you want to store on each node. To modify the default cache value, you can set the following property:  hbase.bucketcache.size: 98304 # defined as MB   In the above example, we set the cache size for each node to 98GB. In each volume only 32GB (98304 / 3) are used, as the total cache size will be evenly distributed across the volumes defined in the hbase.bucketcache.ioengine.  Besides, when using S3 it might be convenient to pre-warm the cache during the region opening to avoid performance degradation when the cache is still not fully initialized. In this case to enable blocks prefetch, you should enable the following configuration.  hbase.rs.prefetchblocksonopen: true   This configuration can also be set for individual Column Family of an HBase table. In this case you should specify the configuration through the HBase shell using the following command:  hbase&gt; create 'MyTable', { NAME =&gt; 'myCF', PREFETCH_BLOCKS_ON_OPEN =&gt; 'true' }   Finally, in write intensive use cases, it might be useful to also enable the following configurations to automatically persist blocks in the cache as they are written, and to repopulate the cache following a compaction (compaction operations invalidate cache blocks). In this case we can set the following additional properties:  hbase.rs.cacheblocksonwrite: true hbase.rs.cachecompactedblocksonwrite: true   Below a sample configuration to tune the Bucket Cache in an Amazon EMR cluster:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.bucketcache.size&quot;: &quot;98304&quot;, &quot;hbase.rs.prefetchblocksonopen&quot;: &quot;true&quot;, &quot;hbase.rs.cacheblocksonwrite&quot;: &quot;true&quot;, &quot;hbase.rs.cachecompactedblocksonwrite&quot;: &quot;true&quot; } } ]   ","version":"Next","tagName":"h2"},{"title":"HBase - Memstore flush size​","type":1,"pageTitle":"Best Practice for Amazon S3","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3#hbase---memstore-flush-size","content":" When using Amazon S3 in HBase, it might be convenient to increase the default memstore flush size to avoid performance degradation, or an excessive number of small compaction operations in write intensive clusters. This can be useful if you have manually disabled the HBase - Persistent File Tracking feature that is enabled on EMR greater than 6.2.0 or if you're using an EMR 5.x cluster.  In this case, you can increase the memstore flush size to 256MB or 512MB (default 128MB). Below an example of how you can change this configuration in an Amazon EMR cluster:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.hregion.memstore.flush.size&quot;: &quot;268435456&quot; # 256 * 1024 * 1024 } } ]   ","version":"Next","tagName":"h2"},{"title":"HBase - Region Split Policy​","type":1,"pageTitle":"Best Practice for Amazon S3","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3#hbase---region-split-policy","content":" Depending on the HBase version that you’re using, you will use different region split policies. By default, you’ll have:  HBase 1.x org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicyHBase 2.x org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy  These specific implementations aims to quickly increase the number of regions when you have a fresh new table that wasn’t pre-partitioned. This might be a good strategy for new tables in a cluster.  However, it might be more convenient for a cluster using S3 as storage layer to use the old split strategy org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy that performs a split operation only when the overall size of a region goes above a threshold as defined by the parameter: hbase.hregion.max.filesize (default: 10GB)  This can help if you want to have more control on the number of regions, as it will allow you to control the growth of the number of regions by a fixed size that you specify. Additionally, this can also be handy in case you’re leveraging Apache Phoenix to query HBase and you have a constant flow of new data. Setting a constant size region split policy will prevent excessive splitting operations. These operations can cause temporary region cache boundaries exceptions while using Phoenix, due to the time required to refresh internal metadata about regions boundaries. This problem might be more frequent when using S3 as storage layer than when using HDFS.  Below an example to modify the Region Server split logic on an Amazon EMR cluster:  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.regionserver.region.split.policy&quot;: &quot;org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy&quot;, &quot;hbase.hregion.max.filesize&quot;: &quot;10737418240&quot; } } ]   ","version":"Next","tagName":"h2"},{"title":"HBase - Persistent File Tracking​","type":1,"pageTitle":"Best Practice for Amazon S3","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_s3#hbase---persistent-file-tracking","content":" When using EMR versions greater than 6.2.0, EMR will enable a feature called Persistent File Tracking when using Amazon S3 as storage layer. This specific feature, is enabled by default and provides performance benefits as it avoids HFile rename operations that might delay write operations due to S3 latencies. However, please note that this feature does not support the native HBase replication feature. So if you want to use replication to implement a Highly Available setup when using Amazon S3, you’ll have to disable this feature. This applies only to S3 and is not required when using HDFS as storage layer.  For more details on this feature, see Persistent HFile tracking. ","version":"Next","tagName":"h2"},{"title":"Data Integrity / Disaster Recovery / High Availability","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity","content":"","keywords":"","version":"Next"},{"title":"Best Practice​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#best-practice","content":" When working with HBase on Amazon EMR, it is good practice to enable the EMR Multimaster feature that allows you to launch three EMR master nodes. This functionality allows the HBase cluster to tolerate impairments that might occur if a single master goes down. Please note that EMR on EC2 launches all the nodes of the cluster within the same Availability Zone, so this solution is not sufficient to create a robust setup for high available clusters.  Nevertheless, this functionality is highly recommended both when using HDFS or Amazon S3 as storage layer. Enabling this, allows you to serve HBase requests (both writes and reads) in case of a master failure. Please note that if you launch the EMR cluster with a single master and this node is terminated for any reason, it will not be possible to recover any data from the HDFS storage of the cluster as the HDFS metadata will be lost after the termination of the EMR master.  Moreover, it is also recommended to specify a SPREAD placement group strategy that places the master instances across separate underlying hardware to guard against the loss of multiple master nodes in the event of a hardware failure. For additional details see Amazon EMR integration with EC2 placement groups  In terms of cluster scale in / out, it’s not recommended to enable the EMR Managed Scaling or the EMR scaling with custom policies when using HBase. These features are designed to operate with YARN workloads, so they might cause data integrity issues if some nodes are terminated by the scaling policies. In case you need to scale your HBase cluster size, you can follow the below procedures:  Scale Out - Use the EMR Web Console or API to increase the number of nodes. New nodes are automatically recognized once they join the cluster, and the HBase balancer will automatically spread regions across new nodes.Scale In - Disable the HBase tables and use the EMR Web Console or API to decrease the number of nodes in the cluster.  ","version":"Next","tagName":"h2"},{"title":"HBase on HDFS​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase-on-hdfs","content":" Within this section you can find additional information to secure your data when launching an Amazon EMR cluster using HDFS as storage layer.  As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API.  If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation.  ","version":"Next","tagName":"h2"},{"title":"HBase - Snapshots​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase---snapshots","content":" HBase snapshots should be considered the first line of defense against data integrity issues that might cause a service disruption in your HBase cluster. To prevent any kind of data loss, is highly recommended to perform daily snapshots of your HBase tables. Please note that taking a snapshot of a table does not involve any data copy operation, so this operation doesn’t generate any additional data in your HDFS storage. However, it is not recommended to maintain a high number of snapshots for a table, especially if its data change frequently. Modified regions that are used by old snapshots are preserved in the archive folder within the HBase root directory, so this can have a significant impact on the amount of data retained in the HDFS.  Besides, please note that HBase snapshots are by default persisted in the same storage layer configured when launching the cluster (in this case HDFS), so they should not be considered a strong disaster recovery mechanism if you want to protect your data in case of a cluster termination. In this case, you can export the snapshots in an Amazon S3 bucket to persist all your data on a reliable storage layer.  Please note that periodic snapshots exports to S3 are recommended only if your tables have a small size (less than few TB) as an HBase export will copy all the data belonging to the snapshot in the S3 bucket using a Map Reduce job. For sample scripts and commands see the related examples in the Data Migration guide.  Additional Tips  Use a time identifier in the snapshot name that can help you identify when the snapshot was created. The creation time is also present in the snapshot metadata, but using this convention in the name can save some time while restoring an impaired cluster.  ","version":"Next","tagName":"h3"},{"title":"HBase - Cluster Replication​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase---cluster-replication","content":" The HBase cluster replication allows you to keep one or more HBase clusters synchronized between each other. Depending on how you set up the replication and peering between the clusters you can achieve different configurations to establish both a Disaster Recovery or a Highly Available setup depending on your needs.  The following sections describe typical architectures that can be achieved with this feature.  Active - Active architecture​  This first approach describes a setup that is suitable to provide Highly Available clusters that can both serve read and write requests. In this case is required to set up a two-way replication peering between the Primary and Secondary cluster as described in the below figure. In this architecture both reads and writes will be routed across the two clusters by the Elastic Load Balancer and data written in a cluster will also be replicated in the other one.    To setup this architecture you should performed the following steps:  Create two HBase clusters in different AWS Availability Zones Create an Elastic Load Balancer using an EC2 Target Group configured with the following specifications: Target Type: IP addresses. EMR master IP for the Primary and Secondary clusterProtocol: TCPPort: 2181. Default port used by the Zookeeper service Establish a Two Way replication peering between the two clusters. To enable the replication, you can run the following commands on each master node. While running these commands, please make sure to replace MASTER_NODE_IP with the IP address of the other master node. For example, if running the commands on the Primary the MASTER_IP should be set with the Secondary IP address.  HBASE_CMD=&quot;sudo -u hbase hbase&quot; MASTER_IP=&quot;MASTER_NODE_IP&quot; PEER_NAME=&quot;aws&quot; ## Create peering with the destination cluster echo &quot;add_peer '$PEER_NAME', CLUSTER_KEY =&gt; '$MASTER_IP:2181:/hbase'&quot; | $HBASE_CMD shell ## List peers in the source cluster echo &quot;list_peers&quot; | $HBASE_CMD shell   Make sure the HBase tables you want to replicate are already available in both EMR clusters, If your tables need to be initialized with the same data use the HBase snapshots to make sure they contain the same data. Enable the table replication using the following snippet for each table you want to replicate on both clusters  ## Enable replication TABLE_NAME=&quot;YOUR_TABLE_NAME&quot; HBASE_CMD=&quot;sudo -u hbase hbase&quot; echo &quot;enable_table_replication '$TABLE_NAME'&quot; | $HBASE_CMD shell   To leverage this setup, specify the Network Load Balancer endpoint in the hbase.zookeeper.quorum property used by your client applications.  This setup can tolerate impairments of an Availability Zone within the same Region and provides the best performance if you need milliseconds / sub milliseconds responses from your clusters. Please note that by default the HBase replication is an asynchronous process executed in background, and replicates WAL data across the clusters for which the replication is enabled. This means that this feature does not guarantee strong consistency when reading data. So carefully evaluate if this meet your business needs.  In case one of the two cluster is terminated or needs to be upgraded, you have to re-create the HBase peering for the new cluster and restore the table’s data and metadata in the new cluster.  Active - Passive architecture​  In a similar way as before, you can set up an Active / Passive architecture that can serve DR purposes. This can be useful if you want to have a backup cluster you can switch to in case of issues on the Primary one. The following picture highlights the overall architecture setup and components.    In order to implement the following architecture, you can perform the steps below:  Create two EMR clusters in separate Availability Zones Create a Route53 Private Hosted Zone and create an A record pointing to the EMR Master you want to act as Primary. If using the EMR Multi-Master feature, it is recommended to add all the 3 Master nodes in the record set Establish a One Way HBase replication from the EMR Primary to EMR Secondary to replicate data to the Secondary cluster. In this case, you can use the commands previously shared and execute them on the EMR Primary cluster only. Once done, specify the Route53 A record previously defined to route your client applications to the EMR Primary cluster.  This architecture serves mainly to implement a DR strategy for your HBase data. However, you can still leverage the Secondary cluster as a read replica of your data to reduce read requests on the Primary EMR Cluster. However, if you want to implement this scenario, please make sure that only client applications that have to perform READ operations (e.g. SCAN, GET) connect to the Secondary EMR cluster.  In case of failures on the EMR Primary cluster, you’ll be able to route your client application traffic to the Secondary EMR cluster by changing the IP address in the A record defined in the Route55 Private Hosted Zone. Please note that your client applications might face some failures while the A record update takes place.  Multi Region architecture​  If you have a business requirement that requires to replicate HBase data in different AWS Regions, you can still leverage the HBase cluster replication feature to synchronize data between two clusters. The setup is very similar to what previously described, but requires to establish an inter-region VPC peering between the two AWS Regions, so that HBase clusters can exchange data between each other. An example multi region Active / Active setup is depicted in the below figure.    ","version":"Next","tagName":"h3"},{"title":"HBase on Amazon S3​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase-on-amazon-s3","content":" The following section provides architectures and best practices that you can use to implement Disaster Recovery (DR) strategies and Highly Available clusters when using Amazon S3 as storage layer for your HBase clusters.  ","version":"Next","tagName":"h2"},{"title":"Storage Classes​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#storage-classes","content":" When using Amazon S3 as storage layer for your HBase cluster, all the objects created in the bucket by HBase will be created using the default S3 Standard storage class. In this case your data will be redundantly stored on a minimum of three Availability Zones within the same AWS Region. This ensures that your data is still available in your HBase cluster, either if there is an impairment in one Availability Zone.  If you want to maintain this level of data availability in case of AZ failures, it is not recommended to set any S3 Lifecycle configuration that might transition HBase files in a storage class that will reduce the internal S3 data replication (e.g. S3 One Zone-IA).  Additional Tips  Always use dedicated S3 Buckets for your HBase on S3 clusters. This minimize chances of API throttling in case other processes or applications (e.g. Spark ETL jobs) are also using the same HBase bucket.  ","version":"Next","tagName":"h3"},{"title":"HBase - Snapshots​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase---snapshots-1","content":" Although Amazon S3 already provides native functionalities to replicate objects across multiple Availability Zones, this doesn’t protect you in case of application issues that might corrupt your HBase data.  In this case, is good practice to leverage HBase existing capabilities to create periodic snapshots of your tables so that you can recover / restore tables in case of HBase inconsistencies or similar data integrity issues. Apache HBase stores snapshot data (store files and metadata) in the archive and .hbase-snapshot folders within the HBase root path. When using Amazon S3 as storage layer, this data will be replicated across multiple Availability Zones as well, as their content will be stored by default in the S3 bucket.  We recommend to create HBase snapshots using the same S3 bucket used while launching the cluster (default behavior). In this way, snapshots will leverage incremental capabilities during the snapshot creation thus minimizing the footprint of data stored in the bucket. Please note that exporting a HBase snapshot in a different S3 bucket or prefix, will force HBase to copy all data required by the snapshot. For this reason, if you manage large clusters (hundred of TB or PB data), it’s not recommended to export snapshots in different AWS Regions or S3 Buckets using this approach.  ","version":"Next","tagName":"h3"},{"title":"HBase - Cluster Replication​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase---cluster-replication-1","content":" As previously described in the HDFS section, the HBase Cluster Replication can be used to create a Highly Available cluster, or to implement different DR solutions depending on your business requirements.  When using Amazon S3 as storage layer, it’s important to remember that two HBase clusters cannot share the same S3 root directory, when they both receive write requests, as this might lead to data inconsistencies. For this reason, you should always use separate buckets for each individual HBase cluster, or as alternative use different prefixes within the same S3 bucket. This latest solution however is not ideal as it might increase the chances to face S3 throttling issues.  ","version":"Next","tagName":"h3"},{"title":"Amazon EMR - Read Replica​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#amazon-emr---read-replica","content":" The EMR HBase Read Replica feature can be used to provide High Available reads for your HBase clusters using S3 as storage layer. Although this feature does not provide additional benefits for a DR recovery mechanism, it can still be useful to serve HBase read requests, in case you want to perform a Blue / Green deployment to modify a cluster configuration on the primary Amazon EMR cluster that requires the termination of the cluster (e.g. EMR release version upgrade)  For additional details see Using a read-replica cluster in the Amazon EMR documentation.  ","version":"Next","tagName":"h3"},{"title":"Amazon S3 - Object Replication​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#amazon-s3---object-replication","content":" If you want to replicate your HBase data on a backup region for DR purposes, you might leverage the Amazon S3 Object Replication feature. This can be used to replicate objects within the same AWS Region (Same-Region Replication) or in a different region (Cross-Region Replication). This approach can be used to implement a DR mechanism that allows you to launch a Secondary cluster in a different AWS Region in case you have an impairment in your Primary one.  The overall architecture is described in the below figure.    This architecture requires you to use a DNS mechanism (for example using Route 53 hosted zones) so that you can switch between the AWS Regions in case of failures. This approach requires the following components:  Primary Amazon EMR cluster with his own dedicated S3 Bucket.Secondary Amazon EMR cluster that will only be launched in case of failures, with his own dedicated S3 bucket. The secondary cluster can be launched in the same AWS Region as the primary or in a different one depending on the requirements.Active S3 Replication between the Primary and the Secondary S3 buckets to replicate S3 objectsA DNS setup that allows you to switch your HBase clients from the primary to the secondary in case of failures. For example this might be achieved using a Route 53 private hosted zones  As previously described, the Secondary cluster should only be launched in case of an impairment in the Primary Region or Availability Zone failure. The cluster can be launched with the same configurations and sizes as the primary, but should point to a different S3 bucket. We also recommend to launch the secondary cluster using the Amazon EMR HBase read replica feature to be sure that no new data will be written on the secondary cluster. This prevent the secondary cluster to receive new data, but simplify the recovery after an impairment.  In order to enable the S3 Object Replication, you should follow the steps below:  Create two Amazon S3 buckets that will be respectively used to store production and replicated data. Make sure to enable the Amazon S3 Versioning, as this functionality is required to enable the S3 replication.In the primary bucket, create a new S3 replication rule to replicate data generated by the primary cluster. You can follow the example in the Amazon S3 documentation to enable the replication in the bucket. While creating the replication rule, make sure to adhere the following best practices: Enable the S3 Object Replication only for the HBase root prefix specified when launching the cluster. This help mitigating delay problems that might occur if you also have objects outside the HBase root prefix that should be replicated.Enable the Replication Time Control (RTC)capabilities. This feature is designed to replicate 99.99% of objects within 15 minutes after upload. Enabling this feature will also automatically enable the S3 replication metrics that are review the pending replication objects.Enable the Delete Marker Replication  Additionally, is also recommend to create a Lifecycle Rule to delete expired object delete markers, incomplete multipart uploads, and non current version of files.  This architecture serves mainly to implement a cost effective DR strategy for your HBase data as only one active cluster will be running. In case of failover, before switching to the secondary cluster, check the S3 replication metrics to verify there are no pending objects to be replicated.  Additional Considerations  Amazon EMR implements internal features that prevents the clusters to be terminated in case of Availability Zone or service issues. If your primary cluster cannot be reached, you might want to launch another cluster pointing to the same Amazon S3 bucket in a different AZ. However, this might lead to inconsistencies in case your Primary HBase cluster is not terminated as you might end up in a situation where two active HBase clusters are pointing to the same S3 root bucket. For this reason, you might want to implement the following safety measures in case of service issues:  If you only require to continue supporting HBase read operations, you can launch a backup cluster pointing to the same S3 root directory until we solve the problem. As alternative, if you’re not able to determine if your cluster instances are terminated (e.g. the failure also impact your ability to use the EC2 service) you might contact our Support to verify if the cluster was terminated to decide launching a new active cluster instead of just launch a HBase read replica.If you want to continue to support HBase write requests within the same Region, you’ll have to leverage a backup S3 bucket where data have been replicated using the S3 Object Replication or HBase cluster replication to avoid data inconsistencies if the Primary EMR cluster has not been yet terminated. As in the previous scenario, you can also contact our Support to determine if the Primary cluster was already terminated, but this might delay the recovery time.  ","version":"Next","tagName":"h3"},{"title":"HBase WALs​","type":1,"pageTitle":"Data Integrity / Disaster Recovery / High Availability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/data_integrity#hbase-wals","content":" When using Amazon S3 as storage layer, HBase still stores WALs on the local HDFS of the cluster. WALs are used internally by HBase to replay mutate operations in case of a region failure. Please note that every mutation request on HBase is first written on a WAL file, then in the HBase memstore and only after a Memstore flush this data will be persisted on S3. If the Amazon EMR cluster is terminated due to an incident, you might lose the latest data not yet persisted on S3.  In this case is a good practice to leverage a persistent event store solution, like Amazon MSK or Amazon Kinesis to retain the latest ingested data, so that you’ll be able to replay any missing data from the moment of the service interruption.  As alternative, you can configure your HBase cluster to store WALs on a persistent storage layer as an external HDFS cluster, or an Amazon EFS filesystem. This last solution might increase the latency of your write operations on HBase so you might want to verify if this solution met your SLA requirements. To configure WALs on Amazon EFS you can use the procedure described here.  If you prefer to simply leverage another HDFS cluster, you can configure the new path using the following EMR Classification when launching your HBase Cluster.  [ { &quot;classification&quot;: &quot;hbase-site&quot;, &quot;properties&quot;: { &quot;hbase.wal.dir&quot;: &quot;hdfs://HDFS_MASTER_NODE:8020/PATH&quot; } } ]   Please note that the PATH used in the external HDFS cluster should be pre-created before launching the cluster and should be writable by the following user and group: hbase:hbase. Additionally, if your external HDFS cluster is secured with Kerberos authentication, you also need to configure your HBase cluster with Kerberos, and both clusters should leverage the same Kerberos REALM to be able to communicate between each other. For additional information, see External KDC in the Amazon EMR documentation. ","version":"Next","tagName":"h3"},{"title":"Performance Tests","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/performance_tests","content":"","keywords":"","version":"Next"},{"title":"Evaluation Framework​","type":1,"pageTitle":"Performance Tests","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/performance_tests#evaluation-framework","content":" Typically, there are different aspects you want to check depending on how your cluster will be used. However, there are two major metrics that are important to define a baseline for the cluster performance: operation throughput (number of requests we can serve for a specific operation in a given period of time, e.g. GET) and operation latency (time required to acknowledge a client request).  It’s very important to baseline these metrics in a production cluster. They will give you hints on when to scale nodes based on clients requests during the day, and they can suggest configuration tuning if it is not matching expected performance.  Typically, you can perform a benchmark in an HBase cluster following the steps below:  Write / data load This is always the first step in the process as you should populate some tables with mock data to perform read tests or simply to evaluate the maximum throughput you can achieve during write operations. For this test, it is important to mimic as much as possible the average payload size of the data that will be ingested in the cluster. This can help to evaluate the number of compactions performed with the ingested volume and see the performance degradation that you might expect during these operations. Besides, this will also give you an idea of the maximum number of write requests you can serve with the specified cluster topology. Read / latency / cache This is the next step to define our baseline. The major aim of this test should be to verify the max throughput that the cluster can serve and understand how well you are leveraging the HBase cache to improve response latency.  As best practice for running these tests, you can follow the following rules:  Separate the clients from the HBase cluster. The goal is to collect metrics without having to care about resources used in our cluster. So as best practice, you should run your client fleet on a separate cluster. If your clients are on a separate cluster, make sure that your fleet is co-located on the same subnet of the cluster. This will improve response latency and avoid extra costs you might incur for data trasfer across Availability Zones. Use EMR Configurations defined as JSON files that are stored on an Amazon S3 bucket to launch your test clusters. This will help you to more easily export configurations used in your QA environment to production. Moreover, it will be easier to track specific configurations used in a test cluster, rather than setting them manually while launching the cluster.  The following section describes some tools that can be used to baseline an HBase cluster.  ","version":"Next","tagName":"h2"},{"title":"Performance Evaluation Tool​","type":1,"pageTitle":"Performance Tests","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/performance_tests#performance-evaluation-tool","content":" The first tool we’re going to use is the HBase Performance Evaluation utility that is already available in your Amazon EMR cluster. This utility can be invoked using the following syntax from the EMR master node:  hbase pe &lt;OPTIONS&gt; &lt;command&gt; &lt;nclients&gt;   The tool allow us to perform both write and read operations specifying different options to control several aspects of our tests (e.g. create a partitioned table, disable WAL flush, etc.)  For example, the following command allows us to create a table called MyWriteTest that will be pre-partitioned with 200 regions (--presplit) and we’re going to write 2GB (--size) of data using a single client. We also enable the latency parameter to report operation latencies that help us to identify if the response time met our requirements.  hbase pe --table=MyWriteTest --presplit=200 --size=2 --latency --nomapred randomWrite 1   As described in the Log section, the log output will be stored in the /var/log/hbase/hbase.log file. Please make sure to run the previous command as hbase user, or you’ll not have the permissions to modify this file using the standard hadoop user. The following shows a sample output for the previous command:  INFO [TestClient-0] hbase.PerformanceEvaluation: Latency (us) : mean=21.45, min=1.00, max=480941.00, stdDev=992.53, 50th=2.00, 75th=2.00, 95th=2.00, 99th=3.00, 99.9th=24.00, 99.99th=37550.00, 99.999th=46364.23 INFO [TestClient-0] hbase.PerformanceEvaluation: Num measures (latency) : 2097151 INFO [TestClient-0] hbase.PerformanceEvaluation: Mean = 21.45 ... INFO [TestClient-0] hbase.PerformanceEvaluation: No valueSize statistics available INFO [TestClient-0] hbase.PerformanceEvaluation: Finished class org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest in 42448ms at offset 0 for 2097152 rows (48.62 MB/s) INFO [TestClient-0] hbase.PerformanceEvaluation: Finished TestClient-0 in 42448ms over 2097152 rows INFO [main] hbase.PerformanceEvaluation: [RandomWriteTest] Summary of timings (ms): [42448] INFO [main] hbase.PerformanceEvaluation: [RandomWriteTest duration ] Min: 42448ms Max: 42448ms Avg: 42448ms INFO [main] hbase.PerformanceEvaluation: [ Avg latency (us)] 21 INFO [main] hbase.PerformanceEvaluation: [ Avg TPS/QPS] 49405 row per second   As you can see this will report min, max and avg response latency for our write requests, along with throughput information about the max number of calls served by the cluster. Please note that in our example we used the nomapred parameter that will use a local thread to perform the test (in this case client resides on the EMR master node).  If we want to generate a higher number of requests is better to remove this option, so that the utility will use a Map Reduce (MR) job to perform the test. In this last scenario, it might be convenient to run the MR job on a separate cluster, to avoid using resources (cpu, network bandwidth) from our HBase cluster and gather more realistic results.  For example, the same tests can performed from a separate EMR cluster adding the following parameter: -Dhbase.zookeeper.quorum=TARGET_HBASE_MASTER_DNS, and replacing TARGET_HBASE_MASTER_DNS with the EMR master hostname we want to test.  hbase pe -Dhbase.zookeeper.quorum=ip-xxx-xx-x-xxx.compute.internal --table=MyWriteTestTwo --presplit=200 --size=2 --latency randomWrite 1   In the same way we can perform Read test operations. For a detailed list of all options and tests available in the utility, please check the help section of the tool from ther command line.  ","version":"Next","tagName":"h2"},{"title":"YCSB​","type":1,"pageTitle":"Performance Tests","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/performance_tests#ycsb","content":" Another popular tool to benchmark your HBase cluster is YCSB (Yahoo Cloud Serving Benchmark). This utility is not available on Amazon EMR, so it should be manually installed on the EMR master itself, or on a separate EC2 instance.  This tool, unlike the previous one, is more focused on testing workloads patterns. In fact, in doesn’t provide several options as the HBase PE utility, but allows you to define different types of workloads (typically called workload A,B,C,D, etc.) where you can mix different volumes of write/read/mutate operations, along with sizes of the data that are going to be read or modified.  By default, the tool comes with pre-defined templates to tests some standard workloads patterns. For example, workload A performs 50% of read operations and 50% of update operations using 1KB payloads for each row.  This tool is especially useful, when you know exactly your workloads patterns, and you want to simulate more realistic use cases. However, please note that the tool can only launch multithreaded clients on the same node. So if you have a large cluster that you want to test, you’ll have to configure a fleet of EC2 instances and run the clients from each node using some automation scripts. ","version":"Next","tagName":"h2"},{"title":"5.2 - Hive","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hive/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 5.2.1 - Upgrading Hive Metastore​","type":1,"pageTitle":"5.2 - Hive","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hive/best_practices#bp-521-----upgrading-hive-metastore","content":" When upgrading from EMR 5.x (hive 2.x) to EMR 6.x (hive 3.x), hive metastore requires a schema upgrade to support changes and new features in Hive 3.x The following steps assumes hive metastore is running on Amazon RDS. Upgrading hive metastore is backwards compatible. Once hive metastore is upgraded, both hive 2x and hive 3.x clients/clusters can use the same hive metastore.  1. Take a snapshot of current Hive Metastore on Amazon RDS  2. Provision a new Amazon RDS with the snapshot that was created in step 1  3. Provision target EMR 6.x version without configuring an external hive metastore  4. SSH into EMR 6 cluster and update the below in hive-site.xml to point to new RDS from the previous step  &quot;javax.jdo.option.ConnectionURL&quot;: &quot;jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true&quot;, &quot;javax.jdo.option.ConnectionDriverName&quot;: &quot;org.mariadb.jdbc.Driver&quot;, &quot;javax.jdo.option.ConnectionUserName&quot;: &quot;username&quot;, &quot;javax.jdo.option.ConnectionPassword&quot;: &quot;password&quot;   5. Run the following to check current hms schema version:  hive --service schemaTool -dbType mysql -info   You should see the below. Make note of the current metastore schema version (2.3.0 in this case)  Hive distribution version: 3.1.0 Metastore schema version: 2.3.0 org.apache.hadoop.hive.metastore.HiveMetaException: Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0 Use --verbose for detailed stacktrace. *** schemaTool failed \\***   6. Change directory to the hive metastore upgrade scripts location  cd /usr/lib/hive/scripts/metastore/upgrade/mysql   Doing these steps on EMR 6.x is required because you need need the target hive distribution version (hive 3.1) and the upgrade scripts inorder to ugprade the schema.  7. Connect to mysql using below command and upgrade the schema as per the hive version. For example, if you are upgrading from 2.3.0 to 3.1.0, you would need to source the 2 scripts. Scripts can also be found in this location: https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql  mysql -u&lt;HIVEUSER&gt; -h&lt;ENDPOINT-RDS&gt; -p'PASSWORD' mysql&gt; use hive; mysql&gt; source upgrade-2.3.0-to-3.0.0.mysql.sql; mysql&gt; source upgrade-3.0.0-to-3.1.0.mysql.sql;   8. Verify the upgrade was succesful  /usr/lib/hive/bin/schematool -dbType mysql -info Metastore connection URL: jdbc:mysql://hostname:3306/hive?createDatabaseIfNotExist=true Metastore Connection Driver : org.mariadb.jdbc.Driver Metastore connection User: admin Hive distribution version: 3.1.0 Metastore schema version: 3.1.0 schemaTool completed   9. After all commands are run, terminate the cluster.  10. Further validation: Provision new 5.x and 6.x cluster with updated hive-site.xml that points to new RDS. In both version, you can run  hive --service schemaTool -dbType mysql -info   In 5.x you'll see  Hive distribution version: 2.3.0 Metastore schema version: 3.1.0   and in 6.x, you'll see  Hive distribution version: 3.1.0 Metastore schema version: 3.1.0  ","version":"Next","tagName":"h2"},{"title":"Best Practice for HDFS","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs","content":"","keywords":"","version":"Next"},{"title":"HDFS - Name Node memory​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hdfs---name-node-memory","content":" When handling large cluster deployments, it’s important to properly size the HDFS NameNode (NN) heap memory which Amazon EMR set accordingly to the instance used. The NN keeps in memory metadata for each file / block allocated in the HDFS, so it’s important to properly size the memory to prevent failures that might create down-times in our services.  To size the NN memory, we can consider that each HDFS block persisted in memory uses approximately 150 bytes. Using this value as reference, you can do a rough estimate of the memory required to store data in the HDFS, considering that a block is 128MB (please note that a file smaller than the HDFS block size will still count as a individual block in memory). As alternative, you can use a rule of thumb and specify 1GB of memory each 1 million blocks stored in the HDFS.  To change the default NN memory, you can use the following EMR Configuration:  [ { &quot;Classification&quot;: &quot;hadoop-env&quot;, &quot;Configurations&quot;: [ { &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;HADOOP_NAMENODE_HEAPSIZE&quot;: &quot;8192&quot; } } ], &quot;Properties&quot;: {} } ]   ","version":"Next","tagName":"h2"},{"title":"HDFS - Service Threads​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hdfs---service-threads","content":" Amazon EMR already configures most of the HDFS parameters that are required to get good HDFS performance for HBase. However, if you’re using a large instance with several vCpu, you might benefit in increasing the number of service threads that are available for the HDFS DataNode service. Please note that if you’re using HDFS - Short Circuit Reads you might not get any additional benefits from this parameter tuning, but this might still be handy if your HDFS is used by other applications.  In this case, setting the dfs.datanode.handler.count to 3 times the number of vCpu available on the node can be a good starting point. In the same way we can also tune the number of dfs.namenode.handler.count for larger cluster installations. For this last parameter, you can use the following formula to determine a good value for your cluster  20 * log2(number of CORE nodes)   Please note that this value might be useful to increase, if you have more than 20 CORE nodes provisioned in the cluster, otherwise you might stick to the default values set by the service. Also for both dfs.namenode.handler.count and dfs.datanode.handler.count you should not set a value higher than 200.  [ { &quot;Classification&quot;: &quot;hdfs-site&quot;, &quot;Properties&quot;: { &quot;dfs.namenode.handler.count&quot;: &quot;64&quot;, &quot;dfs.datanode.handler.count&quot;: &quot;48&quot; } } ]   ","version":"Next","tagName":"h2"},{"title":"HDFS - Short Circuit Reads​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hdfs---short-circuit-reads","content":" In HDFS, reads normally go through the Data Node service. When the client asks the Data Node to read a file, the service reads that file off of the disk and sends the data to the client over a TCP socket. The &quot;short-circuit reads&quot; bypass the Data Node, allowing the client to read the file directly. This is only possible in cases where the client is co-located with the data.  The following configurations allow HBase to directly read store files on the local node bypassing the HDFS service providing better performance while accessing data not cached.  [ { &quot;Classification&quot;: &quot;hdfs-site&quot;, &quot;Properties&quot;: { &quot;dfs.client.read.shortcircuit&quot;: &quot;true&quot;, &quot;dfs.client.socket-timeout&quot;: &quot;60000&quot;, &quot;dfs.domain.socket.path&quot;: &quot;/var/run/hadoop-hdfs/dn_socket&quot; } } ]   For additional details, see HDFS Short-Circuit Local Reads  ","version":"Next","tagName":"h2"},{"title":"HDFS - Replication Factor​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hdfs---replication-factor","content":" As best practice is recommended to launch the EMR cluster using at least 4 CORE nodes. When you launch an EMR cluster with at least 4 CORE nodes, the default HDFS replication factor will be automatically set to 2 by the EMR service. This prevents to lose data in case some CORE nodes get terminated. Please note that you cannot recover a HDFS block if all its replicas are lost (e.g. all CORE nodes containing a specific HDFS block and its replica are terminated). If you want a stronger guarantee about the availability of your data, launch the EMR cluster with at least 10 CORE nodes (this will set the default replication factor to 3), or manually specify the HDFS replication factor using the EMR Configuration API.  If you specify the HDFS replication manually, please make sure to have a sufficient number of CORE nodes to allocate all the replica of your data. For more details see HDFS configuration in the Amazon EMR documentation.  ","version":"Next","tagName":"h2"},{"title":"HBase - Hedged Reads​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hbase---hedged-reads","content":" Hadoop 2.4 introduced a new feature called Hedged Reads. If a read from a block is slow, the HDFS client starts up another parallel read against a different block replica. The result of whichever read returns first is used, and the outstanding read is cancelled. This feature helps in situations where a read occasionally takes a long time rather than when there is a systemic problem. Hedged reads can be enabled for HBase when the HFiles are stored in HDFS. This feature is disabled by default.  To enable hedged reads, set dfs.client.hedged.read.threadpool.size to the number of threads to dedicate to running hedged threads, and dfs.client.hedged.read.threshold.millis to the number of milliseconds to wait before starting another read against a different block replica.  The following is an example configuration to enable hedged reads using EMR Configurations:  [ { &quot;Classification&quot;: &quot;hdfs-site&quot;, &quot;Properties&quot;: { &quot;dfs.client.hedged.read.threadpool.size&quot;: &quot;20&quot;, &quot;dfs.client.hedged.read.threshold.millis&quot;: &quot;100&quot; } } ]   ","version":"Next","tagName":"h2"},{"title":"HBase - Tiered Storage​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#hbase---tiered-storage","content":" HBase can take advantage of the Heterogeneous Storage and Archival Storage feature available in the HDFS to store more efficiently data in different type of storage and provide better performance.  One of the use case where this setup might be useful, is for write intensive clusters that have a high ingestion rate and trigger a lot of internal compaction operations. In this case we can define a policy to store HBase WALs on SSD disks present in our nodes (NVMe instance store volumes), while storing HFiles on additional EBS volumes attached to our instances. Please note that this is an advanced configuration that requires additional steps to be enabled on an EMR cluster and might not be beneficial for small clusters with simple ingestion patterns.  Amazon EMR automatically configures both instances volumes stores and EBS disks that are defined while launching the cluster. However, we need to label the volumes attached to our node to specify the corresponding Storage Type for the corresponding volume.  The first step is to attach a Bootstrap Action while launching the cluster to label NVMe disks. You can use the following script to label as SSD the NVMe disks attached to the cluster's nodes.  #!/bin/bash #=============================================================================== #!# script: emr-ba-disk_labels.sh #!# version: v0.1 #!# #!# This Bootstrap Action can be attached to an EMR Cluster to automatically #!# tag NVMe Disks using the HDFS Storage Type SSD. #!# #=============================================================================== #?# #?# usage: ./emr-ba-disk_labels.sh #?# #=============================================================================== # Force the script to run as root if [ $(id -u) != &quot;0&quot; ] then sudo &quot;$0&quot; &quot;$@&quot; exit $? fi ## Install nvme-cli yum install -y nvme-cli cd /tmp &amp;&amp; wget -O epel.rpm –nv https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y ./epel.rpm &amp;&amp; yum -y install xmlstarlet ## List NVMe disks nvme_disks=($(nvme list | grep &quot;Amazon EC2 NVMe Instance Storage&quot; | awk -F'[[:space:]][[:space:]]+' '{print $1}')) ## If there's no nvme exit [[ ${#nvme_disks[@]} -eq 0 ]] &amp;&amp; echo &quot;No EC2 NVMe Instance Storage found. End script...&quot; &amp;&amp; exit 0 SCRIPT_NAME=&quot;/tmp/disk_labels.sh&quot; cat &lt;&lt; 'EOF' &gt; $SCRIPT_NAME #!/bin/bash # retrieve dfs.data.dir value HDFS_CORE_SITE=&quot;/etc/hadoop/conf/hdfs-site.xml&quot; nvme_disks=($(nvme list | grep &quot;Amazon EC2 NVMe Instance Storage&quot; | awk -F'[[:space:]][[:space:]]+' '{print $1}')) for disk in &quot;${nvme_disks[@]}&quot;; do # Find corresponding mounted partition mount_path=$(mount | grep &quot;$disk&quot; | awk -F'[[:space:]]' '{print $3}') echo &quot;Apply Hadoop Storaget Type Label [SSD] to $disk ($mount_path)&quot; curr_value=$(xmlstarlet sel -t -v '//configuration/property[name = &quot;dfs.data.dir&quot;]/value' $HDFS_CORE_SITE) echo &quot;current: $curr_value&quot; new_value=$(echo $curr_value | sed &quot;s|$mount_path|[SSD]$mount_path|g&quot;) echo &quot;new: $new_value&quot; xmlstarlet ed -L -u &quot;/configuration/property[name='dfs.data.dir']/value&quot; -v &quot;$new_value&quot; $HDFS_CORE_SITE done systemctl restart hadoop-hdfs-datanode.service exit 0 EOF chmod +x $SCRIPT_NAME sed -i &quot;s|null &amp;|null \\&amp;\\&amp; bash $SCRIPT_NAME &gt;&gt; \\$STDOUT_LOG 2&gt;&gt; \\$STDERR_LOG 0&lt;/dev/null \\&amp;|&quot; /usr/share/aws/emr/node-provisioner/bin/provision-node exit 0   Once done, we can specify the following HBase configuration in the hbase-site in order to store our WALs files on SSD disks only.  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.wal.storage.policy&quot;: &quot;ALL_SSD&quot; } } ]   By doing this, WALs will be allocated and persisted on the HDFS using disks that have been labeled as SSD. To verify the setup, you can run the following command from the EMR master node that will display the corresponding allocations on the blocks on the HDFS for WALs.  # Describe block allocation for hbase root dir hdfs fsck /user/hbase/WALs -files -blocks -locations # Sample output /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746301597 135252162 bytes, replicated: replication=1, 1 block(s): OK 0. BP-581531277-172.31.3.43-1674746228762:blk_1073741836_1012 len=135252162 Live_repl=1 [DatanodeInfoWithStorage[172.31.3.138:9866,DS-5ef6e227-738d-4cb5-9fc9-4d636744674d,SSD]] /user/hbase/WALs/ip-172-31-3-138.eu-west-1.compute.internal,16020,1674746296461/ip-172-31-3-138.eu-west-1.compute.internal%2C16020%2C1674746296461.1674746426864 135213883 bytes, replicated: replication=1, 1 block(s): OK 0. BP-581531277-172.31.3.43-1674746228762:blk_1073742073_1255 len=135213883 Live_repl=1 [DatanodeInfoWithStorage[172.31.3.138:9866,DS-bf9acb8e-ad9f-4757-a8cb-59b9d1d0e659,SSD]]   Based on this example, you can create more complex scenarios depending on the volumes attached to the nodes.  HBase also provides another useful feature called Heterogeneous Storage for Date Tiered Compaction to better handle cold and hot data separation. However, this feature has been introduced in the newer HBase 3.x versions only.  ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Best Practice for HDFS","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/best_practice_hdfs#summary","content":" The following summarize a minimal set of configurations you can tune to improve the performance on an HDFS cluster.  [ { &quot;Classification&quot;: &quot;hbase-site&quot;, &quot;Properties&quot;: { &quot;hbase.regionserver.handler.count&quot;: &quot;120&quot; } }, { &quot;Classification&quot;: &quot;hdfs-site&quot;, &quot;Properties&quot;: { &quot;dfs.namenode.handler.count&quot;: &quot;64&quot;, &quot;dfs.datanode.handler.count&quot;: &quot;48&quot;, &quot;dfs.client.hedged.read.threadpool.size&quot;: &quot;20&quot;, &quot;dfs.client.hedged.read.threshold.millis&quot;: &quot;100&quot;, &quot;dfs.client.read.shortcircuit&quot;: &quot;true&quot;, &quot;dfs.client.socket-timeout&quot;: &quot;60000&quot;, &quot;dfs.domain.socket.path&quot;: &quot;/var/run/hadoop-hdfs/dn_socket&quot; } } ]  ","version":"Next","tagName":"h2"},{"title":"Observability","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability","content":"","keywords":"","version":"Next"},{"title":"Logs​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#logs","content":" By default, Amazon EMR stores HBase logs under the following directory: /var/log/hbase. Depending on the role of the node you are connected to (e.g. MASTER, CORE or TASK) you might see different types of log files stored within this folder. Below a list of log files generated by HBase:  hbase-hbase-master-ip-xxx-xxx-xxx-xxx.log This file is present on the EMR master node only and provides details about operations performed by the HBase master service. In an impaired cluster, this is typically the first file to look at to identify the origin of the problem. hbase-hbase-rest-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about REST API requests. If you’re not using the HTTP REST API to communicate with HBase, then you can skip this file. By default, Amazon EMR exposes HTTP REST API for HBase on port 8070. hbase-hbase-thrift-ip-xxx-xxx-xxx-xxx.log This log file is available on the EMR master node only and provides details about connections instantiated through the thrift interface of HBase. By default, Amazon EMR exposes the Thrift server on port 9090. hbase-hbase-regionserver-ip-xxx-xxx-xxx-xxx.log Available on CORE and TASK nodes only. It collects log entries generated by the region server running on the node. Should be reviewed in case of issues on the specific node. hbase.log This file logs the output of client commands issued by end users. For example, if you launch the command hbase hbck -details on the EMR master node, this file will trace the default logger of the hbck utility (which does not correspond to the stdout messages generated by the command). SecurityAuth.audit This file records user accesses to HBase tables, and it’s mainly used for audit purposes. In order to collect useful logs, it’s required to enable Kerberos authentication and enable the Access Controller capabilities of the HBase co-processor to provide more details in terms of tables accessed by the users. For an example configuration, see Security/Authorization  In order to modify the default log level of HBase, you can modify the hbase-log4j classification when launching an EMR cluster. For example, the below configuration will enable TRACE logging to have a more granular audit of user accesses to HBase tables (requires to enable Security/Authorization).  [ { &quot;Classification&quot;: &quot;hbase-log4j&quot;, &quot;Properties&quot;: { &quot;log4j.category.SecurityLogger&quot;:&quot;TRACE,RFAS&quot;, &quot;log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController&quot;: &quot;TRACE&quot;, &quot;log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.visibility.VisibilityController&quot;: &quot;TRACE&quot; } } ]   If you enabled the EMR logging on Amazon S3 these files will also be periodically collected on the S3 bucket that you choose when launching the cluster.  ","version":"Next","tagName":"h2"},{"title":"Monitoring HBase​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#monitoring-hbase","content":" ","version":"Next","tagName":"h2"},{"title":"Web UI​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#web-ui","content":" By default, HBase provides out of the box a Web UI that you can leverage to quickly check the status of your cluster and review detailed metrics of cluster utilization on a friendly interface. Amazon EMR exposes the HBase Web UIs at the following addresses:  HBase Master http://master-dns-name:16010/HBase Region http://region-dns-name:16030/  The HBase Master Web UI is typically the first entry point to check the status of your cluster. It provides details about the current number of tables hosted on the cluster, details about regions servers, cache utilization and much more. In HBase 2.x the Web UI was further improved, thus making it a valuable and quick tool to check the health and status of your HBase cluster. Below a sample image of the HBase master Web UI.    ","version":"Next","tagName":"h3"},{"title":"Ganglia​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#ganglia","content":" Another way to monitor HBase is with the help of Ganglia, which can be installed while provisioning your EMR cluster. Ganglia automatically collects HBase metrics and allows you to create reports and custom visualizations for the metrics of interests. The project has been discontinued in the last years, but it can still provide you with detailed information on your HBase utilization. This software should be installed if you need a quick and easy way for monitoring you HBase cluster. To install Ganglia in your cluster, please see Monitor HBase with Ganglia.  ","version":"Next","tagName":"h3"},{"title":"Grafana & Prometheus​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#grafana--prometheus","content":" Grafana in combination with Prometheus provides a better and more rich experience to monitor your HBase cluster that can be customized based on your needs.  In order to monitor your HBase cluster using a Grafana dashboard, you should install the prometheus node_exporter utility on each node to collect additional details from the nodes (CPU, Memory, Disk, Network metrics) and the Prometheus JMX exporter to prepare HBase JMX metrics for prometheus scraping.  Please refer to the following scripts for additional information on the setup and configurations of the components previously described:  emr-ba-prometheus_exporter.sh Node Exporter and Prometheus JMX exporter setupemr-step-monitoring_apps.sh Prometheus &amp; Grafana setup  Finally, in order to expose HBase JMX metrics from the HBase Master and Region Servers it is required to set the following variables in the /etc/hbase/conf/hbase-env.sh script:  export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml&quot; export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml&quot;   This can easily be achieved using the following EMR Configurations  [ { &quot;Classification&quot;: &quot;hbase-env&quot;, &quot;Configurations&quot;: [ { &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;HBASE_REGIONSERVER_OPTS&quot;: &quot;\\&quot;$HBASE_REGIONSERVER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\&quot;&quot;, &quot;HBASE_MASTER_OPTS&quot;: &quot;\\&quot;$HBASE_MASTER_OPTS -javaagent:/opt/prometheus/prometheus_javaagent.jar=7000:/opt/prometheus/hbase.yml\\&quot;&quot; } } ], &quot;Properties&quot;: {} } ]   A sample CloudFormation template with related dashboard can be found here.    ","version":"Next","tagName":"h3"},{"title":"JMX Metrics​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#jmx-metrics","content":" By default, HBase generates a significant amount of metrics that can be used to monitor the status and utilization of your cluster. To retrieve a list of all the metrics exposed by the cluster with related descriptions, you can directly invoke the HBase REST API using the following addresses:  HBase Master http://master-dns-name:16010/jmx?description=trueHBase Region http://region-dns-name:16030/jmx?description=true  For more details on JMX metrics, see HBase Metrics in the official HBase documentation.  ","version":"Next","tagName":"h3"},{"title":"Monitoring HDFS​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#monitoring-hdfs","content":" Amazon EMR pushes metrics to Amazon CloudWatch, a service that monitors your Amazon Web Services (AWS) resources, where you can define alarms for Amazon EMR metrics. For example, you can configure an alarm to receive a notification any time the HDFS utilization rises above 80%. This can help you to detect problems that might occur in your HBase cluster due to the full utilization of the HDFS space. Typically, it’s useful to define an alarm for the following metrics:  HDFSUtilization - The percentage of HDFS storage currently usedMissingBlocks - The number of blocks in which HDFS has no replicasUnderReplicatedBlocks - The number of blocks that need to be replicated one or more times.  For a list of all the metrics published by Amazon EMR, see Monitor metrics with CloudWatch in the EMR documentation. To configure an alarm for the previously mentioned metrics, see Create or edit a CloudWatch alarm in the Amazon CloudWatch User Guide.  ","version":"Next","tagName":"h2"},{"title":"Monitoring Amazon S3​","type":1,"pageTitle":"Observability","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Hbase/observability#monitoring-amazon-s3","content":" Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. Amazon S3 provides different tools that you can use to both monitor and audit the operations that are performed on the service.  When using Amazon S3 as storage layer for HBase, there are typically two main information that you might want to get out from the S3 service: size of the data stored in the bucket that can help you to estimate costs, and number of 5xx errors that might indicate throttling operations for S3 calls.  In this case, you can use S3 Storage Lens that can provide visibility into object-storage usage and activity. Once enabled, the service generates summary insights that you can review using a pre-built service dashboard. The dashboards can be generated for individual buckets or a list of buckets within the same region. When monitoring HBase, we recommend to enable S3 Storage Lens with the following configurations:  Dashboard Scope This mainly depends on how you want to monitor S3. You might generate individual dashboards for each bucket or a more comprehensive dashboard for a single region if you have multiple HBase clusters using S3 as storage layer.Metrics Selection Enable the ‘“Advanced Metrics and Recommendations” and select Activity Metrics and Detailed Status Code Metrics. Please note that the following metrics are available with additional costs, however they provide a more granular visibility about issues that you might experience on a specific bucket.  Amazon S3 also provides native integrations with Amazon CloudWatch and CloudTrail for a more granular control on the operations performed against S3 buckets. However, if you don’t have strong auditing requirements, the S3 Storage Lens capabilities are usually sufficient to monitor S3 when using HBase.  For additional details on S3 monitoring tools available on the service, see Monitoring Amazon S3 in the Amazon S3 documentation.  Finally, in some cases it might be useful to show in the HBase logs extended details for S3 calls performed by the HBase master and Region Servers. This information is useful to troubleshoot unusual behavior in the cluster or in case of issues. In this specific cases, you might want change the default logging for S3 calls using the following EMR configurations:  [ { &quot;Classification&quot;: &quot;hbase-log4j&quot;, &quot;Properties&quot;: { &quot;log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.request&quot;: &quot;DEBUG&quot;, &quot;log4j.logger.com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.latency&quot;: &quot;ERROR&quot; } } ]  ","version":"Next","tagName":"h2"},{"title":"introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Batch/introduction","content":"introduction coming soon...","keywords":"","version":"Next"},{"title":"introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Open Table Formats/introduction","content":"introduction coming soon...","keywords":"","version":"Next"},{"title":"** Ad Hoc - Introduction **","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction","content":"","keywords":"","version":"Next"},{"title":"** Choosing between Amazon Athena and Trino on Amazon EMR **​","type":1,"pageTitle":"** Ad Hoc - Introduction **","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/introduction#-choosing-between-amazon-athena-and-trino-on-amazon-emr-","content":" Amazon Athena is a serverless interactive query engine that executes SQL queries on data that rests in Amazon S3. Many customers use Athena for a wide variety of use cases, including interactive querying of data to exploring data, to powering dashboards on top of operational metrics saved on S3, to powering visualization tools, such as Amazon QuickSight or Tableau.  We recommend you consider Amazon Athena for these types of workloads. Athena is easy to integrate with, has several features, such as cost management and security controls, and requires little capacity planning. All of these characteristics lead to lower operational burden and costs.  However, there are some use cases where Trino on Amazon EMR may be better suited than Amazon Athena. For example, consider the following priorities:  Cost reduction: If cost reduction is your primary goal, we recommend that you estimate cost based on both approaches. You may find that the load and query patterns are lower in cost with Trino on Amazon EMR. Keep in mind that there is an operational cost associated with managing a Trino EMR environment. You’ll need to weight the cost benefits of Trino on EMR vs its operational overhead. Performance or Specific Tuning requirements: If your use case includes a high sensitivity to performance or you want the ability to fine-tune a Presto cluster to meet the performance requirements then Trino on EMR may be a better fit. Critical features: If there are features that Amazon Athena does not currently provide, such as the use of custom serializer/deserializers for custom data types, or connectors to data stores other than those currently supported, then Trino on EMR may be a better fit.  The rest of the section will focus on Trino on Amazon EMR. For more details on Amazon Athena, see here: &lt;https://aws.amazon.com/athena/&gt; ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/introduction","content":"Introduction This section offers best practices and tuning guidance for running Apache Spark workloads on Amazon EMR. The guidances cover the following main themes: Cost optimizationPerformance optimizationError mitigation","keywords":"","version":"Next"},{"title":"Ad Hoc - Architecture","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Adhoc/architecture","content":"Ad Hoc - Architecture The following diagram illustrates a common architecture to use PrestoSQL/Trino on Amazon EMR with the Glue Data Catalog as a big data query engine to query data in Amazon S3 using standard SQL.","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/Introduction","content":"Introduction EMR Cost Optimization best practices focus on the continual process of refinement and improvement of a system over its entire lifecycle. From the initial design of your very first proof of concept to the ongoing operation of production workloads, adopting the practices in this document can enable you to build and operate cost-aware systems that achieve business outcomes and minimize costs, thus allowing your business to maximize its return on investment. A cost-optimized workload is one that Meets functional and non functional requirementsFully utilizes all cluster resources andAchieves an outcome at the lowest possible price point To better understand this, let’s look at an example. Let’s assume we have an ETL job that needs to be completed within 8 hours. In order to meet the requirements of completing the job within 8 hours, a certain amount of compute resources will be required. This is represented in the graph by the “Job Demand”. Sometimes this is static, where the amount of resources needed is consistent throughout the duration of the job. And sometimes it’s more dynamic, where throughout the job, you have various peaks and valleys depending on the number of tasks that are running at each stage. In order for the job to finish within the 8 hours, it needs enough cluster capacity to meet the jobs compute demand - represented by the blue dotted line. If our cluster capacity is below our jobs compute demand Our job will be resource constrained and It’ll cause the job to run longer than our 8 hour sla Now, just being able to meet your SLA is not enough to be cost optimized. This leads us to our 2nd step of a cost optimized workload - Fully utilizing all cluster resources Take these next two graphs as an example, in the first case, we have a cluster that has compute capacity well beyond the jobs needs, represented by space in between the jobs demand and cluster capacity In this 2nd graph, we have a better match between the clusters capacity and jobs compute demands The space in between in between two is unused resources. These are resources that are being charged for but the job does not actually need. Fully utilizing all resources means reducing this space as much as possible. Going back to our job with less predictable workload patterns, a static cluster size may not be the best way to maximize cluster resources, but instead, using something like EMR autoscaling that adjusts cluster capacity based off of your workload demand would be a better fit. In this graph, our cluster scales up and down depending on demand. Our cluster capacity becomes a function of the jobs demand of resources. The last part of being “Cost Optimized” is achieving your jobs outcomes at the lowest price point possible. EMR has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your needs. For example, On-Demand, Spot and Commitment discounts - Savings Plans/ Reserved Instances/Capacity All of these pricing options leverage the exact same infrastructure but depending on which option you choose, the cost of your job will vary significantly. The numbers are just examples, with spot you can get up to 90% off on demand prices and with saving plans or RI, up to 72%. In the next sections, we’ll discuss best practices on choosing the right pricing model for your workload. For the purpose of this example, regardless of of which option you choose, the cluster compute capacity stays the same.","keywords":"","version":"Next"},{"title":"introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Architecture/Notebooks/introduction","content":"introduction coming soon...","keywords":"","version":"Next"},{"title":"2 - Reliability","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 2.1 Treat all clusters as transient resources​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-21-treat-all-clusters-as-transient-resources","content":" Whether you use your EMR cluster as a long or short running cluster, treat them as transient resources. This means you have the automation in place to re-provision clusters on demand and have standard templates to ensure cluster startup consistency. Even if you are using a long running clusters, it’s recommended to recreate the cluster during some periodical interval.  Services integrated with clusters also need to be decoupled from the cluster. For example any persistent data, meta data, scripts, and job/work orchestrator's (e.g oozie and airflow) should be stored off cluster. Decoupling the cluster from these services minimizes blast radius in the event of a cluster failure and non impacted clusters can continue using these off-cluster services.  There are several benefits to this approach. It makes upgrading, patching, rotating AMI’s or making any other infrastructure changes easier. It allows you to quickly recover from failures and it removes the operational overhead of managing a long running cluster. You may also see an improvement in cost since clusters will only run for the duration of your job or use case.  If you need to store state on cluster, ensure the state is backed up and synced.    For more information on orchestrating transient EMR cluster, see:  (https://aws.amazon.com/blogs/aws/new-using-step-functions-to-orchestrate-amazon-emr-workloads/)  (https://aws.amazon.com/blogs/big-data/orchestrating-analytics-jobs-on-amazon-emr-notebooks-using-amazon-mwaa/)  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html)  Specifically for EMR application logging, consider using EMR’s Persistent Application User Interfaces (Spark, YARN RM, Tez UI, etc) which are hosted by EMR off cluster and available even after clusters are terminated.  For more information on off cluster monitoring options, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/app-history-spark-UI.html)  (https://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/)  For more information on external catalog, see:  (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html)  ","version":"Next","tagName":"h2"},{"title":"BP 2.2 Decouple storage and compute​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-22-decouple-storage-and-compute","content":" Store persistent data in Amazon S3 and use the EMR File System (EMRFS) for reading and writing data from Amazon EMR. EMRFS is an implementation of HDFS that all Amazon EMR clusters use for accessing data in Amazon S3. Applications such as Apache Hive and Apache Spark work with Amazon S3 by mapping the HDFS APIs to Amazon S3 APIs (like EMRFS available with Amazon EMR). You specify which file system to use by the prefix of the URI used to access the data. For example, s3://DOC-EXAMPLE-BUCKET1/path references an Amazon S3 bucket using EMRFS.  By keeping persistent data in Amazon S3, you minimize the impact that infrastructure or service disruptions can have on your data. For example, in the event of an EC2 hardware failure during an application run, data in Amazon S3 will not be impacted. You can provision a new cluster and re run your application that points to the existing S3 bucket.  From an application and user perspective, by decoupling storage and compute, you can point many EMR clusters at the same source of truth. If you have different departments that want to operate different jobs, they can act in isolation without affecting the core production of your environment. This also allows you to split interactive query workloads with ETL type workloads which gives you more flexibility in how you operate For example, In an Amazon EMR environment you can provision a new cluster with a new technology and operate it in parallel on your data with your core production environment. Once you make a decision on which technology to adopt, you can easily cut over from one to other. This allows future proofing and option value because you can keep pace the analytic tool set evolves, your infrastructure can evolve with it, without any expensive re platforming or re transformation of data.  HDFS is still available on Amazon EMR clusters and is a good option for temporary or intermediate data. For example, workloads with iterative reads on the same data set or Disk I/O intensive workloads. For example, some hive jobs write a lot of data to HDFS, either staging data or through a multi step pipeline. It may be more cost efficient and performant to use HDFS for these stages compared to writing to Amazon S3. You lose the HDFS data once EMR clusters are terminated so this should only be used for intermediate or staging data. Another strategy is to ensure that when using HDFS, you checkpoint data at regular intervals so that if you lose cluster mid-work, you do not have to restart from scratch. Once data is written to HDFS, you can use something like s3distcp to move your data to Amazon S3.    ","version":"Next","tagName":"h2"},{"title":"BP 2.3 Use the latest AMI and EMR version available​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-23-use-the-latest-ami-and-emr-version-available","content":" In the Cost Optimization section, we talked about the benefits of using the latest EMR version. Equally important is using the latest AMI available. This ensures your up to date with the latest bug fixes, features and security updates. EMR allows has 2 AMI options available - default EMR AMI and Custom AMI.  The default EMR AMI is based on the most up-to-date Amazon Linux AMI available at the time of the Amazon EMR release. Each Amazon EMR release version is &quot;locked&quot; to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate.  When using a custom AMI, it is recommended to base your customization on the most recent EBS-backed Amazon Linux AMI (AL2 for 5.30.0 and later). Consider creating a new custom EMR AMI each time a new AL AMI is released.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-default-ami.html)  ","version":"Next","tagName":"h2"},{"title":"BP 2.4 Spread clusters across Availability Zones/subnets and time of provisioning​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-24-spread-clusters-across-availability-zonessubnets-and-time-of-provisioning","content":" Spread clusters across multiple Availability Zones (AZ) to provide resiliency against AZ failures. An added benefit is that it can help reduce insufficient capacity errors (ICE) since your EC2 requests are now across multiple EC2 pools. Instances of a single cluster can only be provisioned in a single AZ.  EMR helps you achieve this with Instance Fleets. Instead of specifying a single Amazon EC2 Availability Zone for your Amazon EMR cluster and a specific Amazon EC2 instance type for an Amazon EMR instance group, you can provide a list of Availability Zones and instances, and Amazon EMR will automatically select an optimal combination based on cost and availability. For example, if Amazon EMR detects a large-scale event in one or more of the Availability Zones, or cannot get enough capacity, Amazon EMR automatically attempts to route traffic away from the impacted Availability Zones and tries to launch clusters in alternate Availability Zones according to your selections.  With Instance Groups, you must explicitly set the subnet at provisioning time. You can still spread clusters across your AZs by selecting AZ's through a round robin or random strategy.    If your use case allows, spread cluster provisioning times across the hour or day to distribute your requests to EC2 instead of provisioning clusters at the same time. This decreases the likelihood of getting insufficient capacity errors.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html)  ","version":"Next","tagName":"h2"},{"title":"BP 2.5 Use on demand for core nodes and spot for task​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-25-use-on-demand-for-core-nodes-and-spot-for-task","content":" Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). If a core node is running on Spot Instances and the Spot node is reclaimed, Hadoop has to re balance the data in HDFS to the remaining core nodes. If there are no core nodes remaining, you run the risk of losing HDFS data and the name node going into safe mode making the cluster unhealthy and unusable.    ","version":"Next","tagName":"h2"},{"title":"BP 2.6 Use Instance Fleet with an allocation strategy​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-26-use-instance-fleet-with-an-allocation-strategy","content":" The Instance Fleets configuration for Amazon EMR clusters lets you select a wide variety of provisioning options for Amazon EC2 instances, and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster.  You can have one Instance Fleet for each node group - master, core and task. Within the Instance Fleet, you specify a target capacity for on-demand and spot instances and with the allocation strategy option, you can select up to 30 instance types per fleet.    In an Instance Fleet configuration, you specify a target capacity for On-Demand Instances and Spot Instances within each Fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled using any of the instances specified if your fleet. When Amazon EC2 reclaims a Spot Instance in a running cluster because of a price increase or instance failure, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing.  It is recommended that you use the allocation strategy option for faster cluster provisioning, more accurate Spot Instance allocation, and fewer Spot Instance interruptions. With the allocation strategy enabled, On-Demand Instances use a lowest-price strategy, which launches the lowest-priced instances first. Spot Instances use a capacity-optimized strategy, which launches Spot Instances from pools that have optimal capacity for the number of instances that are launching. For both On-demand and spot, we recommend specifying a larger number of instance types to diversify and reduce the chance of experiencing insufficient capacity errors.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy)  ","version":"Next","tagName":"h2"},{"title":"BP 2.7 With Instance Fleet, diversify with instances in the same family and across generations first​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-27-with-instance-fleet-diversify-with-instances-in-the-same-family-and-across-generations-first","content":" Best practices for instance and availablity zone flexibility can be found, here:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-flexibility.html)  ","version":"Next","tagName":"h2"},{"title":"BP 2.8 With Instance Fleet, ensure the unit/weight matches the instance size or is proportional to the rest of the instances in your fleet​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-28-with-instance-fleet-ensure-the-unitweight-matches-the-instance-size-or-is-proportional-to-the-rest-of-the-instances-in-your-fleet","content":" When using Instance Fleets, you can specify multiple instance types and a total target capacity for your core or task fleet. When you specify an instance, you decide how much each instance counts toward the target. Ensure this unit/weight matches the actual instance size or is proportional to the rest of the instances in your fleet.  For example, if your fleet includes: m5.2xlarge, m5.4xlarge and m5.8xlarge. You would want your units/weights to match the instance size - 2:4:8. This is to ensure that when EMR provision your cluster or scales up, you are consistently getting the same total compute. You could also do 1:2:4 since they are still proportional to the instance sizes. If the weights were not proportional, e.g 1:2:3, each time your cluster provisions, your total cluster capacity can be different.  ","version":"Next","tagName":"h2"},{"title":"BP 2.9 If optimizing for availability, avoid exotic instance types​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-29-if-optimizing-for-availability-avoid-exotic-instance-types","content":" Exotic instances are designed for specific use cases, and includes instance types whose name ends with “zn”, “dn“, and “ad&quot;, as well as large instance types like 24xlarge. Exotic instance type capacity pools are generally smaller, which increases the likelihood of Insufficient Capacity Errors and Spot reclamation. It is recommended to avoid these types of instances if your use case does not have requirements for these types of instances and you want higher instance availability.  ","version":"Next","tagName":"h2"},{"title":"BP 2.10 Handling S3 503 slow downs​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-210-handling-s3-503-slow-downs","content":" When you have an increased request rate to your S3 bucket, S3 might return 503 Slow Down errors while scaling to support the request rate. The default request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are a number of ways to handle S3 503 responses:  Use EMRFS retry strategies  EMRFS provides 2 ways to improve the success rate of your S3 requests. You can adjust your retry strategy by configuring properties in your emrfs-site configuration.  Increase the maximum retry limit for the default exponential back-off retry strategy. By default, the EMRFS retry limit is set to 4. You can increase the retry limit on a new cluster, on a running cluster, or at application runtime. (for example try 20-50 by setting fs.s3.maxRetries in emrfs-site.xml) Enable and configure the additive-increase/multiplicative-decrease (AIMD) retry strategy. AIMD is supported for Amazon EMR versions 6.4.0 and later.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html)  Increase fs.s3n.multipart.uploads.split.size  Specifies the maximum size of a part, in bytes, before EMRFS starts a new part upload when multipart uploads is enabled. Default is 134217728 (134mb). The max is 5368709120 (5GB) – you can start with something in the middle and see if there’s any impact to performance (for example 1-2 gb)  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart)  Combine or stagger out requests to S3  Combining requests to S3 reduces the number of calls per second. This can be achieved in a few ways:  If the error happens during write, reduce the parallelism of the jobs. For example, use Spark .coalesce() or .repartition() operations to reduce number of Spark output partitions before writing to Amazon S3. You can also reduce the number of cores per executor or reduce the number of executors.If the error happens during read, compact small files in the source prefix. Compacting small files reduces the number of input files which reduces the number of Amazon S3 requests.If possible, stagger jobs out across the day or hour. For example, If your jobs don’t all need to start at the same time or top of the hour, spread them across the hour or day to smoothen out the requests to S3.  For more information, see:  (https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/)  Optimize your S3 Data layout  Rate limits (3,500 write and 5,500 read) are applied at the prefix level. By understanding your job access patterns, you can reduce throttling errors by partitioning your data in S3  For example, comparing the two s3 structures below, the second example with product in the prefix will allow you to achieve higher s3 request rates since requests are spread across different prefix. The S3 bucket limit would be 7,000 write requests and 11,000 read requests.  s3://&lt;bucket1&gt;/dt=2021-11-01 s3://&lt;bucket2&gt;/product=1/dt=2021-11-01 s3://&lt;bucket2&gt;/product=2/dt=2021-11-01   It is also important that your S3 data layout is structured in a way that allows for partition pruning. With partition pruning, your applications will only scan the objects it needs and skip over the other prefixes reducing the number of requests to S3.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html#emr-spark-performance-dynamic)  ","version":"Next","tagName":"h2"},{"title":"BP 2.11 Audit and update EMR and EC2 limits to avoid throttling​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-211-audit-and-update-emr-and-ec2-limits-to-avoid-throttling","content":" Amazon EMR throttles API calls to maintain system stability. EMR has two types of limits:  Limit on Resources - maximum number of clusters that can  The maximum number of active clusters that can be run at the same time.The maximum number of active instances per instance group.  Limits on APIs  Burst limit – This is the maximum number of API calls you can make at once. For example, the maximum number of AddInstanceFleet API requests that you can make per second is set at 5 calls/second as a default. This implies that the burst limit of AddInstanceFleet API is 5 calls/second, or that, at any given time, you can make at most 5 AddInstanceFleet API calls. However, after you use the burst limit, your subsequent calls are limited by the rate limit.Rate limit – This is the replenishment rate of the API's burst capacity. For example, replenishment rate of AddInstanceFleet calls is set at 0.5 calls/second as a default. This means that after you reach the burst limit, you have to wait at least 2 seconds (0.5 calls/second X 2 seconds = 1 call) to make the API call. If you make a call before that, you are throttled by the EMR web service. At any point, you can only make as many calls as the burst capacity without being throttled. Every additional second you wait, your burst capacity increases by 0.5 calls until it reaches the maximum limit of 5, which is the burst limit.  To prevent throttling errors, we recommend:  Reduce the frequency of the API calls. For example, if you’re using the DescribeStep API and you don’t need to know the status of the job right away, you can reduce the frequency of the call to 1min+Stagger the intervals of the API calls so that they don't all run at the same time.Implement exponential back-off (https://docs.aws.amazon.com/general/latest/gr/api-retries.html) when making API calls.  For more information, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html)  ","version":"Next","tagName":"h2"},{"title":"BP 2.12 Set dfs.replication > 1 if using Spot for core nodes or for long running clusters​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-212-set-dfsreplication--1-if-using-spot-for-core-nodes-or-for-long-running-clusters","content":" dfs.replication is the number of copies of each block to store for durability in HDFS. If dfs.replication is set to 1, and a Core node is lost due to spot reclamation or hardware failure, you risk losing HDFS data. Depending on the hdfs block that was lost, operating software may be affected and you may not be able to perform certain EMR actions - for example to submit a Hive job if the Tez library in HDFS is missing.  dfs.replication defaults are set based off of initial core count:  (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html)  To ensure the core node instance group is highly available, it is recommended that you launch at least two core nodes and set dfs.replication parameter to 2.  Few other considerations:  Do not scale your node count below dfs.replication. For example if dfs.replication=3, keep your core node minimum to 3Increasing dfs.replication will require additional EBS storage  ","version":"Next","tagName":"h2"},{"title":"BP 2.13 Right size your EBS volumes to avoid UNHEALTHY nodes​","type":1,"pageTitle":"2 - Reliability","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/best_practices#bp-213-right-size-your-ebs-volumes-to-avoid-unhealthy-nodes","content":" When disk usage on a core or task node disk (for example, /mnt or /mnt1) exceeds 90%, the disk will be marked as unhealthy. If fewer than 25% of a node's disks are healthy, the NodeManager marks the whole node as unhealthy and communicates this to the ResourceManager, which then stops assigning containers to the node.  If the node remains UNHEALTHY for more than 45 minutes, YARN ResourceManager gracefully decommissions the node when termination protection is off. If termination protection is on, the core nodes remain in an UNHEALTHY state and only task nodes are terminated.  The two most common reasons disk’s exceed 90% are writing of HDFS and spark shuffle data. To avoid this scenario, it is recommended to right size your EBS volumes for your use case. You can either add more EBS volumes or increase the total size of the EBS capacity so that it never exceeds the default 90% utilization disk checker rate.  From a monitoring and alerting perspective, there are a few options. You can monitor and alert on HDFS utilization using the Cloudwatch metric HDFSUtilization. This can help determine if disks are exceeding the 90% threshold due to HDFS usage. At a per node and disk level, using options in BP 1.12 can help identify if disk is filling due to spark shuffle or some other process. At a cluster level, you can also create an alarm for the MRUnhealthyNodes CloudWatch metric which reports the number of nodes reporting an UNHEALTHY status. Since UNHEALTHY nodes are excluded from processing tasks from YARN Resourcemanager, having UNHEALTHY nodes can degrade job performance.  The 90% is a default value which can be configured by yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage in yarn-site.xml. However, to fix nodes going UNHEALTHY, it is not recommended to adjust this %, but instead right size your EBS volumes.  For more information, see:  (https://aws.amazon.com/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/)  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html)  Calculating required HDFS utilization: (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs) ","version":"Next","tagName":"h2"},{"title":"4.2 - Spot Usage","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 4.2.1 When to use spot vs. on demand​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-421-when-to-use-spot-vs-on-demand","content":" Spot Instances provide a great way to help reduce costs. However, there are certain scenarios where you should consider on demand because there's always a chance that a Spot interruption can happen. The considerations are:  Use Spot for workloads where they can be interrupted and resumed (interruption rates are extremely low), or workloads that can exceed an SLAUse Spot for testing and development workloads or when testing testing new applications.Avoid Spot if your workload requires predictable completion time or has service level agreement (SLA) requirementsAvoid Spot if your workload has 0 fault tolerance or when recomputing tasks are expensiveUse Instance Fleet with allocation strategy while using Spot so that you can diversify across many different instances. The Spot capacity pool can be unpredictable, so diversifying with as many instances that meets your requirements can help increase the likelihood of securing Spot instances which in turn, reduces cost.  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.1 Use Instancefleets when using Spot Instances​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-421-use-instancefleets-when-using-spot-instances","content":" Instance Fleets provides clusters with flexibility - instead of relying on a single instance type to reach your target capacity, you can specify up to 30 different types and families. This is a best practice when using Spot because EMR will automatically provision instances from the most-available Spot capacity pools when allocation strategy is enabled. Because your Spot Instance capacity is sourced from pools with optimal capacity, this decreases the possibility that your Spot Instances will be reclaimed. A good rule of thumb is to be flexible across at least 10 instance types for each workload. In addition, make sure that all Availability Zones are configured for use in your VPC and selected for your workload. An EMR cluster will only be provisioned in a single AZ but will look across all for the initial provisioning.  When using Instance Fleets, it is recommended you diversify across instances and family. For more details, see:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-flexibility.html)  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.2 Ensure Application Masters only run on an On Demand Node​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-422-ensure-application-masters-only-run-on-an-on-demand-node","content":" When a job is submitted to EMR Prior to the 5.x release, the Application Master (AM) can run on any of the nodes*. The AM is is the main container requesting, launching and monitoring application specific resources. Each job launches a single AM and if the AM is assigned to a Spot node, and that Spot node is interrupted, your job will fail.  Therefore, it's important to ensure the AM is as resilient as possible. Assuming you are running a mixed cluster of On Demand and Spot, by placing AM's on On Demand nodes, you'll ensure AM's do not fail due to a Spot interruption.  The following uses yarn.nodemanager.node-labels.provider.script.path to run a script that sets node label to the market type - On Demand or Spot. yarn-site is also updated so that application masters are only assigned to the &quot;on_demand&quot; label. Finally, the cluster is updated to include the new node label.  This is a good option when you run a mix of On Demand and Spot. You can enable this with the following steps:  1. Save getNodeLabels_bootstrap.sh and getNodeLabels.py in S3 and run getNodeLabels_bootstrap.sh as an EMR bootstrap action  getNodeLabels_bootstrap.sh  #!/bin/bash aws s3 cp s3://&lt;bucket&gt;/getNodeLabels.py /home/hadoop chmod +x /home/hadoop/getNodeLabels.py   This script will copy getNodeLabels.py onto each node which is used by YARN to set NODE_PARTITION  getNodeLabels.py  #!/usr/bin/python3 import json k='/mnt/var/lib/info/extraInstanceData.json' with open(k) as f: response = json.load(f) #print ((response['instanceRole'],response['marketType'])) if (response['instanceRole'] in ['core','task'] and response['marketType']=='on_demand'): print (f&quot;NODE_PARTITION:{response['marketType'].upper()}&quot;)   This script is run every time a node is provisioned and sets NODE_PARTITION to on_demand.  2. Set yarn-site classification to schedule AMs on ON_DEMAND nodes.  [ { &quot;classification&quot;:&quot;yarn-site&quot;, &quot;Properties&quot;:{ &quot;yarn.nodemanager.node-labels.provider&quot;:&quot;script&quot;, &quot;yarn.nodemanager.node-labels.provider.script.path&quot;:&quot;/home/hadoop/getNodeLabels.py&quot;, &quot;yarn.node-labels.enabled&quot;:&quot;true&quot;, &quot;yarn.node-labels.am.default-node-label-expression&quot;:&quot;ON_DEMAND&quot;, &quot;yarn.nodemanager.node-labels.provider.configured-node-partition&quot;:&quot;ON_DEMAND,SPOT&quot; } }, { &quot;classification&quot;:&quot;capacity-scheduler&quot;, &quot;Properties&quot;:{ &quot;yarn.scheduler.capacity.root.accessible-node-labels.ON_DEMAND.capacity&quot;:&quot;100&quot;, &quot;yarn.scheduler.capacity.root.default.accessible-node-labels.ON_DEMAND.capacity&quot;:&quot;100&quot; } } ]   3. Add EMR Step  #!/bin/bash sudo -u yarn yarn rmadmin -addToClusterNodeLabels &quot;SPOT(exclusive=false),ON_DEMAND(exclusive=false)&quot;   This Step should be the first step on the EMR cluster, and adds the new node labels.  Once your cluster is provisioned, AM's will only run on On Demand nodes. Other non AM containers will run on all nodes.  * EMR 5.19 and later uses the node label feature to assign AMs on core nodes only. Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The application master processes can run on both core and task nodes by default.  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.3 Allow application masters (AM) to run on all nodes​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-423-allow-application-masters-am-to-run-on-all-nodes","content":" With EMR 5.x, AM only run on core nodes. Because Spot Instances are often used to run task nodes, it prevents applications from failing in case an AM is assigned to a Spot node.  As a result of this, in scenarios where applications are occupying the full core node capacity, AM's will be in a PENDING state since they can only run on core nodes. The application will have to wait for capacity to be available on the core nodes even if there's capacity on the task nodes.  Allowing AM's to run on all nodes is a good option if you are not using Spot, or run a small number of core nodes and do not want your cluster to be limited by Core capacity. You can disable this behavior with the bootstrap action below:  #!/bin/bash echo &quot;backup original init.pp&quot; sudo cp cp /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp /tmp/ echo &quot;replacing node label check&quot; sudo sed -i '/add-to-cluster-node-labels.*/,+5d' /var/aws/emr/bigtop-deploy/puppet/modules/hadoop/manifests/init.pp   Beginning with Amazon EMR 6.x release series, the YARN node labels feature is disabled by default. The AM processes can run on both core and task nodes by default. You can enable the YARN node labels feature by configuring following properties:  yarn.node-labels.enabled: true yarn.node-labels.am.default-node-label-expression: 'CORE'   When you allow AM's to run on all nodes and are using managed scaling, consider increasing yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs so AM's are not automatically terminated after the 1hr timeout in the event of a scale down. See BP 4.1.3 for more details.  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.4 Reserve core nodes for only application masters (am)​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-424-reserve-core-nodes-for-only-application-masters-am","content":" This is not necessarily related to Spot, but An alternative to BP 4.2.2 is to reserve core nodes for only application masters/spark drivers. This means tasks spawned from executors or AMs will only run on the task nodes. The approach keeps the “CORE” label for core nodes and specifies it as exclusive=true. This means that containers will only be allocated to CORE nodes when it matches the node partition during job submission. By default, EMR will set AM=Core and as long as users are not specifying node label = core, all containers will run on task.  Add EMR step during EMR provisioning:  #!/bin/bash #Change core label from exclusive=false to exclusive=true. sudo -u yarn yarn rmadmin -removeFromClusterNodeLabels &quot;CORE&quot; sudo -u yarn yarn rmadmin -addToClusterNodeLabels &quot;CORE(exclusive=true)&quot;   Applications can still be waiting for resources if the # of jobs you’re submitting exceeds the available space on your core nodes. However, this is less likely to occur now that tasks cant be assigned to core. Another other option to consider is allowing AM to run on all nodes but OD, but we do not recommend having AM run on the task group generally.  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.5 Reduce Spot interruptions by setting purchase Option to \"Use on-demand as max price\"​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-425-reduce-spot-interruptions-by-setting-purchase-option-to-use-on-demand-as-max-price","content":" By setting the spot purchase option to &quot;use on-demand as max price&quot;, your Spot nodes will only be interrupted when EC2 takes back Spot capacity and not because of someone outbidding your Spot price.  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.6 Reduce the impact of Spot interruptions​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-426-reduce-the-impact-of-spot-interruptions","content":" There are a few strategies to consider when using Spot Instances that will help you take advantage of Spot pricing while still getting capacity:  Mix On Demand nodes with SpotUse On Demand for core nodes and Spot for taskReduce provisioning timeout and switch to On Demand - When using Instance Fleets, EMR allows you to set a timeout duration for getting Spot capacity. Once the duration is hit, you can choose to terminate the cluster or fall back to on demand. The default value is 60min but consider lowering this quickly fall back to on demand when spot is not availableCheckpoint often - This allows you to retry from a certain part of your pipeline if you ever lose too many Spot nodes  ","version":"Next","tagName":"h2"},{"title":"BP 4.2.7 Adjust Spark task size to complete within 2 minutes​","type":1,"pageTitle":"4.2 - Spot Usage","url":"/aws-emr-best-practices/docs/bestpractices/Features/Spot Usage/best_practices#bp-427-adjust-spark-task-size-to-complete-within-2-minutes","content":" Consider reducing spark task sizes to minimize the impact of a Spot interruption. Smaller task sizes complete faster. These tasks will be less impacted by spot because the amount of time to recompute the failed task is less. There are a number of factors that impact the # of spark tasks and their run time such as spark.sql.files.maxPartitionBytes, spark.default.parallelism, # of objects in S3, or are the objects can be split. See Spark best practices section on how to adjust task run times.  When using Spot, the optimal task run time is less than 2 minutes. This is because EC2 Spot instances receive a two-minute warning when these instances are about to be reclaimed by Amazon EC2 which means most tasks will be able to finish before the spot node is reclaimed. In addition, EMR does not assign new tasks to nodes that have received the 2 minute warning. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Security/introduction","content":"Introduction EMR Security best practices discusses how to take advantage of AWS and EMR features to protect data, systems, and assets in a way that can improve your security posture. It's recommended to first read our Well Architected paper on security to understand the risks we mitigate and how we think about security. (https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html)","keywords":"","version":"Next"},{"title":"Amazon EMR Utilities","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/utilities/","content":"Amazon EMR Utilities https://github.com/aws-samples/aws-emr-utilities This repository contains sample code and utilities for using Amazon EMR on EC2. This package is structured based on the following directories: applications - application specific patches, plugins, etc.utilities - administrative and maintenance utilities for working with EMR","keywords":"","version":"Next"},{"title":"4.1 - Managed Scaling","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 4.1.1 Keep core nodes constant and scale with only task nodes​","type":1,"pageTitle":"4.1 - Managed Scaling","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices#bp-411-keep-core-nodes-constant-and-scale-with-only-task-nodes","content":" Scaling with only task nodes improves the time for nodes to scale in and out because task nodes do not coordinate storage as part of HDFS. During scale up, task nodes do not need to install data node daemons and during scale down, task nodes do not need rebalance HDFS blocks. Improvement in the time it takes to scale in and out improves performance and reduces cost. When scaling down with core nodes, you also risk saturating the remaining nodes' disk volume during HDFS rebalance. If the nodes disk utilization exceeds 90%, it’ll mark the node as unhealthy, making it unusable by YARN.  In order to only scale with task nodes, you keep the number of core nodes constant and right size your core node EBS volumes for your HDFS usage. Remember to consider the HDFS replication factor which is configured via dfs.replication in hdfs-site.xml. It is recommended that at a minimum, you keep 2 core nodes and set dfs.replication=2.  Below is a managed scaling configuration example where the cluster will scale only on task nodes. In this example, the minimum nodes is 25, maximum 100. Of the 25 minimum, they will be all on-demand and core nodes. When the cluster needs to scale up, the remaining 75 will be task nodes on spot.    ","version":"Next","tagName":"h2"},{"title":"BP 4.1.2 Monitor Managed Scaling with Cloudwatch Metrics​","type":1,"pageTitle":"4.1 - Managed Scaling","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices#bp-412-monitor-managed-scaling-with-cloudwatch-metrics","content":" You can monitor your managed scaling cluster with CloudWatch metrics. This is useful if you want to better understand how your cluster is resizing to the change in job load/usage.  Lets looks at an example:    At 18:25, YARNMemoryAvailablePercentage starts at 100%. This means that no jobs are running. At 18:27 a job starts and we see YARNMemoryAvailablePercentage begin to drop, reaching 0% at 18:29. This triggers managed scaling to start a resize request - represented by the increase in the metric TotalNodesRequested. After 5-6 mins, at 18:35 the nodes finish provisioning and are considered RUNNING. We see an increase in the metric, TotalNodesRunning. Around the same time, we see YARNMemoryAvailablePercentage begin increasing back to 100%.  For a full list of metrics and description of each, seehttps://docs.aws.amazon.com/emr/latest/ManagementGuide/managed-scaling-metrics.html  ","version":"Next","tagName":"h2"},{"title":"BP 4.1.3 Consider adjusting YARN decommissioning timeouts depending on your workload​","type":1,"pageTitle":"4.1 - Managed Scaling","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices#bp-413-consider-adjusting-yarn-decommissioning-timeouts-depending-on-your-workload","content":" There are two decommissioning timeouts that are important in managed scaling:  yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED.spark.blacklist.decommissioning.timeout: This is the maximal time that Spark does not schedule new tasks on executors running on that node. Tasks already running are allowed to complete.  When managed scaling triggers a scale down, YARN will put nodes it wants to decomission in a DECOMMISSIONING state. Spark will detect this and add these nodes to a “black list” (AWS acknowledges the use of non-inclusive language in this codebase and will work with the Spark community to update). In this state, Spark will not assign any new tasks to the node and once all tasks are completed, YARN will finish decommissioning the node. If the task runs longer than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, the node is force-terminated and the task will be reassigned to another node.  In certain scale down scenarios where you have long running tasks, many nodes can end up in this state where they are DECOMMISSIONING and blacklisted because of spark.blacklist.decommissioning.timeout. You may observe that new jobs run slower because it cannot assign tasks to all nodes in the cluster.  To mitigate this, you can lower spark.blacklist.decommissioning.timeout to make the node available for other pending containers to continue task processing. This can improve job run times. However, please take the below into consideration:  If a task is assigned to this node, and YARN transitions from DECOMMISSIONING into DECOMMISSIONED, the task will fail and will need to be reassigned to another node.Spark blacklist also protects from bad nodes in the cluster, e.g., faulty hardware leading to high task failure rate. Lowering the blacklist timeout can increase task failure rate since tasks will continue to be assigned to these nodes.  Nodes can be transitioned from DECOMMISSIONING to RUNNING due to a scale up request. In this scenario, tasks will not fail and with a lower blacklist timeout, pending tasks can continuously be assigned to the node.  With yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs, consider increasing this from the default of 1hr to the length of your longest running task. This is to ensure that YARN does not force-terminate the node while the task is running, causing it to re-run on another node. The cost associated with rerunning the long running task is generally higher than keeping the node running to ensure it's completed.  For more information, see:  (https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/)  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-3.html)  (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#spark-decommissioning)  ","version":"Next","tagName":"h2"},{"title":"BP 4.1.5 EMR Managed Scaling compared to Custom Automatic Scaling​","type":1,"pageTitle":"4.1 - Managed Scaling","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices#bp-415-emr-managed-scaling-compared-to-custom-automatic-scaling","content":" The following link highlights the key differences between EMR managed scaling vs. custom automatic scaling:  (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-scale-on-demand.html)  In general, we recommend using EMR managed scaling since the metric evaluation is every 5-10 seconds. This means your EMR cluster will adjust quicker to the change in the required cluster resources. In addition, EMR managed scaling also supports instance fleets and the the scaling policy is simpler to configure because EMR managed scaling only requires min and max amounts for purchasing options (On-Demand/Spot) and node type (core/task).  Custom automatic scaling should be considered if you want autoscaling outside of YARN applications or if you want full control over your scaling policies (e.g., evaluation period, cool down, number of nodes)  ","version":"Next","tagName":"h2"},{"title":"BP 4.1.6 Configure Spark History Server (SHS) custom executor log URL to point to Job History Server (JHS) Directly​","type":1,"pageTitle":"4.1 - Managed Scaling","url":"/aws-emr-best-practices/docs/bestpractices/Features/Managed Scaling/best_practices#bp-416-configure-spark-history-server-shs-custom-executor-log-url-to-point-to-job-history-server-jhs-directly","content":" When you use SHS to access application container logs, YARN ResourceManager relies on the NodeManager that the jobs' Application Master (AM) ran on, to redirect to the JHS. The JHS is what hosts the container logs. A job's executor logs cannot be accessed if the AM ran on a node that’s been decommissioned due to managed scaling or spot.  A solution to this is pointing SHS to the JHS directly, instead of letting node manager redirect.  Spark 3.0 introduced spark.history.custom.executor.log.url, which allows you to specify a custom Spark executor log url.  You can configure spark.history.custom.executor.log.url as below to point to JHS directly:  {{HTTP_SCHEME}}&lt;JHS_HOST&gt;:&lt;JHS_PORT&gt;/jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096   Replace JHS_HOST and JHS_PORT with actual values. JHS_HOST is the EMR master node. ","version":"Next","tagName":"h2"},{"title":"3 - Security","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 3.1 Encrypt Data at rest and in transit​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-31-encrypt-data-at-rest-and-in-transit","content":" Properly protecting your data at rest and in transit using encryption is a core component of AWS' Well-Architected pillar of security. Amazon EMR security configurations make it easy for you to encrypt data both at rest and in transit. A security configuration is like a template for encryption and other security configurations that you can apply to any cluster when you launch it.  For data at rest, EMR provides encryption options for reading and writing data in S3 via EMRFS. You specify Amazon S3 server-side encryption (SSE) or client-side encryption (CSE) as the Default encryption mode when you enable encryption at rest. Optionally, you can specify different encryption methods for individual buckets using Per bucket encryption overrides. EMR also provides the option to encrypt local disk storage. These are EC2 instance store volumes and the attached Amazon Elastic Block Store (EBS) storage that are provisioned with your cluster. You have the options of using Linux Unified Key Setup (LUKS) encryption or using AWS KMS as your key provider.  For data in transit, EMR security configurations allow you to either manually create PEM certificates, zip them in a file, and reference from Amazon S3 or implement a certificate custom provider in Java and specify the S3 path to the JAR. In either case, EMR automatically downloads artifacts to each node in the cluster and later uses them to implement the open-source, in-transit encryption features. For more information on how these certificates are used with different big data technologies, see: (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit)  For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post Secure Amazon EMR with Encryption, see: (https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption)  ","version":"Next","tagName":"h2"},{"title":"BP 3.2 Restrict network access to your EMR cluster and keep EMR block public access feature enabled​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-32-restrict-network-access-to-your-emr-cluster-and-keep-emr-block-public-access-feature-enabled","content":" Inbound and outbound network access to your EMR cluster is controlled by EC2 Security Groups. It is recommended to apply the principle of least privilege to your Security Groups, so that your cluster is locked down to only the applications or individuals who need access from the expected source IPs.  It’s also recommended to not allow SSH access to the hadoop user, which has elevated sudo access and access to this user is typically not required. EMR provides a number of ways for users to interact with clusters remotely. For job submission, users can use EMR Steps API or an orchestration service like AWS Managed Apache Airflow or AWS Step functions. For ad-hoc or notebook use cases, you can use EMR Studio, or allow users to connect to the specific application ports e.g Hiveserver2 JDBC, Livy or Notebook UI’s.  The block public access feature prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception - port 22 is an exception by default. This feature is enabled by default for each AWS Region in your AWS account and is not recommended to be turned off.  Use Persistent Application UI's to remove the need to open firewall to get access to debugging UI  ","version":"Next","tagName":"h2"},{"title":"BP 3.3 Provision clusters in a private subnet​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-33-provision-clusters-in-a-private-subnet","content":" It is recommended to provision your EMR clusters in Private VPC Subnets. Private Subnets allow you to limit access to deployed components, and to control security and routing of the system. With a private Subnet, you can enable communication with your own network over a VPN tunnel or AWS Direct Connect, which allows you to access your EMR clusters from your network without requiring internet routing. For access to other AWS services from your EMR Cluster e.g S3, VPC endpoints can be used.  For more information on configuring EMR clusters in private subnets or VPC endpoints, see: (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html) (https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html)  ","version":"Next","tagName":"h2"},{"title":"BP 3.4 Configure EC2 instance metadata service (IMDS) v2​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-34-configure-ec2-instance-metadata-service-imds-v2","content":" In AWS EC2, the Instance Metadata Service (IMDS) provides “data about your instance that you can use to configure or manage the running instance. Every instance has access to its own IMDS using any HTTP client request (such as the curl command) located at http://169.254.169.254/latest/meta-data. IMDSv1 is fully secure and AWS will continue to support it, but IMDSv2 adds new “belt and braces” protections for four types of vulnerabilities that could be used to try to access the IMDS. For more see: (https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/)  From EMR 5.32 and 6.2 onward, Amazon EMR components use IMDSv2 for all IMDS calls. For IMDS calls in your application code, you can use both IMDSv1 or IMDSv2. It is recommended to turn off IMDSv1 and only allow IMDSv2 for added security. This can be configured in EMR Security Configurations. For more information, see:https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html#emr-security-configuration-imdsv2)  ","version":"Next","tagName":"h2"},{"title":"BP 3.5 Create a separate IAM role for each cluster or use case​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-35-create-a-separate-iam-role-for-each-cluster-or-use-case","content":" EMR uses an IAM service Roles to perform actions on your behalf to provision and manage clusters. It is recommended to create a separate IAM Role for each use case and workload, allowing you to segregate access control between clusters. If you have multiple clusters, each cluster can only access the services and data defined within the IAM policy.  ","version":"Next","tagName":"h2"},{"title":"BP 3.6 Use scoped down IAM policies for authorization such as AmazonEMRFullAccessPolicy_v2​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-36-use-scoped-down-iam-policies-for-authorization-such-as-amazonemrfullaccesspolicy_v2","content":" EMR provides managed IAM policies to grant specific access privileges to users. Managed policies offer the benefit of updating automatically if permission requirements change. If you use inline policies, service changes may occur that cause permission errors to appear.  It is recommended to use new managed policies (v2 policies) which have been scoped-down to align with AWS best practices. The v2 managed policies restrict access using tags. They allow only specified Amazon EMR actions and require cluster resources that are tagged with an EMR-specific key. For more details and usage, see: (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-policy-fullaccess-v2.html)  ","version":"Next","tagName":"h2"},{"title":"BP 3.7 Audit user activity with AWS CloudTrail​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-37-audit-user-activity-with-aws-cloudtrail","content":" AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service, and is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events, which includes calls from the Amazon EMR console or calls to the Amazon EMR API. If you create a Trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR.  You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn’t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR.  Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements.  ","version":"Next","tagName":"h2"},{"title":"BP 3.8 Upgrade your EMR Releases frequently or use a Custom AMI to get the latest OS and application software patches​","type":1,"pageTitle":"3 - Security","url":"/aws-emr-best-practices/docs/bestpractices/Security/best_practices#bp-38-upgrade-your-emr-releases-frequently-or-use-a-custom-ami-to-get-the-latest-os-and-application-software-patches","content":" Each Amazon EMR release version is &quot;locked&quot; to the Amazon Linux AMI version to maintain compatibility. This means that the same Amazon Linux AMI version is used for an Amazon EMR release version even when newer Amazon Linux AMIs become available. For this reason, we recommend that you use the latest Amazon EMR release version unless you need an earlier version for compatibility and are unable to migrate.  If you must use an earlier release version of Amazon EMR for compatibility, we recommend that you use the latest release in a series. For example, if you must use the 5.12 series, use 5.12.2 instead of 5.12.0 or 5.12.1. If a new release becomes available in a series, consider migrating your applications to the new release. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Reliability/introduction","content":"Introduction EMR Reliability best practices discusses how to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, imprve availability of resources when required and mitigate disruptions such as misconfiguration or transient network issues.","keywords":"","version":"Next"},{"title":"5.2 - Spark troubleshooting and performance tuning","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning","content":"","keywords":"","version":"Next"},{"title":"5.2.1 - Spark Structured Streaming applications have high Connection Create Rate to Amazon MSK​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#521-----spark-structured-streaming-applications-have-high-connection-create-rate-to-amazon-msk","content":" Symptom: Amazon MSK cluster has high CPU usage and MSK metrics indicate high connection create rate to the cluster.  Analysis: By default, Spark Structured Streaming Kafka connector has a 1:1 mapping relation of MSK TopicPartitions to Spark tasks. Between micro batches, Spark tries to (best effort) assign the same MSK TopicPartitions to the same executors which in turn reuses the Kafka consumers and connections.  Spark Structured Streaming Kafka connector has an option minPartitions which can divide large TopicPartitions to smaller pieces. When minPartitions is set to a value larger than the number of TopicPartitions, Spark creates tasks based on minPartitions to increase parallelism (the number of Spark tasks will be approximately minPartition).  As 1:1 mapping doesn't exist anymore, Spark executors are randomly assigned to process any TopicPartition OffsetRanges. An executor processed TopicPartition X can be assigned to process TopicPartition Y in next micro batch. A new Kafka consumer/connection needs to be created if Y is on another MSK broker.One Spark executor can be assigned to process multiple Spark tasks with the same MSK TopicPartition on different OffsetRanges. And in Spark 2.x, Kafka consumer cache is disabled when multiple tasks in the same executor read the same TopicPartitions .  Setting minPartitions comes at a cost of initializing Kakfa consumers at each micro batch. This may impact performance especially when using SSL.  A test was run with following test environment:  Kafka version 2.8.1  3 kafka.m5.xlarge instancestest kafka topic has 10 partitionsonly SASL/SCRAM authentication enabled  EMR 5.36 (Spark 2.4.8) cluster  30 core nodes - EC2 m5.4xlarge  Spark Structured Streaming test application has 5 cores 5G memory for each executor.  Below figure shows the test result of different minPartitions values with MSK’s ConnectionCreationRate and CPUUser usages. As shown in the test result, higher ConnectionCreationRate is related to higher CPU usage.  Test1: 50 minPartitions 16:40-17:30 Test2: 100 minPartitions 18:00-18:35 Test3: 200 minPartitions 19:00-19:35 Test4: no minPartitions 20:06 - 20:30    Recommendation:  Upgrade to the latest EMR version (spark 3.x) to use Sparks consumer pool cache feature. This feature allows Spark to cache more than one Kafka consumer with same MSK TopicPartition at each executor, and reuse the consumers in later micro batches. This will allow you to set minPartitions while reduce the ConnectionCreationRate. On EMR 5.x (Spark 2.x), only set min partitions when needed - for example, if you have data skew or if your stream is falling behind. Min partitions will allow you to increase parallelism and process records faster but at the expense of high connection rates and CPU.  ","version":"Next","tagName":"h2"},{"title":"5.2.2 - spark.driver.maxResultSize error on an EMR heterogeneous cluster but the driver is not collecting data​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#522---sparkdrivermaxresultsize-error-on-an-emr-heterogeneous-cluster-but-the-driver-is-not-collecting-data","content":" Symptom: Spark jobs fail from time to time and below error is seen in the log:  22/08/22 14:14:24 ERROR FileFormatWriter: Aborting job f6913a46-d2d8-46f0-a545-2d2ba938b113. org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 179502 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2171)  By setting spark.driver.maxResultSize to 0(unlimited), the error is gone. But the Spark job is not collecting data to driver, how can the result returning to driver exceed 1024MB?  Analysis: Each finished task sends a serialized WriteTaskResult object to driver. The object size is usually several kilobytes, e.g.  22/09/06 22:24:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 5192 bytes result sent to driver  From the log, we can see there are 179502 (or more) tasks. For such a number of tasks, the total result size can exceed 1024MB for serialized WriteTaskResult objects only.  The job is reading parquet files from S3 and the input folder has 3.5K parquet files with average size ~116MB per file. As default spark.sql.files.maxPartitionBytes is 128M, so approximately one file to one Spark task. The job's processing logic further splits one task to ~11 tasks. Total tasks should be 3.5K * 11 = 38.5K. But why there are 179502 (or more) tasks?  To find root cause, we need to understand how Spark SQL decides the max size of a partition for non-bucketed splittable files.  Spark2 is following below formula (Spark3 has a slightly different formula which is described later)  maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))  maxSplitBytes is the max bytes per partition.  defaultMaxSplitBytes is from spark.sql.files.maxPartitionBytes whose default value is 128M.  openCostInBytes is from spark.sql.files.openCostInBytes whose default value is 4M.  bytesPerCore = (Sum of all data file size + num of file * openCostInBytes ) / defaultParallelism  defaultParallelism’s default value is total number of virtual cores running the job. It can also be set to a value by defining spark.default.parallelism.  When there are a large number of virtual cores allocated to the Spark job by Yarn, maxSplitBytes can be smaller than defaultMaxSplitBytes, i.e. more tasks will be created. For this case, from Spark UI, we know total number of vcores is 15837, i.e. defaultParallelism is 15837  bytesPerCore = (116M * 3500 + 4M * 3500)/15837 = 26.5M  maxSplitBytes = min(128M, max(4M, 26.5M)) = 26.5M. So one 116M parquet file is split into 4~5 Spark tasks. 3.5K * 11 * 5 = 192.5K -- that's why there were 179502 (or more) tasks.  Note that the vcore count is based on Yarn containers, not physical cores. As Yarn’s default container allocation is based on available memory, this means there can be vcore over subscriptions in a heterogeneous EMR cluster.  For example, if we have a heterogeneous EMR cluster as below:  Core Node: c5.12xlarge(48cores/96GB) — Memory allocated to Yarn: 90112MB Task Node: r5.8xlarge(32cores/256GB) — Memory allocated to Yarn: 253952MB  The default EMR executor size is based on core node instance type. In this example, for c5.12xlarge, default executor size is 3 cores 4743M memory. Default spark.yarn.executor.memoryOverheadFactor is 0.1875. A Yarn container has 3 cores, 4743MB*(1+0.1875)= 5632MB memory.  On c5.12xlarge, Yarn can allocate 16 conatainers with 48 vcores in total: 90112MB/5632MB = 16 containers * 3 core = 48 vcores  While on r5.8xlarge, Yarn can allocate 45 containers with 135 vcores in total: 253952MB/5632MB = 45 containers * 3 core = 135 vcores - 32cores = 103 vcore oversubscription  Recommendation: When Spark reads splittable files, maxSplitBytes can be smaller than spark.sql.files.maxPartitionBytes if there are a big number of vcores allocated to the job. Use the formula described here to set spark.default.parallelism value properly and have a reasonable maxSplitBytes.  Spark 3  Spark 3 provides more options to controlmaxSplitBytes as below   def maxSplitBytes( sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = { val defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes val minPartitionNum = sparkSession.sessionState.conf.filesMinPartitionNum .getOrElse(sparkSession.leafNodeDefaultParallelism) val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum val bytesPerCore = totalBytes / minPartitionNum Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) }   filesMinPartitionNum is from spark.sql.files.minPartitionNum. It is the suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is spark.default.parallelism.  leafNodeDefaultParallelism is from spark.sql.leafNodeDefaultParallelism. It is the default parallelism of Spark SQL leaf nodes.  Setting either of the above two parameters has the same effect as spark.default.parallelism on maxSplitBytes.  ","version":"Next","tagName":"h2"},{"title":"5.2.3 - Issues related to Spark logs​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#523-----issues-related-to-spark-logs","content":" ","version":"Next","tagName":"h2"},{"title":"5.2.3.1 Spark events logs not getting pushed to S3​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5231-spark-events-logs-not-getting-pushed-to-s3","content":" Symptom: This might be caused if the spark logs properties is configured with HDFS location, not with S3 location.  Troubleshooting Steps::  Check the appusher logs of the cluster for the time of the issue where the app pusher log location on your 21EMR logs bucket is &lt;s3_log_bucket&gt;/&lt;prefix&gt;/j- xxxxxxx&gt;/daemons/apppusher.log.gzLook for error java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Wrong FS: 23 s3://demo-bucket/testing-prod/spark/logs,expected: hdfs://ip-XX-XX-XXX-XXX.eu-west- 241.compute.internal:8020 By default, Spark Structured Streaming Kafka connector has a 1:1 mapping relation of MSK TopicPartitions to Spark tasks. Between micro batches, Spark tries to (best effort) assign the same MSK TopicPartitions to the same executors which in turn reuses the Kafka consumers and connections.Check the spark-defaults.conf file found in the location /etc/spark/conf.dist on the master node for the 26configurationsmentionedbelow: 27spark.eventLog.dir : The spark jobs themselves must be configured to log events, and to log them to the same 28shared,writabledirectory 29spark.history.fs.logDirectory: For the filesystem history provider, the URL to the directory containing 30application event logs to load.Restart the spark history server to reflect the configuration changes. Various EMR version have different 32command to restart the services. Sudo systemctl restart spark-history-server  Additional Reference:  Consideration and limitations of spark history UI  Spark monitoring and instrumentation  ","version":"Next","tagName":"h3"},{"title":"5.2.3.2 - Spark History Server not opening up​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5232----spark-history-server-not-opening-up","content":" Symptom: This issue occurs if the Spark history server memory is low. You may also have to enable web UI access to the spark history server  Troubleshooting Steps::  SSH in to the master node, go to the location /etc/spark/conf/spark-env.sh and set the parameter SPARK_DAEMON_MEMORY to an higher value, for example 4g (or more) You can manually edit the spark-env file at &quot;/etc/spark/conf/spark-env.sh&quot; on the master node Restart the spark-history-server You can confirm the memory increase with the given command below, in this parameter &quot;-Xmx4g&quot;: Or you can reconfigure from EMR console select the primary instance group and click on reconfigure and add the json and while saving click on apply to all instance groups.  [ { &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Properties&quot;: { &quot;SPARK_DAEMON_MEMORY&quot;: &quot;4g&quot; } ]   Note: In multimaster setup does not support persistent application user interface.  Additional Reference:  Why can’t I view the Apache Spark history events or logs from the Spark web UI in Amazon EMR?  Web Interfaces  Configure Spark  ","version":"Next","tagName":"h3"},{"title":"5.2.3.3 -Spark application failure or Spark streaming application failure with 137 error code​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5233---spark-application-failure-or-spark-streaming-application-failure-with-137-error-code","content":" Symptom: There can be various reasons for a spark application failures, most common root causes arestage failure spark EMR, Container killed on request. Exit code is 137 and ExecutorLostFailure &quot;Slave lost&quot;.  Troubleshooting Steps:  If you have submitted the application via step, check the step logs stderr to get the exception details. In the step logs you will also find the application ID related to the step use the step ID logs to get the application ID and check the container logs stderr.gz to find the error In the above logs look for the information as mentioned on this article EMR troubleshooting failed spark jobs  ","version":"Next","tagName":"h3"},{"title":"5.2.3.4 -Spark application event with incomplete logs​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5234---spark-application-event-with-incomplete-logs","content":" Symptom:You might encounter this error if in case of incorrect configuration of log aggregation for long running applications. Application logs from &quot;/var/log/hadoop-yarn/apps&quot; hdfs directory are deleted after every 2 days. Long-running Spark jobs such as Spark streaming, and large jobs, such as Spark SQL queries can generate large event logs. Large events logs, can rapidly deplete disk space on compute instances and may result in OutOfMemory errors when loading persistent UIs. To avoid these issues, it is recommended that you turn on the Spark event log rolling and compaction feature. This feature is available on Amazon EMR versions emr-6.1.0 and later. Check how to enable yarn log aggregation for spark jobs  Troubleshooting Steps:  Check the instance state logs &lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/deamon/instance-state/(find the log as per the error time frame) of the your EMR instances and search for df -h and check for the /mnt usage for that particular instance. Below parameters can be configured.  spark.eventLog.rolling.enabled – Turns on event log rolling based on size. This is deactivated by default. spark.eventLog.rolling.maxFileSize – When rolling is activated, specifies the maximum size of the event log file before it rolls over. spark.history.fs.eventLog.rolling.maxFilesToRetain – Specifies the maximum number of non-compacted event log files to retain.  Note: Instance-state logs are snapshots captured at the instance level every 15 min.  ","version":"Next","tagName":"h3"},{"title":"5.2.3.5 -Access spark history server for old cluster​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5235---access-spark-history-server-for-old-cluster","content":" Symptom: To access your old EMR cluster logs, go to application history and relevant log files for active and terminated clusters. The logs are available for 30 days after the application ends. On the Application user interfaces tab or the cluster Summary page for your cluster in the old console for Amazon EMR 5.30.1 or 6.x, choose the YARN timeline server, Tez UI, or Spark history server link. Starting from EMR version 5.25.0, persistent application user interface (UI) links are available for Spark History Server, which doesn't require you to set up a web proxy through a SSH connection and can be directly accessed via the AWS EMR Web Console. Navigate to the terminated cluster whose logs you require, clicking on Application user interfaces tab (on old EMR console) or Applications tab and select Persistent application UIs (on new EMR console) and finally clicking on Spark History Server link  ","version":"Next","tagName":"h3"},{"title":"5.2.4 - Issues Related to Spark session​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#524----issues-related-to-spark-session","content":" ","version":"Next","tagName":"h2"},{"title":"5.2.4.1 -Spark session not getting generated with Missing std out​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5241---spark-session-not-getting-generated-with-missing-std-out","content":" Symptom: You might encounter below listed spark session issues  Error when trying to run jupyter note book for spark. The code failed because of a fatal error: Invalid status code '500' from http://localhost:8998/sessions with error payload: &quot;java.lang.NullPointerException&quot;.java.net.ConnectException: Connection refused ERROR SparkContext: Error initializing SparkContext.org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for s3ablock-0001-  Probable Root cause  Spark code might lack spark session initialization. Make sure you follow the official documentation of Spark to build an spark session. Invalid status code '500'is related to a server-side error, check the livy server logs running on the EMR cluster.  ","version":"Next","tagName":"h3"},{"title":"5.2.4.2 -Spark session with livy is failing​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5242---spark-session-with-livy-is-failing","content":" Symptom: If you are seeing Error message :The code failed because of a fatal error:Session 11 did not start up in 60 seconds, you can try the below steps:  Troubleshooting Steps:  Make sure Spark has enough available resources for Jupyter to create a Spark context.Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.Restart the kernel.Check the following configuration are present on your EMR cluster, if not then please add the same  [ { &quot;Classification&quot;: &quot;livy-conf&quot;, &quot;Properties&quot;: { &quot;livy.server.session.timeout-check&quot;: &quot;true&quot;, &quot;livy.server.session.timeout&quot;: &quot;2h&quot;, &quot;livy.server.yarn.app-lookup-timeout&quot;: &quot;120s&quot; } } ]   Restart the Livy server to apply the configuration  sudo systemctl stop livy-server sudo systemctl start livy-server   To further troubleshoot the issue, you can find more information on the livy-livy-server.out found on the following location /var/log/livy  Additional resource:  To know more about the livy we you can go over the following resource.  ","version":"Next","tagName":"h3"},{"title":"5.2.4.3 -Spark application failure due to Disk space or CPU Load​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5243---spark-application-failure-due-to-disk-space-or-cpu-load","content":" Symptom: You might encounter disk space/CPU related issues due to:  The log pusher service is not running or consuming high CPU, to know more check the instance-state logs for the CPU, memory state, and garbage collector threads of the node. Local disk of the node is getting full, as application logs might be stored on the nodes local disk where your application is running, so a long running application can fill in the local disk of a node There might be another app which is running on the cluster at the same time could be filling up disk space/consuming CPU cycles. High retention period of the spark event and the yarn container logs  Note: Disk space or CPU load may not always cause an application failure, however could lead to performance regression  Troubleshooting Steps:  If the logpusher is malfunctioning or taking up lot of CPU then, it can result disk usage climb up Check the log pusher server status by using the commandsystemctl status logpusherIf the log pusher shows up near the top by running the below command in the instance-state logs, then it would benefit a restart  ps auxwww --sort -%cpu | head -20 sudo systemctl status logpushe   You can also check latest logs of the logpusher to find errors and exceptions to report to the AWS premium support to help you further. Logpusher logs can be found in the following location /emr/logpusher/logYou can also proactively manage the disk space by deleting old logs form the below mentioned location  Hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///mnt/hdfs,file:///mnt1/hdfs&lt;/value&gt; &lt;/property&gt; yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/mnt/yarn,/mnt1/yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;Where to store container logs.&lt;/description&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/var/log/hadoop-yarn/containers&lt;/value&gt; &lt;/property&gt;   You can use the command sudo du -h /emr | sort -hr to find the files consuming space. Use this command repeatedly and backup the files and delete unwanted files. You can also Dynamically scale up storage on Amazon EMR clusters You add more EBS capacity if you encounter no space left on device error in the spark job in Amazon EMR You can prevent hadoop and spark user cache consuming disk space  ","version":"Next","tagName":"h3"},{"title":"5.2.5 - Spark application failure​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#525----spark-application-failure","content":" ","version":"Next","tagName":"h2"},{"title":"5.2.5.1 Spark app class not found exception​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5251-spark-app-class-not-found-exception","content":" Refer to the the blog post to troubleshoot spark app class not found exception.  ","version":"Next","tagName":"h3"},{"title":"5.2.5.2 Spark app failing to write to s3 or Spark job failure due access denied exception​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5252-spark-app-failing-to-write-to-s3-or-spark-job-failure-due-access-denied-exception","content":" Symptom: You can encounter the error EmrOptimizedParquetOutputCommitter: EMR Optimized committer is not supported by this filesystem (org.apache.hadoop.hdfs.DistributedFileSystem)Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services .s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied  Probable Root causes:  Bucket policies and IAM policies related issue AWS s3 ACL setting S3 block Public Access setting S3 object lock settings VPC endpoint policy AWS organization policies Access point setting  Troubleshooting Steps:  Check the application logs to confirm the error Status Code: 403; Error Code:AccessDenied and check the probable root causes Generate Amazon S3 request ID for AWS support to help in debugging the issue  Best practices and Recommendations:  Aws recommends using &quot;s3://&quot; instead of S3a:// S3n:// which is based on EMRFS as it is build and maintained by AWS and supports additional features. 403 S3 Error troubleshooting Cross Account Bucket Access  ","version":"Next","tagName":"h3"},{"title":"5.2.5.3 Cluster mode job failure​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5253-cluster-mode-job-failure","content":" Symptom: In the absence of correct parameters and the dependencies installed before running a job in the cluster mode, you can encounter cluster mode job failures as below:  ERROR Client: Application diagnostics message: User application exited with status 1 exitCode: 13 User application exited with status 1  Troubleshooting Steps:  Inspect the container of application master logs containers/application_xx_0002/container_xx_0002_01_000001 Look for the probable keywords like &quot;exitCode: 13&quot; or module not found error or User application exited with status 1 Look into the application master container stdout and controller log of the Application master to get more information on the error. Make sure to pass the right parameters with the dependencies installed correctly before running the job in the cluster mode.  ","version":"Next","tagName":"h3"},{"title":"5.2.5.4 Spark Troubleshooting Spark FetchFailedExceptions, Memory Issues and Scaling Problems in Applications​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#5254-spark-troubleshooting-spark-fetchfailedexceptions-memory-issues-and-scaling-problems-in-applications","content":" Symptom:  org.apache.spark.shuffle.FetchFailedException: java.util.concurrent.TimeoutException: Timeout waiting for task.at org.org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748 Caused by: java.io.IOException: Map_1: Shuffle failed with too many fetch failures and insufficient progress  Probable Root cause:  If the executors are busy or under high GC pressure, then executor cannot cater to the shuffle request. You can use the external shuffle service hosted on Yarn Node manager, which will help any shuffle state written by an executor to be served beyond the executor’s lifetime. NodeManager(NM) may have been killed and restarted multiple times while running your job, with java.lang.OutOfMemoryError: GC overhead limit exceeded and java.lang.OutOfMemoryError: Java heap space errors, indicating not enough heap space to address the shuffle request. As YarnShuffleService is launched by the Node Manager, insufficient memory allocated for the Node Manager can cause aggressive GC during the shuffle phase.Amazon EMR enables the External Shuffle Service by default, the shuffle output is written to local disk. Since LocalDiskEncryption is enabled, if thousands of blocks are being fetched from local disk, it could lead to higher memory utilization on Nodemanager JVM as well.  Troubleshooting Steps:  Check the application logs, yarn resource manager logs and instance-controller logs to get more information on the error.Find the node which is causing the shuffle data loss and make note of the event timeline.Review the application logs of the Managed Scaling resize request cluster manager to identify the nodes included in the shrink policy. Verify whether the update to the EMR DB for writing the shrink policy was successful.Check the Instance controller to exactly find new resize target and which instance being send by the cluster manager to Instance controller to keep and to Remove listIf node is not in toKeep but toRemove list or if they are empty although it might have shuffle data, dig into Spark Shuffle enricher task how was it added in toRemove list for log.In some cases, the shuffle metrics may altogether be missing around the time of resize affecting toKeep and toRemove list.Check the spark metric collector logs to find out and share with the premium support if there was an issue collecting the metrics at cluster  Note- Emr Managed scaling is shuffle aware  Configuration to consider:  spark.reducer.maxBlocksInFlightPerAddress: limits the number of remote blocks being fetched per reduce task from a given host port. spark.reducer.maxSizeInFlight: Maximum size of map outputs to fetch simultaneously from each reduce task. spark.shuffle.io.retryWait (Netty only): How long to wait between retries of fetches. spark.shuffle.io.maxRetries (Netty only): Fetches that fail due to IO-related exceptions are automatically retried if this is set to a non-zero value. spark.shuffle.io.backLog: Length of the accept queue for the shuffle service. Increasing the memory for NodeManager Deamon on EMR cluster startup yarn-env = YARN_NODEMANAGER_HEAPSIZE&quot;: &quot;10000&quot; Tuning number of Partitions and Spark parallelism(spark.default.parallelism) for your cluster/job. This may limit the number of shuffles that's happening per job and consequently reduce the load on all NodeManager's. Note that changing configuration can have performance impact. Keep the EMR default spark.shuffle.service.enabled true(spark-defaults.conf)spark.dynamicAllocation.enabled set to true (EMR default)  Additional resource:  Spark EMR configuration  Spark Shuffle Data Aware  Container killed by Yarn for exceeding memory limit troubleshooting  Optimize Spark performance  ","version":"Next","tagName":"h3"},{"title":"5.2.6 - Spark SQL issue​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#526----spark-sql-issue","content":" ","version":"Next","tagName":"h2"},{"title":"Spark Query failures for jupyter notebook Python 3​","type":1,"pageTitle":"5.2 - Spark troubleshooting and performance tuning","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/troubleshooting_and_tuning#spark-query-failures-for-jupyter-notebook-python-3","content":" Symptom: If EMR job fails with an error message Exit status: -100. Diagnostics: Container released on a lost node, it might be because of  A core or task node is terminated because of high disk space utilization. A node becomes unresponsive due to prolonged high CPU utilization or low available memory.  Troubleshooting Steps:  Check the following Cloudwatch Metrics for metrics MR unhealthy nodes and MR lost nodes to determine root cause. Add more Amazon Elastic Block Store (Amazon EBS) capacity to the new EMR clusters. You can do this when you launch a new cluster or by modifying a running cluster. You can also add more core or task nodes. If larger EBS volumes don't resolve the problem, attach more EBS volumes to the core and task nodes. ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/migration/introduction","content":"Introduction This section provides best practice guidance and tools to migrate data processing applications from self-managed environments to Amazon EMR. EMR Migration Guide : This is a comprehensive technical document that provides guidance for migrating various components including data, application, security configurations etc from self-managed data processing applictions to Amazon EMR Data Migration: We recommend using AWS Datasync for migrating HDFS to S3. Start with this Data Sync support for HDFS blog to review Datasync capabilities and how to get started with Data migrations Data pipelines Migrations: The following tools can be useful in migrating your current data pipelines to AWS Oozie to MWAAOozie to stepfunctions Data Governance: The following tools can helpful in migrating your current data catalogs to AWS Migrate metadata between Hive metastore and AWS Glue Data CatalogHive Glue Catalog Sync Agent For further assistance reach out to aws-bdms-emr@amazon.com","keywords":"","version":"Next"},{"title":"Cost Optimizations","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 1.1 Use Amazon S3 as your persistent data store​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-11-use-amazon-s3-as-your-persistent-data-store","content":" As of Oct 1, 2021, Amazon S3 is 2.3 cents a GB/month for the first 50TB. This is $275 per TB/year which is a much lower cost than 3x replicated data in HDFS. With HDFS, you’ll need to provision EBS volumes. EBS is 10 cents a GB/month, which is ~4x the cost of Amazon S3 or 12x if you include the need for 3x HDFS replication.  Using Amazon S3 as your persistent data store allows you to grow your storage infinitely, independent of your compute. With on premise Hadoop systems, you would have to add nodes just to house your data which may not be helping your compute and only increase cost. In addition, Amazon S3 also has different storage tiers for less frequently accessed data providing opportunity for additional cost savings.  EMR makes using Amazon S3 simple with EMR File System (EMRFS). EMRFS is an implementation of HDFS that all EMR clusters use for accessing data in Amazon S3.  Note: HDFS is still available on the cluster if you need it and can be more performant compared to Amazon S3. HDFS on EMR uses EBS local block store which is faster than Amazon S3 object store. Some amounts of HDFS/EBS may be still be required. You may benefit from using HDFS for intermediate storage or need it to store application jars. However, HDFS is not recommended for persistent storage. Once a cluster is terminated, all HDFS data is lost.  ","version":"Next","tagName":"h2"},{"title":"BP 1.2 Compress, compact and convert your Amazon S3 Objects​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-12-compress-compact-and-convert-your-amazon-s3-objects","content":" Compress - By compressing your data, you reduce the amount of storage needed for the data, and minimize the network traffic between S3 and the EMR nodes. When you compress your data, make sure to use a compression algorithm that allows files to be split or have each file be the optimal size for parallelization on your cluster. File formats such as Apache Parquet or Apache ORC provide compression by default. The following image shows the size difference between two file formats, Parquet (has compression enabled) and JSON (text format, no compression enabled). The Parquet dataset is almost five times smaller than the JSON dataset despite having the same data.    Compact - Avoid small files. Generally, anything less than 128 MB. By having fewer files that are larger, you can reduce the amount of Amazon S3 LIST requests and also improve the job performance. To show the performance impact of having too many files, the following image shows a query executed over a dataset containing 50 files and a query over a dataset of the same size, but with 25,000 files. The query that executed on 1 file is 3.6x faster despite the tables and records being the same.    Convert - Columnar file formats like Parquet and ORC can improve read performance. Columnar formats are ideal if most of your queries only select a subset of columns. For use cases where you primarily select all columns, but only select a subset of rows, choose a row optimized file format such as Apache Avro. The following image shows a performance comparison of a select count(*) query between Parquet and JSON (text) file formats.  The query that executed over parquet ran 74x faster despite being larger in size.    ","version":"Next","tagName":"h2"},{"title":"BP 1.3 Partition and Bucket your data in Amazon S3​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-13-partition-and-bucket-your-data-in-amazon-s3","content":" Partition your data in Amazon S3 to reduce the amount of data that needs to be processed. When your applications or users access the data with the partition key, it only retrieves the objects that are required. This reduces the amount of data scanned and the amount of processing required for your job to run. This results in lower cost.  For example, the following image shows two queries executed on two datasets of the same size. One dataset is partitioned, and the other dataset is not.    The query over the partitioned data (s3logsjsonpartitioned) took 20 seconds to complete and it scanned 349 MB of data. The query over the non-partitioned data (s3logsjsonnopartition) took 2 minutes and 48 seconds to complete and it scanned 5.13 GB of data.  Bucketing is another strategy that breaks down your data into ranges in order to minimize the amount of data scanned. This makes your query more efficient and reduces your job run time. The range for a bucket is determined by the hash value of one or more columns in the dataset. These columns are referred to as bucketing or clustered by columns. A bucketed table can be created as in the below example:  CREATE TABLE IF NOT EXISTS database1.table1 ( col1 INT, col2 STRING, col3 TIMESTAMP ) CLUSTERED BY (col1) INTO 5 BUCKETS STORED AS PARQUET LOCATION ‘s3:///buckets_test/hive-clustered/’;   In this example, the bucketing column (col1) is specified by the CLUSTERED BY (col1) clause, and the number of buckets (5) is specified by the INTO 5 BUCKETS clause.  Bucketing is similar to partitioning – in both cases, data is segregated and stored – but there are a few key differences. Partitioning is based on a column that is repeated in the dataset and involves grouping data by a particular value of the partition column. While bucketing organizes data by a range of values, mainly involving primary key or non-repeated values in a dataset. Bucketing should be considered when your partitions are not comparatively equal in size or you have data skew with your keys. Certain operations like map-side joins are more efficient in bucket tables vs non bucketed ones.  ","version":"Next","tagName":"h2"},{"title":"BP 1.4 Use the right hardware family for the job type​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-14-use-the-right-hardware-family-for-the-job-type","content":" Most Amazon EMR clusters can run on general-purpose EC2 instance types/families such as m5.xlarge and m6g.xlarge. Compute-intensive clusters may benefit from running on high performance computing (HPC) instances, such as the compute-optimized instance family (C5). High memory-caching spark applications may benefit from running on high memory instances, such as the memory-optimized instance family (R5). Each of the different instance families have a different core:memory ratio so depending on your application characteristic, you should choose accordingly.  The master node does not have large computational requirements. For most clusters of 50 or fewer nodes, you can use a general-purpose instance type such as m5. However, the master node is responsible for running key services such as Resource manager, Namenode, Hiveserver2 as such, it’s recommended to use a larger instance such as 8xlarge+. With single node EMR cluster, the master node is a single point of failure.    ","version":"Next","tagName":"h2"},{"title":"BP 1.5 Use instances with instance store for jobs that require high disk IOPS​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-15-use-instances-with-instance-store-for-jobs-that-require-high-disk-iops","content":" Use dense SSD storage instances for data-intensive workloads such as I3en or d3en. These instances provide Non-Volatile Memory Express (NVMe) SSD-backed instance storage optimized for low latency, very high random I/O performance, high sequential read throughput and provide high IOPS at a low cost. EMR workloads that heavily use HDFS or spend a lot of time writing spark shuffle data can benefit from these instances and see improved performance which reduces overall cost.  ","version":"Next","tagName":"h2"},{"title":"BP 1.6 Use Graviton2 instances​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-16-use-graviton2-instances","content":" Amazon EMR supports Amazon EC2 graviton instances with EMR Versions 6.1.0, 5.31.0 and later. These instances are powered by AWS Graviton2 processors that are custom designed by AWS utilizing 64-bit ArmNeoverse cores to deliver the best price performance for cloud workloads running in Amazon EC2. On Graviton2 instances, Amazon EMR runtime for Apache Spark provides an additional cost savings of up to 30%, and improved performance of up to 15% relative to equivalent previous generation instances.  For example, when you compare m5.4xlarge vs m6g.4xlarge. The total cost (EC2+EMR) / hour is  Instance Type\tEC2 + EMR Costm5.4xlarge:\t$0.960 m6g.4xlarge:\t$0.770  This is a 19.8% reduction in cost for the same amount of compute - 16vCPU and 64Gib Memory  For more information, see [here] (https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances)  ","version":"Next","tagName":"h2"},{"title":"BP 1.7 Select the appropriate pricing model for your use case and node type​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-17-select-the-appropriate-pricing-model-for-your-use-case-and-node-type","content":" The following table is general guideline for purchasing options depending on your application scenario.  Application scenario\tMaster node purchasing option\tCore nodes purchasing option\tTask nodes purchasing optionLong-running clusters and data warehouses\tOn-Demand\tOn-Demand or instance-fleet mix\tSpot or instance-fleet mix Cost-driven workloads\tSpot\tSpot\tSpot Data-critical workloads\tOn-Demand\tOn-Demand\tSpot or instance-fleet mix Application testing\tSpot\tSpot\tSpot  For clusters where you need a minimum compute at all times - e.g spark streaming, ad hoc clusters. Using reserved instances or saving plans is recommended.  For more information, see here  ","version":"Next","tagName":"h2"},{"title":"BP 1.8 Use spot instances​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-18-use-spot-instances","content":" Spot instances are unused EC2 Capacity that is offered at up to a 90% discount (vs On-Demand pricing) and should be used when applicable. While EC2 can reclaim Spot capacity with a two-minute warning, less than 5% of workloads are interrupted. Due to the fault-tolerant nature of big data workloads on EMR, they can continue processing, even when interrupted. Running EMR on Spot Instances drastically reduces the cost of big data, allows for significantly higher compute capacity, and reduces the time to process big data sets.    For more information, see the spot usage best practices section  Additionally, AWS Big Data Blog: Best practices for running Apache Spark applications using Amazon EC2 Spot Instances with Amazon EMR, see here  Amazon EMR Cluster configuration guidelines and best practices, see here  ","version":"Next","tagName":"h2"},{"title":"BP 1.9 Mix on-Demand and spot instances​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-19-mix-on-demand-and-spot-instances","content":" Consider using a combination of Spot and On-Demand instances to lower cost and runtime. Two examples where where this may be applicable are when:  Cost is more important than the time to completion, but you cannot tolerate an entire cluster being terminated.  In this case, you can use Spot instances for the task nodes, and use On-Demand/Reserved instances for the master and core nodes. Even if all spot nodes are reclaimed, your cluster will still be accessible and tasks will be re-run on the remaining core nodes.  You need to meet SLAs but are also considered about cost  In this case, you would provision enough on demand capacity to meet your SLAs and then use additional spot to bring down your average cost. If spot is not available, you’ll still have on demand nodes to meet your SLA. When spot is available, your cluster will have additional compute which reduce run time and lowers the total cost of your job.For example:  10 node cluster running for 14 hours Cost = 1.0 * 10 * 14 = $140 Add 10 more nodes on Spot at 0.5$/node 20 node cluster running for 7 hours Cost = 1.0 * 10 * 7 = $70 = 0.5 * 10 * 7 = $35 Total $105 50 % less run-time ( 14→ 7) 25% less cost (140→ 105)     One consideration when mixing on demand and spot is if spot nodes are reclaimed, tasks or shuffle data that were on those spot nodes may have to be re executed on the remaining nodes. This reprocessing would increase the total run time of the job compared to running on only on demand.  ","version":"Next","tagName":"h2"},{"title":"BP 1.10 Use EMR managed scaling​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-110-use-emr-managed-scaling","content":" With Amazon EMR versions 5.30.0 and later (except for Amazon EMR 6.0.0), you can enable EMR managed scaling. Managed scaling lets you automatically increase or decrease the number of instances or units in your cluster based on workload. EMR continuously evaluates cluster metrics to make scaling decisions that optimize your clusters for cost and speed improving overall cluster utilization. Managed scaling is available for clusters composed of either instance groups or instance fleets  This helps you reduce costs by running your EMR clusters with just the correct of amount of resources that your application needs. This feature is also useful for use cases where you have spikes in cluster utilization (i.e. a user submitting a job) and you want the cluster to automatically scale based on the requirements for that application.  Here’s an example of cluster without auto scaling. Since the size of the cluster is static, there are resources you are paying for but your job does not actually need.    Here’s an example of cluster with auto scaling. The cluster capacity (blue dotted line) adjusts to the job demand reducing unused resources and cost.  ","version":"Next","tagName":"h2"},{"title":"BP 1.11 Right size application containers​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-111-right-size-application-containers","content":" By default, EMR will try to set YARN and Spark memory settings to best utilize the instances compute resources. This is important to maximize your cluster resources. Whether you are migrating jobs to EMR or writing a new application, It is recommended that you start with default EMR configuration. If you need to modify the default configuration for your specific use case, It’s important to use all the available resources of the cluster - both CPU and Memory.  For example, if you had a cluster that is using m5.4xlarge instances for its data nodes, you’d have 16 vCPU and 64GB of memory.  EMR will automatically set yarn.nodemanager.resource.cpu-vcores and yarn.nodemanager.resource.memory-mb in yarn-site.xml to allocate how much of the instances resources can be used for YARN applications. In the m5.4xlarge case, this is 16vCPU and 57344 mb. When using custom configuration for your spark containers, you want to ensure that the memory and cores you allocate to your executor is a multiple of the total resources allocated to yarn. For example, if you set  spark.executor.memory 20,000M spark.yarn.executor.memoryOverhead 10% (2,000M) spark.executor.cores 4   Spark will only be able to allocate 2 executors on each node resulting in 57,344 - 44,000(22,000*2 = 13,344 of unallocated resources and 76.7% memory utilization  However, if spark.executor.memory was right sized to the available total yarn.nodemanager.resource.memory-mb you would get higher instance utilization. For example,  spark.executor.memory 12,000M spark.yarn.executor.memoryOverhead 10% (1,200M) spark.executor.cores 4   Spark will be able to allocate 4 executors on each node resulting in only 57,344 - 52,800(13,200 * 4) = 4,544 of unallocated resources and 92.0% memory utilization  For more information on Spark and YARN right sizing see here  and here  ","version":"Next","tagName":"h2"},{"title":"BP 1.12 Monitor cluster utilization​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-112-monitor-cluster-utilization","content":" Monitoring cluster utilization is important for right sizing your cluster which can help reduces costs. To monitor cluster utilization, you can use EMR cloudwatch metrics, Ganglia (can be installed with EMR) or configure a 3rd party tool like Grafana and Prometheus.  Regardless of which tool you use, you’ll want to monitor cluster metrics such as available vCPU, Memory and disk utilization to determine if you’re right sized for your workload. If some containers are constantly available, shrinking your cluster saves cost without decreasing performance because containers are sitting idle.  For example, If looking at Ganglia shows that either CPU or memory is 100% but the other resources are not being used significantly, then consider moving to another instance type that may provide better performance at a lower cost or reducing the total cluster size. For example,  if CPU is 100%, and memory usage is less than 50% on R4 or M5 series instance types, then moving to C4 series instance type may be able to address the bottleneck on CPU.If both CPU and memory usage is at 50%, reducing cluster capacity in half could give you the same performance at half the cost    These recommendations are more applicable towards job scoped pipelines or transient clusters where the workload pattern is known or constant. If the cluster is long running or the workload pattern is not predictable, using managed scaling should be considered since it will attempt to rightsize the cluster automatically.  For more information on which cloudwatch metrics are available, see here  For more information on the Grafana and Prometheus solution, see here  ","version":"Next","tagName":"h2"},{"title":"BP 1.13 Monitor and decommission idle EMR cluster​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-113-monitor-and-decommission-idle-emr-cluster","content":" Decommission Amazon EMR clusters that are no longer required to lower cost. This can be achieved in two ways. You can use EMR’s automatic termination policy starting 5.30.0 and 6.1.0 or, by monitoring the isIdle metric in CloudWatch and terminating yourself.  With EMR’s automatic termination policy feature, EMR continuously samples key metrics associated with the workloads running on the clusters, and auto-terminates when the cluster is idle. For more information on when a cluster is considered idle and considerations, see here  With EMR’s isIdle CloudWatch metric, EMR will emit 1 if no tasks are running and no jobs are running, and emit 0 otherwise. This value is checked at five-minute intervals and a value of 1 indicates only that the cluster was idle when checked, not that it was idle for the entire five minutes. You can set an alarm to fire when the cluster has been idle for a given period of time, such as thirty minutes. Non-YARN based applications such as Presto, Trino, or HBase are not considered with isIdle metrics.  For a sample solution of this approach, see here  ","version":"Next","tagName":"h2"},{"title":"BP 1.14 Use the latest Amazon EMR version​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-114-use-the-latest-amazon-emr-version","content":" Use the latest EMR version and upgrade whenever possible. New EMR versions have performance improvements, cost savings, bug fixes stability improvements and new features.  For more information, see EMR Release Guide see here  ","version":"Next","tagName":"h2"},{"title":"BP 1.15 Using transient and long-running clusters​","type":1,"pageTitle":"Cost Optimizations","url":"/aws-emr-best-practices/docs/bestpractices/Cost Optimizations/best_practices#bp-115-using-transient-and-long-running-clusters","content":" Amazon EMR supports both transient clusters and long running clusters and both should be considered depending on your use case and job type.  In general, transient clusters are good for job scoped pipelines. Clusters can be right-sized to meet the exact needs of your job. Using transient clusters reduces the blast radius across other jobs and makes it easier to upgrade clusters and restart jobs. Since transient clusters are shutdown after the job is run, you don’t need to worry about idle resources and managing many aspects of cluster life cycle, including replacing failed nodes, upgrades, patching, etc.  In general, long running clusters are good for short-running jobs, ad hoc queries and streaming applications. Long running clusters can also be considered to save costs and operations for multi tenanted data science and engineering jobs.  From a cost optimization standpoint,  If using transient clusters, ensure your instances and containers are right sized so that you are not over provisioned. Use BP 1.12 to determine cluster utilization and if you’re able to lower your requested compute while still meeting your SLA.If using long running clusters, ensure you’re using EMR Managed Scaling to scale up and down resources based off your jobs needs. It is also important to treat the cluster as a transient resources and have the automation in place to decommission and restart clusters. ","version":"Next","tagName":"h2"},{"title":"5.1 - Spark General","type":0,"sectionRef":"#","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices","content":"","keywords":"","version":"Next"},{"title":"BP 5.1.1 - Use the most recent version of EMR​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-511-----use-the-most-recent-version-of-emr","content":" Amazon EMR provides several Spark optimizations out of the box with EMR Spark runtime which is 100% compliant with the open source Spark APIs i.e., EMR Spark does not require you to configure anything or change your application code. We continue to improve the performance of this Spark runtime engine for new releases. Several optimizations such as Adaptive Query Execution are only available from EMR 5.30 and 6.0 versions onwards. For example, following image shows the Spark runtime performance improvements in EMR 6.5.0 (latest version as of writing this) compared to its previous version EMR 6.1.0 based on a derived TPC-DS benchmark test performed on two identical EMR clusters with same hardware and software configurations (except for the version difference).    As seen in the above image, Spark runtime engine on EMR 6.5.0 is 1.9x faster by geometric mean compared to EMR 6.1.0. Hence, it is strongly recommended to migrate or upgrade to the latest available Amazon EMR version to make use of all these performance benefits.  Upgrading to a latest EMR version is typically a daunting task - especially major upgrades (for eg: migrating to Spark 3.1 from Spark 2.4). In order to reduce the upgrade cycles, you can make use of EMR Serverless (in preview) to quickly run your application in an upgraded version without worrying about the underlying infrastructure. For example, you can create an EMR Serverless Spark application for EMR release label 6.5.0 and submit your Spark code.  aws --region us-east-1 emr-serverless create-application \\ --release-label emr-6.5.0-preview \\ --type 'SPARK' \\ --name spark-6.5.0-demo-application   Detailed documentation for running Spark jobs using EMR Serverless can be found here. Since EMR Serverless and EMR on EC2 will use the same Spark runtime engine for a given EMR release label, once your application runs successfully in EMR Serverless, you can easily port your application code to the same release version on EMR. Please note that this approach does not factor in variables due to infrastructure or deployment into consideration and is only meant to validate your application code quickly on an upgraded Spark version in the latest Amazon EMR release available.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.2 - Determine right infrastructure for your Spark workloads​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-512-----determine-right-infrastructure-for-your-spark-workloads","content":" Spark workloads may require different types of hardware for different job characteristics to ensure optimal performance. EMR supports several instance types to cover all types of processing requirements. While onboarding new workloads, it is recommended to start benchmarking with general instance types like m5s or m6gs. Monitor the OS and YARN metrics from Ganglia and CloudWatch to determine the system bottlenecks at peak load. Bottlenecks include CPU, memory, storage and I/O. Once identified, choose the appropriate hardware type for your job requirements.  ","version":"Next","tagName":"h2"},{"title":"Memory-optimized​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#memory-optimized","content":" Memory-optimized instances like r5 and r4 are good candidates for memory intensive workloads. Spark jobs that cache large DataFrames, Datasets or RDDs, perform operations like joins and unions on large tables, use many internal or user-defined broadcast variables or accumulators, go through many GC cycles and perform massive shuffles are likely to be memory intensive. Following diagram shows YARN memory available percentage and aggregated OS memory utilization from Cloudwatch EMR namespace and Ganglia respectively.    ","version":"Next","tagName":"h3"},{"title":"CPU-optimized​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#cpu-optimized","content":" CPU-optimized instances like c5 and c4 are good candidates for CPU intensive workloads. Spark jobs with complex aggregate operations involving many in-built arithmetic functions or UDFs and jobs that use a lot of LRU caches are likely to be CPU intensive. Following screenshot shows aggregated CPU utilization of the EMR cluster from Ganglia.    ","version":"Next","tagName":"h3"},{"title":"General purpose​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#general-purpose","content":" General purpose instances like m5 and m4 are good candidates for a mix of CPU and memory intensive workloads. They are also great for benchmarking and onboarding your Spark applications. Following sheet outlines the CPU:Memory ratio of 3 different instances types at a similar price. It is important to use instance types with right CPU:memory ratio based on your workload needs.  Instance Type\tInstance\tEC2 price\tEMR price\tCores\tMemory in GiB\tCPU:memory ratioCompute\tc5.18xlarge\t$3.06\t$0.27\t72\t144\t2 Memory\tr5.12xlarge\t$3.02\t$0.27\t48\t384\t8 General\tm5.16xlarge\t$3.07\t$0.27\t64\t256\t4  ","version":"Next","tagName":"h3"},{"title":"Storage-optimized​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#storage-optimized","content":" Storage-optimized instances like i3ens, d2 are good candidates for I/O intensive workloads. If your use case is CPU/memory bound but also consumes a lot of I/O, and demands high disk throughput or low read or write latencies from transient HDFS storage, you can consider using instances backed by SSD volumes like r5ds, c5ds, m5ds etc.. Spark jobs that perform massive shuffles may also benefit from instance types with optimized storage since Spark external shuffle service will write the shuffle data blocks to the local disks of worker nodes running the executors.  ","version":"Next","tagName":"h3"},{"title":"GPU instances​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#gpu-instances","content":" GPU instances such as p3 family are typically used for Spark ML and intensive analytical workloads like image processing. From EMR 6.2, you can make use of Nvidia RAPIDS accelerator plugin to improve your GPU instance performance without any changes to your code or data processing pipelines.  ","version":"Next","tagName":"h3"},{"title":"Graviton instances​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#graviton-instances","content":" Starting EMR 5.31+ and 6.1+, EMR supports Graviton instance (eg: r6g, m6g, c6g) which offers up to 15% improvement in performance and 30% reduction in cost based on our derived benchmark tests. They are a great choice to replace your legacy instances and achieve better price-performance.    ","version":"Next","tagName":"h3"},{"title":"BP 5.1.3 - Choose the right deploy mode​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-513-----choose-the-right-deploy-mode","content":" Spark offers two kinds of deploy modes called client and cluster deploy modes. Spark deploy mode determines where your application's Spark driver runs. Spark driver is the cockpit for your Spark application. It hosts the SparkContext (or SparkSession) for your application. It keeps track of the statuses of all the tasks and executors via heartbeats. Spark driver also fetches the results from the executors running tasks. Choose the right deploy mode based on your workload requirements.  ","version":"Next","tagName":"h2"},{"title":"Client deploy mode​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#client-deploy-mode","content":" This is the default deploy mode for Spark applications in EMR. In this deploy mode, the driver process will be launched within your Spark client - whether you submit your application from EMR master node (using EMR Step API or spark-submit) or using a remote client. In this case, Spark driver will be the single point of failure. A failed Spark driver process will not be retried in client deploy mode. Also, when client loses connectivity with YARN, your job will be killed.    Use this deploy mode if:  You are running only one or two Spark applications at a time in a cluster. This deploy mode is not ideal if you are running multiple applications at the same time on the same cluster since all those Spark drivers running on a single master/remote node can lead to resource contention.You want to be more in control of your Spark driver configurations. In client mode, Spark driver resources will not compete with YARN resources and can be adjusted separately without affecting the YARN resource procurement of your applications.You are running too many executors (1000+) or tasks (30000+) in a single application. Since Spark driver manages and monitors all the tasks and executors of an application, too many executors/tasks may slow down the Spark driver significantly while polling for statuses. Since EMR allows you to specify a different instance type for master node, you can choose a very powerful instance like z1d and reserve a large amount of memory and CPU resources for the Spark driver process managing too many executors and tasks from an application.You want to write output to the console i.e., send the results back to the client program where you submitted your application.Notebook applications such as Jupyter, Zeppelin etc. will use client deploy mode.  ","version":"Next","tagName":"h3"},{"title":"Cluster deploy mode​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#cluster-deploy-mode","content":" In cluster deploy mode, your Spark driver will be located within the Application Master (AM) container from YARN regardless of where you submit your Spark application from.    Use cluster deploy mode if:  You are submitting multiple applications at the same time or have higher application or EMR step concurrency. While running multiple applications, Spark drivers will be spread across the cluster since AM container from a single application will be launched on one of the worker nodes.There are relatively fewer number of executors per application. i.e., the Spark driver process does not have to do intensive operations like manage and monitor tasks from too many executors.You are saving results in S3/HDFS and there is no need to print output to the console.You want to specify detailed instructions on where AM runs. You can launch AMs on CORE partition or both CORE and TASK partitions based on where you want your AM and executors to launch. For example, you can run AM on only on-demand CORE nodes and executors only on spot task nodes.You want to relaunch a failed driver JVM i.e., increased resiliency. By default, YARN re-attempts AM loss twice based on property spark.yarn.maxAppAttempts. You can increase this value further if needed.You want to ensure that termination of your Spark client will not lead to termination of your application. You can also have Spark client return success status right after the job submission if the property spark.yarn.submit.waitAppCompletion is set to &quot;false&quot;.  Regardless of which deploy mode you choose, make sure that your driver Spark configs are tuned for your workload needs.  ","version":"Next","tagName":"h3"},{"title":"BP 5.1.4 - Use right file formats and compression type​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-514-----use-right-file-formats-and-compression-type","content":" Right file formats must be used for optimal performance. Avoid legacy file formats like CSV, JSON, text files etc. since the read/write performance will much slower. It is highly recommended that you use columnar file formats like Parquet and ORC. For Spark, Parquet file format would be the best choice considering performance benefits and wider community support.  When writing Parquet files to S3, EMR Spark will use EMRFSOutputCommitter which is an optimized file committer that is more performant and resilient than FileOutputCommitter. Using Parquet file format is great for schema evolution, filter push downs and integration with applications offering transactional support like Apache Hudi, Apache Iceberg etc.  Also, it is recommended to use an optimal compression format. Avoid using unsplittable compression formats like GZIP. Since they are not splittable, when a Spark task processes a large GZIP compressed file, it will lead to executor OOM errors.    Use splittable compression formats like BZ2, LZO etc. Parquet uses Snappy compression by default. ORC uses ZLIB compression by default. Both compression types are good choices and you can continue to use the defaults.    You can also apply columnar encryption on Parquet files using KMS:  sc.hadoopConfiguration.set(&quot;parquet.encryption.kms.client.class&quot; ,&quot;org.apache.parquet.crypto.keytools.mocks.InMemoryKMS&quot;) // Explicit master keys (base64 encoded) - required only for mock InMemoryKMS sc.hadoopConfiguration.set(&quot;parquet.encryption.key.list&quot; ,&quot;keyA:AAECAwQFBgcICQoLDA0ODw , keyB:AAECAAECAAECAAECAAECAA&quot;) // Activate Parquet encryption, driven by Hadoop properties sc.hadoopConfiguration.set(&quot;parquet.crypto.factory.class&quot; ,&quot;org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory&quot;) // Write encrypted dataframe files. // Column &quot;square&quot; will be protected with master key &quot;keyA&quot;. // Parquet file footers will be protected with master key &quot;keyB&quot; squaresDF.write.option(&quot;parquet.encryption.column.keys&quot; , &quot;keyA:square&quot;).option(&quot;parquet.encryption.footer.key&quot; , &quot;keyB&quot;).parquet(&quot;/path/to/table.parquet.encrypted&quot;) // Read encrypted dataframe files val df2 = spark.read.parquet(&quot;/path/to/table.parquet.encrypted&quot;)   ","version":"Next","tagName":"h2"},{"title":"BP 5.1.5 - Partitioning​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-515-----partitioning","content":" Partitioning your data or tables is very important if you are going to run your code or queries with filter conditions. Partitioning helps arrange your data files into different S3 prefixes or HDFS folders based on the partition key. It helps minimize read/write access footprint i.e., you will be able to read files only from the partition folder specified in your where clause - thus avoiding a costly full table scan. Partitioning can also help if your data ingestion is incremental in nature. However, partitioning can reduce read throughput when you perform full table scans.  You can choose one or more partition fields from your dataset or table columns based on:  Query pattern. i.e., if you find queries use one or more columns frequently in the filter conditions more so than other columns, it is recommended to consider leveraging them as partitioning field.Ingestion pattern. i.e., if you are loading data into your table based on a fixed schedule (eg: once everyday) and you want to avoid re-writing historical data, you can partition your data based on date field (typically in YYYY-MM-DD format or YYYY/MM/DD nested partitions).Cardinality of the partitioning column. For partitioning, cardinality should not be too high. For example, fields like employee_id or uuid should not be chosen as partition fields.File sizes per partition. It is recommended that your individual file sizes within each partition are &gt;=128 MB.  The number of shuffle partitions will determine the number of output files per table partition.  df.repartition(400).write.partitionBy(&quot;datecol&quot;).parquet(&quot;s3://bucket/output/&quot;)   The above code will create maximum of 400 files per datecol partition. Repartition API alters the number of shuffle partitions dynamically. PartitionBy API specifies the partition column(s) of the table. You can also control the number of shuffle partitions with the Spark property spark.sql.shuffle.partitions. You can use repartition API to control the output file size i.e., for merging small files. For splitting large files, you can use the property spark.sql.files.maxPartitionBytes.  Partitioning ensures that dynamic partition pruning takes place during reads and writes. Pruning makes sure that only necessary partition(s) are read from S3 or HDFS. Spark optimized logical plan or DAG can be studied to ensure that the partition filters are pushed down while reading and writing to partitioned tables from Spark.  For example, following query will push partition filters for better performance. l_shipdate and l_shipmode are partition fields of the table &quot;testdb.lineitem_shipmodesuppkey_part&quot;.  val df = spark.sql(&quot;select count(*) from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'&quot;) df.queryExecution.toString   Printing the query execution plan where we can see pushed filters for the two partition fields in where clause:  == Physical Plan == AdaptiveSparkPlan isFinalPlan=true +- == Final Plan == *(2) HashAggregate(keys=[], functions=[count(1)], output=[count(1)#320]) +- ShuffleQueryStage 0 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#198] +- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#318L]) +- *(1) Project +- *(1) ColumnarToRow +- FileScan orc testdb.lineitem_shipmodesuppkey_part[l_shipdate#313,l_shipmode#314] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[s3://vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt; +- == Initial Plan == HashAggregate(keys=[], functions=[count(1)], output=[count(1)#320]) +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#179] +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#318L]) +- Project +- FileScan orc testdb.lineitem_shipmodesuppkey_part[l_shipdate#313,l_shipmode#314] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[s3://vasveena-test-vanguard/bigtpcparq/lineitem_shipmodesuppkey_part/l_shipdate..., PartitionFilters: [isnotnull(l_shipdate#313), isnotnull(l_shipmode#314), (l_shipdate#313 = 1993-12-03), (l_shipmode..., PushedFilters: [], ReadSchema: struct&lt;&gt;   ","version":"Next","tagName":"h2"},{"title":"BP 5.1.6 - Tune driver/executor memory, cores and spark.sql.shuffle.partitions to fully utilize cluster resources​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-516----tune-driverexecutor-memory-cores-and-sparksqlshufflepartitions-to-fully-utilize-cluster-resources","content":" Amazon EMR configures Spark defaults during the cluster launch based on your cluster's infrastructure (number of instances and instance types). EMR configured defaults are generally sufficient for majority of the workloads. However, if it is not meeting your performance expectations, it is recommended to tune the Spark driver/executor configurations and see if you can achieve better performance. Following are the general recommendations on driver/executor configuration tuning.  For a starting point, generally, it is advisable to set spark.executor.cores to 4 or 5 and tune spark.executor.memory around this value. Also, when you calculate the spark.executor.memory, you need to account for the executor overhead which is set to 0.1875 by default (i.e., 18.75% of spark.executor.memory). For example, for a 2 worker node r4.8xlarge cluster, following will be the configurations.  Based on Task Configurations r4.8xlarge node has YARN memory of 241664 MB (based on the value of yarn.nodemanager.resource.memory-mb). The instance has 32 vCores. If we set spark.executor.cores as 4, we can run 8 executors at a time. So, the configurations will be following.  spark.executor.cores = 4 spark.executor.memory + (spark.executor.memory * spark.yarn.executor.memoryOverheadFactor) = (241664 MB / 8) = 30208 MB spark.executor.memory = 24544 MB (substituting default spark.yarn.executor.memoryOverheadFactor = 0.1875)   If you have a cluster of 10 r4.8xlarge nodes, then totally, 80 executors can run with 24544 MB memory and 4 vCores each.  Please note that some of the jobs benefit from bigger executor JVMs (with more cores assigned). Some jobs benefit from smaller but more number of executors. So, you can use the above formula to arrive at optimal values for your application. EMR Spark has a feature called maximizeResourceAllocation. Setting this property to &quot;true&quot; will lead to one fat executor JVM per node that uses all of the available cores in that instance. However, please note that this setting may not prove to be optimal for many different types of workloads. It is not recommended to enable this property if your cluster is a shared cluster with multiple parallel applications or if your cluster has HBase installed.  After configuring the values, run a sample job and monitor the Resource Manager UI, ContainerPendingRatio and YARNMemoryAvailablePcnt Cloudwatch metrics to verify that the vCores and YARN memory are being fully utilized. Spark JMX metrics provides JMX level visibility which is the best way to determine resource utilization.  While using instance fleets, it is generally advisable to request worker nodes with similar vCore:memory ratio (for eg: requesting r4, r5 and r6gs in the same fleet). However, in some cases, in order to ensure capacity, you may have to diversify the instance type families as well in your request (for eg: requesting c5s, m5s and r5s within the same fleet). EMR will configure driver/executor configurations based on minimum of (master, core, task) OS resources. Generally, with variable fleets, it is best to use the default configurations. However, if needed, you can fine tune the driver and executor configurations based on above principles. But in this case, you will need to take YARN memory and vCores of all the different instance families into consideration.  To provide an example, lets say you have requested a cluster with a core fleet containing following instances: r5.4xlarge, r5.12xlarge, c5.4xlarge, c5.12xlarge, m5.4xlarge, m5.12xlarge. All the 4xlarge instances in this fleet have 16 vCores and 12xlarge instances have 48 vCores. But the OS/YARN memory of these instances are different.  Instance\tYARN memory in MBc5.4xlarge\t24576 c5.12xlarge\t90112 m5.4xlarge\t57344 m5.12xlarge\t188416 r5.4xlarge\t122880 r5.12xlarge\t385024  Now, let us calculate executor memory after setting spark.executor.cores = 4 by starting with smallest YARN memory from the above table (c5.4xlarge) and dividing the YARN memory by spark.executor.cores to get the total container size -&gt; 24576 / 4 = 6144.  spark.executor.memory = 6144 - (6144 * 0.1875) = 4992 MB  Using default Spark implementation, with the above math, if you set 4992 MB as executor memory, then in r5.12xlarge instances in your fleet, the resources will be under utilized even though you will not see the evidence of it from the Resource Manager UI. With the above configs, 77 executors can run in r5.12xlarge but there are only 48 vCores. So, even though 77 executors will have YARN resources allocated, they are only able to run 48 tasks at any given time which could be considered a wastage of memory resources.  In order to alleviate this issue, from EMR 5.32 and EMR 6.2, there is a feature called Heterogenous Executors which dynamically calculates executor sizes. It is defined by the property spark.yarn.heterogeneousExecutors.enabled and is set to &quot;true&quot; by default. Further, you will be able to control the maximum resources allocated to each executor with properties spark.executor.maxMemory and spark.executor.maxCores. Minimum resources are calculated with spark.executor.cores and spark.executor.memory. For uniform instance groups or for flexible fleets with instance types having similar vCore:memory ratio, you can try setting spark.yarn.heterogeneousExecutors.enabled to &quot;false&quot; and see if you get better performance.  Similar to executors, driver memory and vCores can be calculated as well. The default memory overhead for driver container is 10% of driver memory. If you are using cluster deploy mode, then the driver resources will be allocated from one of the worker nodes. So, based on the driver memory/core configurations, it will take away some of the YARN resources that could be used for launching executors - which shouldn't matter that much if your cluster is not very small. If you are using client deploy mode and submitting jobs from EMR master node or a remote server, then the driver resources are taken from the master node or remote server and your driver will not compete for YARN resources used by executor JVMs. The default driver memory (without maximizeResourceAllocation) is 2 GB. You can increase driver memory or cores for the following conditions:  Your cluster size is very large and there are many executors (1000+) that need to send heartbeats to driver.Your result size retrieved during Spark actions such as collect() or take() is very large. For this, you will also need to tune spark.driver.maxResultSize.  You can use smaller driver memory (or use the default spark.driver.memory) if you are running multiple jobs in parallel.  Now, coming to spark.sql.shuffle.partitions for Dataframes and Datasets and spark.default.parallelism for RDDs, it is recommended to set this value to total number of vCores in your cluster or a multiple of that value. For example, a 10 core node r4.8xlarge cluster can accommodate 320 vCores in total. Hence, you can set shuffle partitions or parallelism to 320 or a multiple of 320 such that each vCore handles a single Spark partition at any given time. It is not recommended to set this value too high or too low. Generally 1 or 2x the total number of vCores is optimal. Generally, each Spark shuffle partition should process ~128 MB of data. This can be determined by looking at the execution plan from the Spark UI.    From the above image, you can see that the average size in exchange (shuffle) is 2.2 KB which means we can try to reduce spark.sql.shuffle.partitions to increase partition size during the exchange.  Apart from this, if you want to use tools to receive tuning suggestions, consider using Sparklens and Dr. Elephant with Amazon EMR which will provide tuning suggestions based on metrics collected during the runtime of your application.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.7 - Use Kryo serializer by registering custom classes especially for Dataset schemas​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-517----use-kryo-serializer-by-registering-custom-classes-especially-for-dataset-schemas","content":" Spark uses Java Serializer by default. From Spark 2.0+, Spark internally uses Kryo Serializer when shuffling RDDs with simple types, arrays of simple types, or string type. It is highly recommended that you use Kryo Serializer and also register your classes in the application.  val spark = SparkSession .builder .appName(&quot;my spark application name&quot;) .config(getConfig) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) // use this if you need to increment Kryo buffer size. Default 64k .config(&quot;spark.kryoserializer.buffer&quot;, &quot;1024k&quot;) // use this if you need to increment Kryo buffer max size. Default 64m .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;1024m&quot;) /* * Use this if you need to register all Kryo required classes. * If it is false, you do not need register any class for Kryo, but it will increase your data size when the data is serializing. */ .config(&quot;spark.kryo.registrationRequired&quot;, &quot;true&quot;) .getOrCreate   If you do not specify classesToRegister, then there will be a Kryo conversion overhead which could impact performance. Hence, it is recommended to register Kryo classes in your application. Especially, if you are using Datasets, consider registering your Dataset schema classes along with some classes used by Spark internally based on the data types and structures used in your program. An example provided below:  val conf = new SparkConf() conf.registerKryoClasses( Array( classOf[org.myPackage.FlightDataset], classOf[org.myPackage.BookingDataset], classOf[scala.collection.mutable.WrappedArray.ofRef[_]], classOf[org.apache.spark.sql.types.StructType], classOf[Array[org.apache.spark.sql.types.StructType]], classOf[org.apache.spark.sql.types.StructField], classOf[Array[org.apache.spark.sql.types.StructField]], Class.forName(&quot;org.apache.spark.sql.types.StringType$&quot;), Class.forName(&quot;org.apache.spark.sql.types.LongType$&quot;), Class.forName(&quot;org.apache.spark.sql.types.BooleanType$&quot;), Class.forName(&quot;org.apache.spark.sql.types.DoubleType$&quot;), classOf[org.apache.spark.sql.types.Metadata], classOf[org.apache.spark.sql.types.ArrayType], Class.forName(&quot;org.apache.spark.sql.execution.joins.UnsafeHashedRelation&quot;), classOf[org.apache.spark.sql.catalyst.InternalRow], classOf[Array[org.apache.spark.sql.catalyst.InternalRow]], classOf[org.apache.spark.sql.catalyst.expressions.UnsafeRow], Class.forName(&quot;org.apache.spark.sql.execution.joins.LongHashedRelation&quot;), Class.forName(&quot;org.apache.spark.sql.execution.joins.LongToUnsafeRowMap&quot;), classOf[org.apache.spark.util.collection.BitSet], classOf[org.apache.spark.sql.types.DataType], classOf[Array[org.apache.spark.sql.types.DataType]], Class.forName(&quot;org.apache.spark.sql.types.NullType$&quot;), Class.forName(&quot;org.apache.spark.sql.types.IntegerType$&quot;), Class.forName(&quot;org.apache.spark.sql.types.TimestampType$&quot;), Class.forName(&quot;org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult&quot;), Class.forName(&quot;org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage&quot;), Class.forName(&quot;scala.collection.immutable.Set$EmptySet$&quot;), Class.forName(&quot;scala.reflect.ClassTag$$anon$1&quot;), Class.forName(&quot;java.lang.Class&quot;) ) ) }   You can also optionally fine tune the following Kryo configs :-  spark.kryo.unsafe - Set to false for faster serialization. This is not unsafer for same platforms but should not be used if your EMR cluster fleets use a mix of different processors (for eg: AMD, Graviton and Intel types within the same fleet).spark.kryoserializer.buffer.max - Maximum size of Kryo buffer. Default is 64m. Recommended to increase this property upto 1024m but the value should be below 2048m.spark.kryoserializer.buffer - Initial size of Kryo serialization buffer. Default is 64k. Recommended to increase up to 1024k.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.8 - Tune Garbage Collector​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-518------tune-garbage-collector","content":" By default, EMR Spark uses Parallel Garbage Collector which works well in most cases. You can change the GC to G1GC if your GC cycles are slow since G1GC may provide better performance in some cases specifically by reducing GC pause times. Also, since G1GC is the default garbage collector since Java 9, you may want to switch to G1GC for forward compatibility.  Following is the spark configuration:  [{ &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.executor.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'&quot;, &quot;spark.driver.extraJavaOptions&quot;: &quot;-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'&quot; }, &quot;configurations&quot;: [] }]   You can also tune the GC parameters for better GC performance. You can see the comprehensive list of parameters here for G1GC and here for ParallelGC. Some useful ones are below:  -XX:ConcGCThreads=n -XX:ParallelGCThreads=n -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxGCPauseMillis=200   You can monitor GC performance using Spark UI. The GC time should be ideally &lt;= 1% of total task runtime. If not, consider tuning the GC settings or experiment with larger executor sizes. For example, we see below in the Spark UI that GC takes almost 25% of task runtime which is indicative of poor GC performance.    ","version":"Next","tagName":"h2"},{"title":"BP 5.1.9 - Use optimal APIs wherever possible​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-519------use-optimal-apis-wherever-possible","content":" When using Spark APIs, try to use the optimal ones if your use case permits. Following are a few examples.  ","version":"Next","tagName":"h2"},{"title":"repartition vs coalesce​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#repartition-vs-coalesce","content":" Both repartition and coalesce are used for changing the number of shuffle partitions dynamically. Repartition is used for both increasing and decreasing the shuffle partitions whereas coalesce is used for only decreasing the number of shuffle partitions. If your goal is to decrease the number of shuffle partitions, consider using coalesce instead of repartition. Repartition triggers a full shuffle but coalesce triggers only a partial shuffle and thus minimizes the amount of data shuffled by keeping a few nodes as solely receivers of the shuffle data.  df.coalesce(1) //instead of df.repartition(1)   But please note that when you coalesce (or repartition) to a very small number, your JVM will process a lot of data which can lead to OOM issues or disk space issues due to shuffle spill.  ","version":"Next","tagName":"h3"},{"title":"groupByKey vs reduceByKey​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#groupbykey-vs-reducebykey","content":" Use reduceByKey instead of groupByKey wherever possible. With groupByKey, data will be transferred over the network and collected on the reduced workers. This can lead to OOMs since all data is sent across the network. Whereas, with reduceByKey, data is combined at partition-level, with only one output for one key at each partition to send over the network.  ","version":"Next","tagName":"h3"},{"title":"orderBy vs sortBy or sortWithinPartitions​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#orderby-vs-sortby-or-sortwithinpartitions","content":" orderBy performs global sorting. i.e., all the data is sorted using a single JVM. Whereas, sortBy or sortWithinPartitions performs local sorting i.e., data is sorted within each partition but it does not preserve global ordering. Use sortBy or sortWithinPartitions if global ordering is not necessary. Try to avoid orderBy clause especially during writes.  ","version":"Next","tagName":"h3"},{"title":"BP 5.1.10 - Leverage spot nodes with managed autoscaling​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5110-----leverage-spot-nodes-with-managed-autoscaling","content":" Enable managed autoscaling for your EMR clusters. From EMR 5.32 and EMR 6.2, several optimizations have been made to managed scaling to make it more resilient for your Spark workloads. It is not recommended to use Spot with core or master nodes since during a Spot reclaimation event, your cluster could be terminated and you would need to re-process all the work. Try to leverage task instance fleets with many instance types per fleet along with Spot since it would give both cost and performance gains. However, in this case, make sure that your output is being written directly to S3 using EMRFS since we will aim to have limited/fixed core node capacity.  Following policy defines max core nodes to 2 and we are requesting the core nodes to be on-demand as recommended. Rest of the nodes are Spot task nodes.    Following experimentation illustrates the performance gains using Managed Autoscaling.    For some of our Spark workloads, we observed ~50% gains compared to custom autoscaling clusters. Please note that the results may vary for your workloads.    If your workloads are SLA sensitive and fault intolerant, it is best to use on-demand nodes for task fleets as well since reclaimation of Spot may lead to re-computation of one or more stages or tasks.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.11 - For workloads with predictable pattern, consider disabling dynamic allocation​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5111------for-workloads-with-predictable-pattern-consider-disabling-dynamic-allocation","content":" Dynamic allocation is enabled in EMR by default. It is a great feature for following cases:  Workloads processing variable amount of dataWhen your cluster uses autoscalingDynamic processing requirements or unpredictable workload patternsStreaming and ad-hoc workloadsWhen your cluster runs multiple concurrent applicationsYour cluster is long-running  The above cases would cover at least 95% of the workloads run by our customers today. However, there are a very few cases where:  Workloads have a very predicatable patternAmount of data processed is predictable and consistent throughout the applicationCluster runs Spark application in batch modeClusters are transient and are of fixed size (no autoscaling)Application processing is relatively uniform. Workload is not spikey in nature.  For example, you may have a use case where you are collecting weather information of certain geo regions twice a day. In this case, your data load will be predictable and you may run two batch jobs per day - one at BOD and one at EOD. Also, you may use two transient EMR clusters to process these two jobs.  For such use cases, you can consider disabling dynamic allocation along with setting the precise number and size of executors and cores like below.  [{ &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.dynamicAllocation.enabled&quot;: &quot;false&quot;, &quot;spark.executor.instances&quot;: &quot;12&quot;, &quot;spark.executor.memory&quot;: &quot;8G&quot;, &quot;spark.executor.cores&quot;: &quot;4&quot; }, &quot;configurations&quot;: [] }]   Please note that if you are running more than one application at a time, you may need to tweak the Spark executor configurations to allocate resources to them. By disabling dynamic allocation, Spark driver or YARN Application Master does not have to calculate resource requirements at runtime or collect certain heuristics. This may save anywhere from 5-10% of job execution time. However, you will need to carefully plan Spark executor configurations in order to ensure that your entire cluster is being utilized. If you choose to do this, then it is better to disable autoscaling since your cluster only runs a fixed number of executors at any given time unless your cluster runs other applications as well.  However, only consider this option if your workloads meet the above criteria since otherwise your jobs may fail due to lack of resources or you may end up wasting your cluster resources.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.12 - Leverage HDFS as temporary storage for I/O intensive workloads​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5112------leverage-hdfs-as-temporary-storage-for-io-intensive-workloads","content":" Many EMR users directly read and write data to S3. This is generally suited for most type of use cases. However, for I/O intensive and SLA sensitive workflows, this approach may prove to be slow - especially during heavy writes.    For I/O intensive workloads or for workloads where the intermediate data from transformations is much larger than the final output, you can leverage HDFS as temporary storage and then use S3DistCp to copy the data into final location once your application is finished. For example, for a fraud detection use case, you could be performing transforms on TBs of data but your final output report may only be a few KBs. In such cases, leveraging HDFS will give you better performance and will also help you avoid S3 throttling errors.    Following is an example where we leverage HDFS for intermediate results. A Spark context could be shared between multiple workflows, wherein, each workflow comprises of multiple transformations. After all transformations are complete, each workflow would write the output to an sHDFS location. Once all workflows are complete, you can save the final output to S3 either using S3DistCp or simple S3 boto3 client determined by the number of files and the output size.    However, while using this architecture, please make sure that you are sizing your HDFS properly to prevent job failures due to lack of storage space when the job is running. Refer to best practice BP 2.13 in Reliability section. Also, checkpoint your data frequently to S3 using S3DistCp or boto to prevent data loss due to unexpected cluster terminations.  Even if you are using S3 directly to store your data, if your workloads are shuffle intensive, use storage optimized instances or SSD/NVMe based storage (for example: r5d’s and r6gd’s instead of r5s and r6g’s). This is because when dynamic allocation is turned on, Spark will use external shuffle service that spills data to local disks when the executor JVM cannot hold any more shuffle data. This process is a very I/O intensive one and will benefit from instance types that offer high disk throughput.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.13 - Spark speculation with EMRFS​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5113------spark-speculation-with-emrfs","content":" In Hadoop/Spark, speculative execution is a concept where a slower task will be launched in parallel on another node using a different JVM (based on resource availability). Whichever task completes first (original or speculated task), will write the output to S3. This works well for HDFS based writes. However, for EMRFS, turning on spark.speculation may lead to serious issues such as data loss or duplicate data. By default, spark.speculation is turned off. Only enable spark.speculation if you are doing one of the following.  Writing Parquet files to S3 using EMRFSOutputCommitterUsing HDFS as temporary storage in an understanding that final output will be written to S3 using S3DistCpUsing HDFS as storage (not recommended)  Do not enable spark.speculation if none of the above criteria is met since it will lead to incorrect or missing or duplicate data.  You can consider enabling spark.speculation especially while running workloads on very large clusters, provided you are performing one of the above actions. This is because, due to some hardware or software issues, one node out of 500+ nodes could be slower and may run tasks slowly even if data size being processed is the same as other tasks. Chances of this happening are higher in larger clusters. In that case, spark.speculation will help relaunch those slow tasks on other nodes providing SLA consistency (as long as the above criteria are met).  You can set spark.speculation to true in spark-defaults or pass it as a command line option (--conf spark.speculation=&quot;true&quot;).  [{ &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.speculation&quot;: &quot;true&quot; }, &quot;configurations&quot;: [] }]   Please do not enable spark.speculation if you are writing any non-Parquet files to S3 or if you are writing Parquet files to S3 without the default EMRFSOutputCommitter.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.14 - Data quality and integrity checks with deequ​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5114-----data-quality-and-integrity-checks-with-deequ","content":" Spark and Hadoop frameworks do not inherently guarantee data integrity. While it is very rare, you may observe some data corruption or missing data or duplicate data due to unexpected errors in the hardware and software stack. It is highly recommended that you validate the integrity and quality of your data atleast once after your job execution. It would be best to check for data correctness in multiple stages of your job - especially if your job is long-running.  In order to check data integrity, consider using Deequ for your Spark workloads. Following are some blogs that can help you get started with Deequ for Spark workloads.  Test data quality at scale with Deequ | AWS Big Data Blog  Testing data quality at scale with PyDeequ | AWS Big Data Blog  Sometimes, you may have to write your own validation logic. For example, if you are doing a lot of calculations or aggregations, you will need to compute twice and compare the two results for accuracy. In other cases, you may also implement checksum on data computed and compare it with the checksum on data written to disk or S3. If you see unexpected results, then check your Spark UI and see if you are getting too many task failures from a single node by sorting the Task list based on &quot;Status&quot; and check for error message of failed tasks. If you are seeing too many random unexpected errors such as &quot;ArrayIndexOutOfBounds&quot; or checksum errors from a single node, then it may be possible that the node is impaired. Exclude or terminate this node and re-start your job.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.15 - Use DataFrames wherever possible​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5115-----use-dataframes-wherever-possible","content":" WKT we must use Dataframes and Datasets instead of RDDs since Dataframes and Datasets have several enhancements over RDDs like catalyst optimizer and adaptive query execution. But between Datasets and Dataframes, Dataframes perform certain optimizations during DAG creation and execution. These optimizations can be identified by inspecting the query plan. For example -  Datasets perform many serializations and deserializations that Dataframes tries to skip.Dataframes perform more push downs when compared to Datasets. For example, if there is a filter operation, it is applied early on in the query plan in Dataframes so that the data transfer in-memory is reduced.Dataframes avoid unnecessary exchanges. For example, distinct after join will be accomplished with two exchanges in Datasets but with only one exchange in Dataframes.  Only downside to using dataframes instead of datasets is that, with dataset, you generally define schema in a class.  case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long )   This provides you type-safety. When there are changes to your schema, it can be consolidated and tracked within a single class. This can be considered as the industry standard. While using Spark Dataframes, you can achieve something similar by maintaining the table columns in a list and fetching from that list dynamically from your code. But this requires some additional coding effort.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.16 - Data Skew​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5116------data-skew","content":" Data skew can significantly slow down the processing since a single JVM could be handling a large amount of data. In this case, as observed in Spark UI, a single task is processing 25 times more data than other tasks. This can inevitably lead to slowness, OOMs and disk space filling issues.    When there is a data skew, it is best handled at code level since very little can be done in terms of configuration. You can increase JVM size or use one fat executor per node in order to prevent OOMs to the best of ability. But this will impact other running tasks and also will not improve your job performance since one task uses only one vCPU. Following are some of the common strategies to mitigate data skew at code level.  ","version":"Next","tagName":"h2"},{"title":"Salting​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#salting","content":" Salting is one of the most common skew mitigation techniques where you add a &quot;salt&quot; to the skewed column say &quot;col1&quot;. You can split it into multiple columns like &quot;col1_0&quot;,&quot;col1_1&quot;,&quot;col1_2&quot; and so on. As number of salts increase, the skew decreases i.e., more parallelism of tasks can be achieved.  Original data    Salted 4 times    Salted 8 times    A typical Salting workflow looks like below:    For example, a salt column is added to the data with 100 randomized salts during narrow transformation phase (map or flatMap type of transforms).  n = 100 salted_df = df.withColumn(&quot;salt&quot;, (rand * n).cast(IntegerType))   Now, aggregation is performed on this salt column and the results are reduced by keys  unsalted_df = salted_df.groupBy(&quot;salt&quot;, groupByFields).agg(aggregateFields).groupBy(groupByFields).agg(aggregateFields)   Similar logic can be applied for windowing functions as well.  A downside to this approach is that it creates too many small tasks for non-skewed keys which may have a negative impact on the overall job performance.  ","version":"Next","tagName":"h3"},{"title":"Isolated Salting​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#isolated-salting","content":" In this approach salting is applied to only subset of the keys. If 80% or more data has a single value, isolated salting approach could be considered (for eg: skew due to NULL columns). In narrow transformation phase, we will isolate the skewed column. In the wide transformation phase, we will isolate and reduce the heavily skewed column after salting. Finally, we will reduce other values without the salt and merge the results.  Isolated Salting workflow looks like below:    Example code looks like below:  val count = 4 val salted = df.withColumn(&quot;salt&quot;, when('col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0) val replicaDF = skewDF .withColumn(&quot;replica&quot;, when('col === &quot;A&quot;, (0 until count) toArray) otherwise Array(0)) .withColumn(&quot;salt&quot;, explode('replica')) .drop('replica') val merged = salted.join(replicaDF, joinColumns :+ &quot;salt&quot;)   ","version":"Next","tagName":"h3"},{"title":"Isolated broadcast join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#isolated-broadcast-join","content":" In this approach, smaller lookup table is broadcasted across the workers and joined in map phase itself. Thus, reducing the amount of data shuffles. Similar to last approach, skewed keys are separated from normal keys. Then, we reduce the ”normal” keys and perform map-side join on isolated ”skewed” keys. Finally, we can merge the results of skewed and normal joins  Isolated map-side join workflow looks like below:    Example code looks like below:  val count = 8 val salted = skewDF.withColumn(&quot;salt&quot;, when('col === &quot;A&quot;, rand(1) * count cast IntegerType) otherwise 0).repartition('col', 'salt') // Re-partition to remove skew val broadcastDF = salted.join(broadcast(sourceDF), &quot;symbol&quot;)   ","version":"Next","tagName":"h3"},{"title":"Hashing for SparkSQL queries​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#hashing-for-sparksql-queries","content":" While running SparkSQL queries using window functions on skewed data, you may have observed that it runs out of memory sometimes.  Following could be an example query working on top of a skewed dataset.  select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem   Considering there is a skew in l_orderkey field, we can split the above query into 4 hashes.  select * from (select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 1 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 2 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 3 union select *, ROW_NUMBER() OVER (partition by l_orderkey order by l_orderkey ) AS row_num FROM testdb.skewlineitem where cast(l_orderkey as integer)%4 = 4 ) limit 10;   If the values are highly skewed, then salting approaches should be used instead since this approach will still send all the skewed keys to a single task. This approach should be used to prevent OOMs quickly rather than to increase performance. The read job is re-computed for the number of sub queries written.  ","version":"Next","tagName":"h3"},{"title":"BP 5.1.17 - Choose the right type of join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5117-----choose-the-right-type-of-join","content":" There are several types of joins in Spark. Some are more optimal than others based on certain considerations. Spark by default does a few join optimizations. However, we can pass join &quot;hints&quot; as well if needed to instruct Spark to use our preferred type of join. For example, in the following SparkSQL queries we supply broadcast and shuffle join hints respectively.  SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key; SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;   ","version":"Next","tagName":"h2"},{"title":"Broadcast Join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#broadcast-join","content":" Broadcast join i.e., map-side join is the most optimal join, provided one of your tables is small enough - in the order of MBs and you are performing an equi (=) join. All join types are supported except full outer joins. This join type broadcasts the smaller table as a hash table across all the worker nodes in memory. Note that once the small table has been broadcasted, we cannot make changes to it. Now that the hash table is locally in the JVM, it is merged easily with the large table based on the condition using a hash join. High performance while using this join can be attributed to minimal shuffle overhead. From EMR 5.30 and EMR 6.x onwards, by default, while performing a join if one of your tables is &lt;= 10 MB, this join strategy is chosen. This is based on the parameter spark.sql.autoBroadcastJoinThreshold which is defaulted to 10 MB.  If one of your join tables are larger than 10 MB, you can either modify spark.sql.autoBroadcastJoinThreshold or use an explicit broadcast hint. You can verify that your query uses a broadcast join by investigating the live plan from SQL tab of Spark UI.    Please note that you should not use this join if your &quot;small&quot; table is not small enough. For eg, when you are joining a 10 GB table with a 10 TB table, your smaller table may still be large enough to not fit into the executor memory and will subsequently lead to OOMs and other type of failures. Also, it is not recommended to pass GBs of data over network to all of the workers which will cause serious network bottlenecks. Only use this join if broadcast table size is &lt;1 GB.  ","version":"Next","tagName":"h3"},{"title":"Sort Merge Join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#sort-merge-join","content":" This is the most common join used by Spark. If you are joining two large tables (&gt;10 MB by default), your join keys are sortable and your join condition is equi (=), it is highly likely that Spark uses a Sort Merge join which can be verified by looking into the live plan from the Spark UI.    Spark configuration spark.sql.join.preferSortMergeJoin is defaulted to true from Spark 2.3 onwards. When this join is implemented, data is read from both tables and shuffled. After this shuffle operation, records with the same keys from both datasets are sent to the same partition. Here, the entire dataset is not broadcasted, which means that the data in each partition will be of manageable size after the shuffle operation. After this, records on both sides are sorted by the join key. A join is performed by iterating over the records on the sorted dataset. Since the dataset is sorted, the merge or join operation is stopped for an element as soon as a key mismatch is encountered. So a join attempt is not performed on all keys. After sorting, join operation is performed upon iterating the datasets on both sides which will happen quickly on the sorted datasets.  Continue to use this join type if you are joining two large tables with an equi condition on sortable keys. Do not convert a sort merge join to broadcast unless one of the tables is &lt; 1 GB. All join types are supported.  ","version":"Next","tagName":"h3"},{"title":"Shuffle Hash Join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#shuffle-hash-join","content":" Shuffle Hash Join sends data with the same join keys in the same executor node followed by a Hash Join. The data is shuffled among the executors using the join key. Then, the data is combined using Hash Join since data from the same key will be present in the same executor. In most cases, this join type performs poorly when compared to Sort Merge join since it is more shuffle intensive. Typically, this join type is avoided by Spark unless spark.sql.join.preferSortMergeJoin is set to &quot;false&quot; or the join keys are not sortable. This join also supports only equi conditions. All join types are supported except full outer joins. If you find out from the Spark UI that you are using a Shuffle Hash join, then check your join condition to see if you are using non-sortable keys and cast them to a sortable type to convert it into a Sort Merge join.  ","version":"Next","tagName":"h3"},{"title":"Broadcast Nested Loop Join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#broadcast-nested-loop-join","content":" Broadcast Nested Loop Join broadcasts one of the entire datasets and performs a nested loop to join the data. Some of the results are broadcasted for a better performance. Broadcast Nested Loop Join generally leads to poor job performance and may lead to OOMs or network bottlenecks. This join type is avoided by Spark unless no other options are applicable. It supports both equi and non-equi join conditions (&lt;,&gt;,&lt;=,&gt;=,like conditions,array/list matching etc.). If you see this join being used by Spark upon investigating your query plan, it is possible that it is being caused by a poor coding practice.    Best way to eliminate this join is to see if you can change your code to use equi condition instead. For example, if you are joining two tables by matching elements from two arrays, explode the arrays first and do an equi join. However, there are some cases where this join strategy is not avoidable.  For example, below code leads to Broadcast Nested Loop Join.  val df1 = spark.sql(&quot;select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-03' and l_shipmode='SHIP'&quot;) val df2 = spark.sql(&quot;select * from testdb.lineitem_shipmodesuppkey_part where l_shipdate='1993-12-04' and l_shipmode='SHIP'&quot;) val nestedLoopDF = df1.join(df2, df1(&quot;l_partkey&quot;) === df2(&quot;l_partkey&quot;) || df1(&quot;l_linenumber&quot;) === df2(&quot;l_linenumber&quot;))   Instead, you can change the code like below:  val result1 = df1.join(df2, df1(&quot;l_partkey&quot;) === df2(&quot;l_partkey&quot;)) val result2 = df1.join(df2, df1(&quot;l_linenumber&quot;) === df2(&quot;l_linenumber&quot;)) val resultDF = result1.union(result2)   The query plan after optimization looks like below. You can also optionally pass a broadcast hint to ensure that broadcast join happens if any one of your two tables is small enough. In the following case, it picked broadcast join by default since one of the two tables met spark.sql.autoBroadcastJoinThreshold.    ","version":"Next","tagName":"h3"},{"title":"Cartesian Join​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#cartesian-join","content":" Cartesian joins or cross joins are typically the worst type of joins. It is chosen if you are running &quot;inner like&quot; queries. This type of join follows the below procedure which as you can see is very inefficient and may lead to OOMs and network bottlenecks.  for l_key in lhs_table: for r_key in rhs_table: #Execute join condition   If this join type cannot be avoided, consider passing a Broadcast hint on one of the tables if it is small enough which will lead to Spark picking Broadcast Nested Loop Join instead. Broadcast Nested Loop Join may be slightly better than the cartesian joins in some cases since atleast some of the results are broadcasted for better performance.  Following code will lead to a Cartesian product provided the tables do not meet spark.sql.autoBroadcastJoinThreshold.  val crossJoinDF = df1.join(df2, df1(&quot;l_partkey&quot;) &gt;= df2(&quot;l_partkey&quot;))     Now, passing a broadcast hint which leads to Broadcast Nested Loop Join  val crossJoinDF = df1.join(broadcast(df2), df1(&quot;l_partkey&quot;) &gt;= df2(&quot;l_partkey&quot;))     ","version":"Next","tagName":"h3"},{"title":"BP 5.1.18 - Consider Spark Blacklisting for large clusters​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5118----consider-spark-blacklisting-for-large-clusters","content":" Spark provides blacklisting feature which allows you to blacklist an executor or even an entire node if one or more tasks fail on the same node or executor for more than configured number of times. Spark blacklisting properties may prove to be very useful especially for very large clusters (100+ nodes) where you may rarely encounter an impaired node. We discussed this issue briefly in BPs 5.1.13 and 5.1.14.  This blacklisting is enabled by default in Amazon EMR with the spark.blacklist.decommissioning.enabled property set to true. You can control the time for which the node is blacklisted using spark.blacklist.decommissioning.timeout property, which is set to 1 hour by default, equal to the default value for yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs. It is recommended to set spark.blacklist.decommissioning.timeout to a value equal to or greater than yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs to make sure that Amazon EMR blacklists the node for the entire decommissioning period.  Following are some experimental blacklisting properties.  spark.blacklist.task.maxTaskAttemptsPerExecutor determines the number of times a unit task can be retried on one executor before it is blacklisted for that task. Defaults to 2.  spark.blacklist.task.maxTaskAttemptsPerNode determines the number of times a unit task can be retried on one worker node before the entire node is blacklisted for that task. Defaults to 2.  spark.blacklist.stage.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire stage.  spark.blacklist.stage.maxFailedExecutorsPerNode determines how many different executors are marked as blacklisted for a given stage, before the entire worker node is marked as blacklisted for the stage. Defaults to 2.  spark.blacklist.application.maxFailedTasksPerExecutor is same as spark.blacklist.task.maxTaskAttemptsPerExecutor but the executor is blacklisted for the entire application.  spark.blacklist.application.maxFailedExecutorsPerNode is same as spark.blacklist.stage.maxFailedExecutorsPerNode but the worker node is blacklisted for the entire application.  spark.blacklist.killBlacklistedExecutors when set to true will kill the executors when they are blacklisted for the entire application or during a fetch failure. If node blacklisting properties are used, it will kill all the executors of a blacklisted node. It defaults to false. Use with caution since it is susceptible to unexpected behavior due to red herring.  spark.blacklist.application.fetchFailure.enabled when set to true will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted. This setting is aggressive. Fetch failures usually happen due to a rare occurrence of impaired hardware but may happen due to other reasons as well. Use with caution since it is susceptible to unexpected behavior due to red herring.  The node blacklisting configurations are helpful for the rarely impaired hardware case we discussed earlier. For example, following configurations can be set to ensure that if a task fails more than 2 times in an executor and if more than two executors fail in a particular worker or if you encounter a single fetch failure, then the executor and worker are blacklisted and subsequently removed from your application.  [{ &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.blacklist.killBlacklistedExecutors&quot;: &quot;true&quot;, &quot;spark.blacklist.application.fetchFailure.enabled&quot;: &quot;true&quot; }, &quot;configurations&quot;: [] }]   You will be able to distinguish blacklisted executors and nodes from the Spark UI and from the Spark driver logs.    When a stage fails because of fetch failures from a node being decommissioned, by default, Amazon EMR does not count the stage failure toward the maximum number of failures allowed for a stage as set by spark.stage.maxConsecutiveAttempts. This is determined by the setting spark.stage.attempt.ignoreOnDecommissionFetchFailure being set to true. This prevents a job from failing if a stage fails multiple times because of node failures for valid reasons such as a manual resize, an automatic scaling event, or Spot instance interruptions.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.19 - Debugging and monitoring Spark applications​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5119----debugging-and-monitoring-spark-applications","content":" EMR provides several options to debug and monitor your Spark application. As you may have seen from some of the screenshots in this document, Spark UI is very helpful to determine your application performance and identify any potential bottlenecks. With regards to Spark UI, you have 3 options in Amazon EMR.  Spark Event UI - This is the live user interface typically running on port 20888. It shows the most up-to-date status of your jobs in real-time. You can go to this UI from Application Master URI in the Resource Manager UI. If you are using EMR Studio or EMR Managed Notebooks, you can navigate directly to Spark UI from your Jupyter notebook anytime after a Spark application is created using Livy. This UI is not accessible once the application finishes or if your cluster terminates.Spark History Server - SHS runs on port 18080. It shows the history of your job runs. You may also see live application status but not in real time. SHS will persist beyond your application runtime but it becomes inaccessible when your EMR cluster is terminated.EMR Persistent UI - Amazon EMR provides Persistent User Interface for Spark. This UI is accessible for up to 30 days after your application ends even if your cluster is terminated since the logs are stored off-cluster. This option is great for performing post-mortem analysis on your applications without spending on your cluster to stay active.  Spark UI options are also helpful to identify important metrics like shuffle reads/writes, input/output sizes, GC times, and also information like runtime Spark/Hadoop configurations, DAG, execution timeline etc. All these UIs will redirect you to live driver (cluster mode) or executor logs when you click on &quot;stderr&quot; or &quot;stdout&quot; from Tasks and Executors lists. When you encounter a task failure, if stderr of the executor does not provide adequate information, you can check the stdout logs.    Apart from the UIs, you can also see application logs in S3 Log URI configured when you create your EMR cluster. Application Master (AM) logs can be found in s3://bucket/prefix/containers/YARN application ID/container_appID_attemptID_0001/. AM container is the very first container. This is where your driver logs will be located as well if you ran your job in cluster deploy mode. If you ran your job in client deploy mode, driver logs are printed on to the console where you submitted your job which you can write to a file. If you used EMR Step API with client deploy mode, driver logs can be found in EMR Step's stderr. Spark executor logs are found in the same S3 location. All containers than the first container belong to the executors. S3 logs are pushed every few minutes and are not live.    If you have SSH access to the EC2 nodes of your EMR cluster, you can also see application master and executor logs stored in the local disk under /var/log/containers. You will only need to see the local logs if S3 logs are unavailable for some reason. Once the application finishes, the logs are aggregated to HDFS and are available for up to 48 hours based on the property yarn.log-aggregation.retain-seconds.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.20 - Spark Observability Platforms​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5120-----spark-observability-platforms","content":" Spark JMX metrics will supply you with fine-grained details on resource usage. It goes beyond physical memory allocated and identifies the actual heap usage based on which you can tune your workloads and perform cost optimization. There are several ways to expose these JMX metrics. You can simply use a ConsoleSink which prints the metrics to console where you submit your job or CSVSink to write metrics to a file which you can use for data visualization. But these approaches are not tidy. There are more options as detailed here. You can choose an observability platform based on your requirements. Following are some example native options.  ","version":"Next","tagName":"h2"},{"title":"Amazon Managed Services for Prometheus and Grafana​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#amazon-managed-services-for-prometheus-and-grafana","content":" AWS offers Amazon Managed Prometheus (AMP) which is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. Amazon Managed Grafana (AMG) is a fully managed service for open source Grafana developed in collaboration with Grafana Labs. Grafana is a popular open source analytics platform that enables you to query, visualize, alert on and understand your metrics no matter where they are stored. You can find the deployment instructions available to integrate Amazon EMR with OSS Prometheus and Grafana which can be extended to AMP and AMG as well. Additionally, Spark metrics can be collected using PrometheusServlet and prometheus/jmx_exporter. However, some bootstrapping is necessary for this integration.  ###Amazon OpensearchAmazon Opensearch is a community-driven open source fork of Elasticsearch and Kibana. It is a popular service for log analytics. Logs can be indexed from S3 or local worker nodes to Amazon Opensearch either using AWS Opensearch SDK or Spark connector. These logs can then be visualized using Kibana To analyze JMX metrics and logs, you will need to develop a custom script for sinking the JMX metrics and importing logs.  Apart from native solutions, you can also use one of the AWS Partner solutions. Some of the popular choices are Splunk, Data Dog and Sumo Logic.  ","version":"Next","tagName":"h3"},{"title":"BP 5.1.21 - Potential resolutions for not-so-common errors​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5121-----potential-resolutions-for-not-so-common-errors","content":" Following are some interesting resolutions for common (but not so common) errors faced by EMR customers. We will continue to update this list as and when we encounter new and unique issues and resolutions.  ###Potential strategies to mitigate S3 throttling errors For mitigating S3 throttling errors (503: Slow Down), consider increasing fs.s3.maxRetries in emrfs-site configuration. By default, it is set to 15 and you may need to increase it further based on your processing needs. You can also increase the multipart upload threshold in EMRFS. Default value at which MPU triggers is 128 MB.  [{ &quot;classification&quot;: &quot;emrfs-site&quot;, &quot;properties&quot;: { &quot;fs.s3.maxRetries&quot;: &quot;20&quot;, &quot;fs.s3n.multipart.uploads.split.size&quot;: &quot;268435456&quot; }, &quot;configurations&quot;: [] }]   Consider using Iceberg format ObjectStoreLocationProvider to store data under S3 hash [0*7FFFFF] prefixes. This would help S3 scale traffic more efficiently as your job's processing requirements increase and thus help mitigate the S3 throttling errors.   CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ( 'write.object-storage.enabled'=true, 'write.data.path'='s3://my-table-data-bucket') PARTITIONED BY (category);   Your S3 files will be arranged under MURMUR3 S3 hash prefixes like below.   2021-11-01 05:39:24 809.4 KiB 7ffbc860/my_ns/my_table/00328-1642-5ce681a7-dfe3-4751-ab10-37d7e58de08a-00015.parquet 2021-11-01 06:00:10 6.1 MiB 7ffc1730/my_ns/my_table/00460-2631-983d19bf-6c1b-452c-8195-47e450dfad9d-00001.parquet 2021-11-01 04:33:24 6.1 MiB 7ffeeb4e/my_ns/my_table/00156-781-9dbe3f08-0a1d-4733-bd90-9839a7ceda00-00002.parquet   Please note that using Iceberg ObjectStoreLocationProvider is not a fail proof mechanism to avoid S3 503s. You would still need to set appropriate EMRFS retries to provide additional resiliency. You can refer to a detailed POC on Iceberg ObjectStoreLocationProviderhere.  If you have exhausted all the above options, you can create an AWS support case to partition your S3 prefixes for bootstrapping capacity. Please note that the prefix pattern needs to be known in advance for eg: s3://bucket/000-fff/ or s3://bucket/&lt;date fields from 2020-01-20 to 2030-01-20&gt;/.  ###Precautions to take while running too many executors If you are running Spark jobs on large clusters with many number of executors, you may have encountered dropped events from Spark driver logs.  ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. WARN scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 01:00:00 UTC 1970   For this issue, you can increase spark.scheduler.listenerbus.eventqueue.size from default of 10000 to 2x or more until you do not see dropped events anymore.  Running large number of executors may also lead to driver hanging since the executors constantly heartbeat to the driver. You can minimize the impact by increasing spark.executor.heartbeatInterval from 10s to 30s or so. But do not increase to a very high number since this will prevent finished or failed executors from being reclaimed for a long time which will lead to wastage cluster resources.  If you see the Application Master hanging while requesting executors from the Resource Manager, consider increasing spark.yarn.containerLauncherMaxThreads which is defaulted to 25. You may also want to increase spark.yarn.am.memory (default: 512 MB) and spark.yarn.am.cores (default: 1).  ###Adjust HADOOP, YARN and HDFS heap sizes for intensive workflows You can see the heap sizes of HDFS and YARN processes under /etc/hadoop/conf/hadoop-env.sh and /etc/hadoop/conf/yarn-env.sh on your cluster.  In hadoop-env.sh, you can see heap sizes for HDFS daemons.  export HADOOP_OPTS=&quot;$HADOOP_OPTS -server -XX:+ExitOnOutOfMemoryError&quot; export HADOOP_NAMENODE_HEAPSIZE=25190 export HADOOP_DATANODE_HEAPSIZE=4096   In yarn-env.sh, you can see heap sizes for YARN daemons.  export YARN_NODEMANAGER_HEAPSIZE=2048 export YARN_RESOURCEMANAGER_HEAPSIZE=7086   Adjust this heap size as needed based on your processing needs. Sometimes, you may see HDFS errors like &quot;MissingBlocksException&quot; in your job or other random YARN errors. Check your HDFS name node and data node logs or YARN resource manager and node manager logs to ensure that the daemons are healthy. You may find that the daemons are crashing due to OOM issues in .out files like below:  OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f0beb662000, 12288, 0) failed; error='Cannot allocate memory' (errno=12) # # There is insufficient memory for the Java Runtime Environment to continue. # Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory. # An error report file with more information is saved as: # /tmp/hs_err_pid14730.log   In this case, it is possible that your HDFS or YARN daemon was trying to grow its heap size but the OS memory did not have sufficient room to accommodate that. So, when you launch a cluster, you can define -Xms JVM opts to be same as -Xmx for the heap size of the implicated daemon so that the OS memory is allocated when the daemon is initialized. Following is an example for the data node process which can be extended to other daemons as well:  [ { &quot;Classification&quot;: &quot;hadoop-env&quot;, &quot;Properties&quot;: { }, &quot;Configurations&quot;: [ { &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;HADOOP_DATANODE_OPTS&quot;: &quot;-Xms4096m -Xmx4096m $HADOOP_DATANODE_OPTS&quot; “HADOOP_DATANODE_HEAPSIZE”: &quot;4096&quot; }, &quot;Configurations&quot;: [] } ] } ]   Additionally, you can also consider reducing yarn.nodemanager.resource.memory-mb by subtracting the heap sizes of HADOOP, YARN and HDFS daemons from yarn.nodemanager.resource.memory-mb for your instance types.  ###Precautions to take for highly concurrent workloads  When you are running multiple Spark applications in parallel, you may sometimes encounter job or step failures due to errors like “Caused by: java.util.zip.ZipException: error in opening zip file” or hanging of the application or Spark client while trying to launch the Application Master container. Check the CPU utilization on the master node when this happens. If the CPU utilization is high, this issue could be because of the repeated process of zipping and uploading Spark and job libraries to HDFS distributed cache from many parallel applications at the same time. Zipping is a compute intensive operation. Your name node could also be bottlenecked while trying to upload multiple large HDFS files.  22/02/25 21:39:45 INFO Client: Preparing resources for our AM container 22/02/25 21:39:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME. 22/02/25 21:39:48 INFO Client: Uploading resource file:/mnt/tmp/spark-b0fe28f9-17e5-42da-ab8a-5c861d81e25b/__spark_libs__3016570917637060246.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/__spark_libs__3016570917637060246.zip 22/02/25 21:39:49 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/hive-site.xml 22/02/25 21:39:49 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/pyspark.zip 22/02/25 21:39:49 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/py4j-0.10.9-src.zip 22/02/25 21:39:50 INFO Client: Uploading resource file:/mnt/tmp/spark-b0fe28f9-17e5-42da-ab8a-5c861d81e25b/__spark_conf__7549408525505552236.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0003/__spark_conf__.zip   To mitigate this, you can zip your job dependencies along with Spark dependencies in advance, upload the zip file to HDFS or S3 and set spark.yarn.archive to that location. Below is an example:  zip -r spark-dependencies.zip /mnt/jars/ hdfs dfs -mkdir /user/hadoop/deps/ hdfs dfs -copyFromLocal spark-dependencies.zip /user/hadoop/deps/   /mnt/jars location in the master node contains the application JARs along with JARs in /usr/lib/spark/jars. After this, set spark.yarn.archive or spark.yarn.jars in spark-defaults.  spark.yarn.archive hdfs:///user/hadoop/deps/spark-dependencies.zip   You can see that this file size is large.  hdfs dfs -ls hdfs:///user/hadoop/deps/spark-dependencies.zip -rw-r--r-- 1 hadoop hdfsadmingroup 287291138 2022-02-25 21:51 hdfs:///user/hadoop/deps/spark-dependencies.zip   Now you will see that the Spark and Job dependencies are not zipped or uploaded when you submit the job saving a lot of CPU cycles especially when you are running applications at a high concurrency. Other resources uploaded to HDFS by driver can also be zipped and uploaded to HDFS/S3 prior but they are quite lightweight. Monitor your master node's CPU to ensure that the utilization has been brought down.  22/02/25 21:56:08 INFO Client: Preparing resources for our AM container 22/02/25 21:56:08 INFO Client: Source and destination file systems are the same. Not copying hdfs:/user/hadoop/deps/spark-dependencies.zip 22/02/25 21:56:08 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/hive-site.xml 22/02/25 21:56:08 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/pyspark.zip 22/02/25 21:56:08 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/py4j-0.10.9-src.zip 22/02/25 21:56:08 INFO Client: Uploading resource file:/mnt/tmp/spark-0fbfb5a9-7c0c-4f9f-befd-3c8f56bc4688/__spark_conf__5472705335503774914.zip -&gt; hdfs://ip-172-31-45-211.ec2.internal:8020/user/hadoop/.sparkStaging/application_1645574675843_0007/__spark_conf__.zip   If you are using EMR Step API to submit your job, you may encounter another issue during the deletion of your Spark dependency zip file (which will not happen if you follow the above recommendation) and other conf files from /mnt/tmp upon successful YARN job completion. If there is a delay of over 30s during this operation, it leads to EMR step failure even if the corresponding YARN job itself is successful. This is due to the behavior of Hadoop’s ShutdownHook. If this happens, increase hadoop.service.shutdown.timeout property from 30s to to a larger value.  Please feel free to contribute to this list if you would like to share your resolution for any interesting issues that you may have encountered while running Spark workloads on Amazon EMR.  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.22 - How the number of partitions are determined when reading a raw file​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5122-----how-the-number-of-partitions-are-determined-when-reading-a-raw-file","content":" When reading a raw file, that can be a text file, csv, etc. the count behind the number of partitions created from Spark depends from many variables as the methods used to read the file, the default parallelism and so on. Following an overview of how these factors are related between each other so to better understand how files are processed.  Here a brief summary of relationship between core nodes - executors - tasks:  each File is composed by blocks that will be parsed according to the InputFormat corresponding to the specific data format, and generally combines several blocks into one input slice, called InputSplitInputSplit and Task are one-to-one correspondence relationshipeach of these specific Tasks will be assigned to one executor of the nodes on the clustereach node can have one or more Executors, depending on the node resources and executor settingseach Executor consists of cores and memory whose default is based on the node type. Each executor can only execute one task at time.  So based on that, the number of threads/tasks will be based on the number of partitions while reading.  Please note that the S3 connector takes some configuration option (e.g. s3a: fs.s3a.block.size) to simulate blocks in Hadoop services, but the concept of blocks in S3 does not really exists. Unlike HDFS that is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior, EMRFS is an object store, not a file system. For more information, see Hadoop documentation for Object Stores vs. Filesystems.  Now, there are several factors that dictate how a dataset or file is mapped to a partition. First is the method used to read the file (e.g. text file), that changes if you're working with rdds or dataframes:  sc.textFile(...) returns a RDD[String] textFile(String path, int minPartitions) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.   spark.read.text(...) returns a DataSet[Row] or a DataFrame text(String path) Loads text files and returns a DataFrame whose schema starts with a string column named &quot;value&quot;, and followed by partitioned columns if there are any.   ","version":"Next","tagName":"h2"},{"title":"Spark Core API (RDDs)​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#spark-core-api-rdds","content":" When using sc.textFile Spark uses the block size set for the filesysytem protocol it's reading from, to calculate the number of partitions in input:  SparkContext.scala   / * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file */ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) }   FileInputFormat.java   if (isSplitable(fs, path)) { long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize);   When using the S3A protocol the block size is set through the fs.s3a.block.size parameter (default 32M), and when using S3 protocol through fs.s3n.block.size (default 64M). Important to notice here is that with S3 protocol the parameter used is fs.s3n.block.size and not fs.s3.block.size as you would expect. In EMR indeed, when using EMRFS, which means using s3 with s3:// prefix, fs.s3.block.size will not have any affect on the EMRFS configration.  Following some testing results using these parameters:  CONF Input: 1 file, total size 336 MB TEST 1 (default) S3A protocol - fs.s3a.block.size = 32M (default) - Spark no. partitions: 336/32 = 11 S3 protocol - fs.s3n.block.size = 64M (default) - Spark no. partitions: 336/64 = 6 TEST 2 (modified) S3A protocol - fs.s3a.block.size = 64M (modified) - Spark no. partitions: 336/64 = 6 S3protocol - fs.s3n.block.size = 128M (modified) - Spark no. partitions: 336/128 = 3   ","version":"Next","tagName":"h3"},{"title":"Spark SQL (DATAFRAMEs)​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#spark-sql-dataframes","content":" When using spark.read.text no. of spark tasks/partitions depends on default parallelism:  DataSourceScanExec.scala   val defaultMaxSplitBytes = fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism val totalBytes = selectedPartitions.flatMap(_.files.map (_.getLen + openCostInBytes)).sum val bytesPerCore = totalBytes / defaultParallelism val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))   The default Parallelism is determined via:  CoarseGrainedSchedulerBackend.scala   override def defaultParallelism(): Int = { conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2)) }   If defaultParallelism is too large, bytesPerCore will be small, and maxSplitBytes can be small, which can result in more no. of spark tasks/partitions. So if there're more cores, spark.default.parallelism can be large, defaultMaxSplitBytes can be small, and no. of spark tasks/partitions can be large.  In order to tweak the input no. of partitions the following parameters need to be set:  Classification\tProperty\tDescriptionspark-default\tspark.default.parallelism\tdefault: max(total number of vCores, 2) spark-default\tspark.sql.files.maxPartitionBytes\tdefault: 128MB  If these parameters are modified, maximizeResourceAllocation need to be disabled, as it would override spark.default.parallelism parameter.  Following some testing results using these parameters:  CONF - Total number of vCores = 16 -&gt; spark.default.parallelism = 16 - spark.sql.files.maxPartitionBytes = 128MB TEST 1 - Input: 1 CSV file, total size 352,3 MB - Spark no. partitions: 16 - Partition size = 352,3/16 = ∼22,09 MB TEST 2 - Input: 10 CSV files, total size 3523 MB - Spark no. partitions: 30 - Partition size = 3523/30 = ∼117,43 MB     Disclaimer  When writing a file the number of partitions in output will depends from the number of partitions in input that will be maintained if no shuffle operations are applied on the data processed, changed otherwise based on spark.default.parallelism for RDDs and spark.sql.shuffle.partitions for dataframes.  ","version":"Next","tagName":"h3"},{"title":"BP 5.1.23 - Common heath checks recommendations​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5123----common-heath-checks-recommendations","content":" Below are the recommended steps to monitor the health of your cluster:  ● Resource Manager UI : Yarn is the resource manager of the EMR cluster. You can access the Resource Manage persistent UI to get a high level cluster metrics like Apps submitted, Apps Pending, Containers Running,Physical Mem Used % etc. You can also check Cluster node level metric, Scheduler Metrics and application level information and access the Resource Manager UI from the application tab of EMR cluster.  ● NameNode UI: NameNode is the HDFS master. You can access this UI to get information on HDFS status such as Configured Capacity, DFS Used, DFS Remaining, Live Nodes,Decommissioning Nodes. Datanode information tab tells the status of the datanode,datanode volume failures, snapshot and startup progress. The UI also gives utilities like metrics, log level information etc about the HDFS cluster. You can access the namenode UI from the application tab of EMR cluster  ● You can get cluster performance graphs from the Monitoring Tab, the metrics there are of three categories, Cluster Status, Node Status and Inputs and outputs.  Cloudwatch Metrics : EMR cluster Metrics can be monitored while key metrics for lookout are YarnavailableMemoryPercentage, IsIdel, ContainerPendingRatio, CoreNodesPending and CoreNodesRunning. You can create custom cloudwatch metrics as well.  ● EMR metrics Dashboard can be created directly form the EMR monitoring tab or from the Cloudwatch by picking and choosing the EMR metric for the dashboard.  ● Set alarms by specifying metrics and conditions. Search the EMR cluster that you would like to create an alarm on then select the metric of EMR. Select the statistics and time period. Then select the condition for the alarm. Select the Alarm trigger, create or choose the SNS topic, subscribe to the SNS topic. You will get an email for the confirmation, confirm the subscription of the topic. Name the alarm and select create alarm.  ● Why did my Spark job in Amazon EMR fail?  ","version":"Next","tagName":"h2"},{"title":"BP 5.1.24 - Support Reach Out Best Practice​","type":1,"pageTitle":"5.1 - Spark General","url":"/aws-emr-best-practices/docs/bestpractices/Applications/Spark/best_practices#bp-5124-----support-reach-out-best-practice","content":" Try to troubleshoot your spark issues based on the above steps mentioned in the document and refer to additional troubleshooting steps. Reach out to Support if needed by following the below best practices:  Make sure to open the Support Case using an IAM User / Role in the account(s) with affected resources. Cross Account Support is provided for some customers.Open a support case with right severity level and clearly identify the business impact, urgency and add sufficient contact details in the ‘Description’ field in your ticket.NEVER include keys, credentials, passwords, or other sensitive information.Provide the system impact explanation and include necessary details such as logs, regions, AZ’s instance ID’s, Cluster Ids, resource ARNs, etc.For a faster response use the Chat / Phone options, and use a dedicated resource on your end who will be the point of contact. Escalate to your Technical Account Manager (TAM) if needed.  Sample template that can be used while raising support case for AWS EMR service  Issue Details: Cluster ID: Error timeline: Use case Description: Similar/new occurrence   Please attach the logs as needed. Find the below logs and log location:  Log Name\tLog LocationContainer logs\t&lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/containers Step logs\t&lt;s3_log_bucket&gt;/&lt;prefix&gt;/&lt;j-xxxxxxx&gt;/&lt;Step-ID&gt;/stederr Driver logs\tS3 bucket → Container → /application_id/container_XXX_YYYY_00001 In case of client mode → step id and check the step logs which will have driver logs. Executor logs\tContainer → /application_id/container_XXX_YYYY_0000X YARN ResourceManager (Master Node) logs\tnode/application/yarn → yarn-resourcemanager-XXX.gz NodeManager Logs(Core Node) logs\tsudo stop spark-history-server sudo start spark-history-server Amazon EMR 5.30.0 and later release versions systemctl stop spark-history-server systemctl start spark-history-server ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}